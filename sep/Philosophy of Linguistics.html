<div id="article-content">

<!-- BEGIN ARTICLE HTML -->


<div id="aueditable"><!--DO NOT MODIFY THIS LINE AND ABOVE-->

<h1>Philosophy of Linguistics</h1><div id="pubinfo"><em>First published Wed Sep 21, 2011; substantive revision Thu Jan 1, 2015</em></div>

<div id="preamble">

<p>

Philosophy of linguistics is the philosophy of science as applied to 
linguistics. This differentiates it sharply from the philosophy of 
language, traditionally concerned with matters of meaning and 
reference.</p>

<p>

As with the philosophy of other special sciences, there are general 
topics relating to matters like methodology and explanation (e.g., 
the status of statistical explanations in psychology and sociology, 
or the physics-chemistry relation in philosophy of chemistry), and 
more specific philosophical issues that come up in the special 
science at issue (simultaneity for philosophy of physics; 
individuation of species and ecosystems for the philosophy of 
biology). General topics of the first type in the philosophy of 
linguistics include:</p>

<ul>

<li>What the subject matter is,</li>

<li>What the theoretical goals are,</li>

<li>What form theories should take, and</li>

<li>What counts as data.</li>
</ul>

<p>

Specific topics include issues in language learnability, language 
change, the competence-performance distinction, and the expressive 
power of linguistic theories.</p>

<p>

There are also topics that fall on the borderline between philosophy 
of language and philosophy of linguistics: of &#8220;linguistic 
relativity&#8221; (see the supplement on the linguistic relativity hypothesis 
  in the Summer 2015 archived version of the entry on 
 <a href="https://plato.stanford.edu/archives/sum2015/entries/relativism/supplement2.html">relativism</a>),
 language vs. 
 <a href="https://plato.stanford.edu/entries/idiolects/">idiolect</a>,
 
 <a href="https://plato.stanford.edu/entries/assertion/index.html#SpeAct">speech acts</a>
 (including the distinction between locutionary, illocutionary, and 
perlocutionary acts), the language of thought, implicature, and the 
semantics of mental states (see the entries on 
 <a href="https://plato.stanford.edu/entries/analysis/">analysis</a>,

  <a href="https://plato.stanford.edu/entries/compositionality/">semantic compositionality</a>,
 
 <a href="https://plato.stanford.edu/entries/mental-representation/">mental representation</a>,
 
 <a href="https://plato.stanford.edu/entries/pragmatics/">pragmatics</a>,
 and 
 <a href="https://plato.stanford.edu/entries/defaults-semantics-pragmatics/">defaults in semantics and pragmatics</a>).
 In these cases it is often the kind of answer given and not the 
inherent nature of the topic itself that determines the 
classification. Topics that we consider to be more in the philosophy 
of language than the philosophy of linguistics include intensional 
contexts, direct reference, and empty names (see the entries on 
 <a href="https://plato.stanford.edu/entries/prop-attitude-reports/">propositional attitude reports</a>,
 
 <a href="https://plato.stanford.edu/entries/logic-intensional/">intensional logic</a>,
 
 <a href="https://plato.stanford.edu/entries/rigid-designators/">rigid designators</a>,
 
 <a href="https://plato.stanford.edu/entries/reference/">reference</a>,
 and 
 <a href="https://plato.stanford.edu/entries/descriptions/">descriptions</a>).
 </p>

<p>

This entry does not aim to provide a general introduction to 
linguistics for philosophers; readers seeking that should consult a 
suitable textbook such as Akmajian et al. (2010) or Napoli (1996). 
For a general history of Western linguistic thought, including recent
theoretical linguistics, see Seuren (1998). Newmeyer (1986) is useful
additional reading for post-1950 American linguistics. 
Tomalin&#160;(2006) traces the philosophical, scientific, and 
linguistic antecedents of Chomsky&#8217;s magnum opus (1955/1956; published
1975), and Scholz and Pullum (2007) provide a critical review.</p>

</div>

<div id="toc">
<!--Entry Contents-->
<ul>
<li><a href="#ThrAppLinTheExtEmeEss">1. Three Approaches to Linguistic Theorizing: Externalism, Emergentism, and Essentialism</a>
   <ul>
   <li><a href="#Ext">1.1  The Externalists</a></li>
   <li><a href="#Eme">1.2  The Emergentists</a></li>
   <li><a href="#Ess">1.3  The Essentialists</a></li>
   <li><a href="#ComThrApp">1.4  Comparing the three approaches</a></li>
   </ul></li>
<li><a href="#SubMatEssThe">2. The Subject Matter of Linguistic Theories</a>
   <ul>
   <li><a href="#ComPer">2.1 Competence and performance</a></li>
   <li><a href="#ILanELan">2.2  &#8216;I-Language&#8217; and &#8216;E-Language&#8217;</a></li>
   <li><a href="#FacLanNarBroSen">2.3  The faculty of language in narrow and broad senses</a></li>
   <li><a href="#KatPla">2.4  Katzian Platonism</a></li>
   <li><a href="#ComLinThe">2.5  Components of linguistic theories</a></li>
   </ul></li>
<li><a href="#LinMetDat">3.  Linguistic Methodology and Data</a>
   <ul>
   <li><a href="#AcrOveLinInt">3.1  Acrimony over linguistic intuitions</a></li>
   <li><a href="#GraAccJud">3.2  Grammaticality and acceptability judgments</a></li>
   <li><a href="#AssDegAcc">3.3  Assessing degrees of acceptability</a></li>
   <li><a href="#InfExpEli">3.4  Informal and experimental elicitation</a></li>
   <li><a href="#WhaInfMetAct">3.5  What informal methods actually are</a></li>
   <li><a href="#CorDat">3.6  Corpus data</a></li>
   </ul></li>
<li><a href="#Who">4.  Whorfianism</a>
   <ul>
   <li><a href="#BanWho">4.1  Banal Whorfianism</a></li>
   <li><a href="#SoCalSapWhoHyp">4.2  The so-called Sapir-Whorf hypothesis</a></li>
   <li><a href="#AntWhoRhe">4.3  Anti-Whorfian rhetoric</a></li>
   <li><a href="#StrWeaWho">4.4  Strong and weak Whorfianism</a></li>
   <li><a href="#ConEvaWhoHyp">4.5  Constructing and evaluating Whorfian hypotheses</a></li>
   </ul></li>
<li><a href="#LanAcq">5.  Language Acquisition</a>
   <ul>
   <li><a href="#LinNat">5.1  Linguistic nativism</a></li>
   <li><a href="#LanLea">5.2  Language learnability</a></li>
   </ul></li>
<li><a href="#LanEvo">6. Language Evolution</a>
   <ul>
   <li><a href="#PhyEme">6.1  Phylogenetic emergence</a></li>
   <li><a href="#HisEvo">6.2  Historical evolution</a></li>
   </ul></li>
<li><a href="#Bib">Bibliography</a></li>
<li><a href="#Aca">Academic Tools</a></li>
<li><a href="#Oth">Other Internet Resources</a></li>
<li><a href="#Rel">Related Entries</a></li>
</ul>
<!--Entry Contents-->

<hr>

</div>

<div id="main-text">

<h2><a name="ThrAppLinTheExtEmeEss">1.  Three Approaches to Linguistic Theorizing: Externalism, Emergentism, and Essentialism</a></h2>

<p>

The issues we discuss have been debated with vigor and sometimes 
venom. Some of the people involved have had famous exchanges in the 
linguistics journals, in the popular press, and in public forums. To 
understand the sharp disagreements between advocates of the 
approaches it may be useful to have a sketch of the dramatis personae
before us, even if it is undeniably an oversimplification.</p>

<p>

We see three tendencies or foci, divided by what they take to be the 
subject matter, the approach they advocate for studying it, and what 
they count as an explanation. We characterize them roughly in Table 
1.</p>

<div class="figure wide">
 <table class="hrules cellpad-small vert-top avoid-break">
 <tr>
 <td></td>
 <td><span class="sc">externalists</span></td>
 <td><span class="sc">emergentists</span></td>
 <td><span class="sc">essentialists</span></td>
      </tr>

      <tr>
	<td><i>Primary phenomena</i></td>
	<td>Actual utterances as produced by language users</td>
	<td>Facts of social cognition, interaction, and communication</td>
	<td>Intuitions of grammaticality and literal meaning</td>
      </tr>
      <tr>
	<td><i>Primary subject matter</i></td>
	<td>Language use; structural properties of expressions and languages</td>
	<td>Linguistic communication, cognition, variation, and change</td>
	<td>Abstract universal principles that explain the properties of specific languages</td>
      </tr>
      <tr>
	<td><i>Aim</i></td>
	<td>To describe attested expression structure and
	interrelations, and predicting properties of unattested
	expressions</td>
	<td>To explain structural properties of languages in terms of
	general cognitive mechanisms and communicative functions</td>
	<td>To articulate universal principles and provide
	explanations for deep and cross-linguistically constant
	linguistic properties</td>
      </tr>

      <tr>
	<td><i>Linguistic structure</i></td>
	<td>A system of patterns, inferrable from generally
	accessible, objective features of language use</td>
	<td>A system of constructions that range from fixed idiomatic
	phrases to highly abstract productive types</td>
	<td>A system of abstract conditions that may not be evident
	from the experience of typical language users</td>
      </tr>

      <tr>
	<td><i>Values</i></td>
	<td>Accurate modeling of linguistic form that accords with
	empirical data and permits prediction concerning unconsidered
	cases</td>
	<td>Cognitive, cultural, historical, and evolutionary
	explanations of phenomena found in linguistic communication
	systems</td>
	<td>Highly abstract, covering-law explanations for properties
	of language as inferred from linguistic intuitions</td>
      </tr>
      <tr>
	<td><i>Children&#8217;s language</i></td>
	<td>A nascent form of language, very different from adult
	linguistic competence</td>
	<td>A series of stages in an ontogenetic process of developing
	adult communicative competence</td>
	<td>Very similar to adult linguistic competence though
	obscured by cognitive, articulatory, and lexical limits</td>
      </tr>
      <tr>
	<td><i>What is acquired</i></td>
	<td>A grasp of the distributional properties of the
	constituents of expressions of a language</td>
	<td>A mainly conventional and culturally transmitted system
	for linguistic communication</td>
	<td>An internalized generative device that characterizes an
	infinite set of expressions</td>
      </tr>
  </table>
<p class="center"><span class="figlabel">Table 1.</span> Three Approaches to the Study of Language</p>
</div>


<p>

A broad and varied range of distinct research projects can be pursued
within any of these approaches; one advocate may be more motivated by
some parts of the overall project than others are. So the tendencies 
should not be taken as sharply honed, well-developed research 
programs or theories. Rather, they provide background biases for the 
development of specific research programs&#8212;biases which 
sometimes develop into ideological stances or polemical programs or 
lead to the branching off of new specialisms with separate journals. 
In the judgment of Phillips (2010), &#8220;Dialog between adherents 
of different approaches is alarmingly rare.&#8221;</p>

<p>

The names we have given these approaches are just mnemonic tags, not 
descriptions. The Externalists, for example, might well have been 
called &#8216;structural descriptivists&#8217; instead, since they 
tend to be especially concerned to develop models that can be used to
predict the structure of natural language expressions. The 
Externalists have long been referred to by Essentialists as 
&#8216;empiricists&#8217; (and sometimes Externalists apply that term to 
themselves), though this is misleading (see Scholz and Pullum 2006: 
60&#8211;63): the &#8216;empiricist&#8217; tag comes with an 
accusation of denying the role of learning biases in language 
acquisition (see Matthews 1984, Laurence and Margolis 2001), but that
is no part of the Externalists&#8217; creed (see e.g. Elman 1993, Lappin 
and Shieber 2007).</p>

<p>

Emergentists are also sometimes referred to by Essentialists as 
&#8216;empiricists&#8217;, but they either use the Emergentist label 
for themselves (Bates et al. 1998, O&#8217;Grady 2008, MacWhinney 2005) or 
call themselves &#8216;usage-based&#8217; linguists (Barlow and 
Kemmer 2002, Tomasello 2003) or &#8216;construction 
grammarians&#8217; (Goldberg 1995). Newmeyer (1991), like Tomasello, 
refers to the Essentialists as &#8216;formalists&#8217;, because of 
their tendency to employ abstractions, and to use tools from 
mathematics and logic.</p>

<p>

Despite these terminological inconsistencies, we can look at what 
typical members of each approach would say about their vision of 
linguistic science, and what they say about the alternatives. Many of
the central differences between these approaches depend on what 
proponents consider to be the main project of linguistic theorizing, 
and what they count as a satisfying explanation.</p>

<p>

Many researchers&#8212;perhaps most&#8212;mix elements from each of 
the three approaches. For example, if Emergentists are to explain the
syntactic structure of expressions by appeal to facts about the 
nature of the use of symbols in human communication, then they will 
presuppose a great deal of Externalist work in describing linguistic 
patterns, and those Externalists who work on computational parsing 
systems frequently use (at least as a starting point) rule systems 
and &#8216;structural&#8217; patterns worked out by Essentialists. 
Certainly, there are no logical impediments for a researcher with one
tendency from simultaneously pursuing another; these approaches are 
only general centers of emphasis.</p>

<h3><a name="Ext">1.1  The Externalists</a></h3>

<p>

If one assumes, with the Externalists, that the main goal of a 
linguistic theory is to develop accurate models of the structural 
properties of the speech sounds, words, phrases, and other linguistic
items, then the clearly privileged information will include corpora 
(written and oral)&#8212;bodies of attested and recorded language use
(suitably idealized). The goal is to describe how this public record 
exhibits certain (perhaps non-phenomenal) patterns that are 
projectable.</p>

<p>

American structural linguistics of the 1920s to 1950s championed the 
development of techniques for using corpora as a basis for developing
structural descriptions of natural languages, although such work was 
really not practically possible until the wide-spread availability of
cheap, powerful, and fast computers. Andr&#233; Martinet (1960: 1) 
notes that one of the basic assumptions of structuralist approaches 
to linguistics is that &#8220;nothing may be called 
&#8216;linguistic&#8217; that is not manifest or manifested one way 
or another between the mouth of the speaker and the ears of the 
listener&#8221;. He is, however, quick to point out that &#8220;this 
assumption does not entail that linguists should restrict their field
of research to the audible part of the communication 
process&#8212;speech can only be interpreted as such, and not as so 
much noise, because it stands for something else that is not 
speech.&#8221;</p>

<p>

American structuralists&#8212;Leonard Bloomfield in 
particular&#8212;were attacked, sometimes legitimately and sometimes 
illegitimately, by certain factions in the Essentialist tradition. 
For example, it was perhaps justifiable to criticize Bloomfield for 
adopting a nominalist ontology as popularized by the logical 
empiricists. But he was later attacked by Essentialists for holding 
anti-mentalist views about linguistics, when it is arguable that his 
actual view was that the science of linguistics should not commit 
itself to any particular psychological theory. (He had earlier been 
an enthusiast for the mentalist and introspectionist psychology of 
Wilhelm Wundt; see Bloomfield 1914.)</p>

<p>

Externalism continues to thrive within computational linguistics, 
where the American structuralist vison of studying language through 
automatic analysis of corpora has enjoyed a recrudescence, and very 
large, computationally searchable corpora are being used to test 
hypotheses about the structure of languages (see Sampson 2001, 
chapter 1, for discussion).</p>

<h3><a name="Eme">1.2  The Emergentists</a></h3>

<p>

Emergentists aim to explain the capacity for language in terms of 
non-linguistic human capacities: thinking, communicating, and 
interacting. Edward Sapir expressed a characteristic Emergentist 
theme when he wrote:</p>

<blockquote>
Language is primarily a cultural or social product and must be
understood as such&#8230; It is peculiarly important that linguists,
who are often accused, and accused justly, of failure to look beyond
the pretty patterns of their subject matter, should become aware of
what their science may mean for the interpretation of human conduct in
general. (Sapir 1929: 214)
</blockquote>

<p>

The &#8220;pretty patterns&#8221; derided here are characteristic of 
structuralist analyses. Sociolinguistics, which is much closer in 
spirit to Sapir&#8217;s project, studies the influence of social and 
linguistic structure on each other. One particularly influential 
study, Labov (1966), examines the influence of social class on 
language variation. Other sociolinguists examine the relation between
status within a group on linguistic innovation (Eckert 1989). This 
interest in variation within languages is characteristic of 
Emergentist approaches to the study of language.</p>

<p>

Another kind of Emergentist, like Tomasello (2003), will stress the 
role of theory of mind and the capacity to use symbols to change 
conspecifics&#8217; mental states as uniquely human preadaptations for 
language acquisition, use, and invention. MacWhinney (2005) aims to 
explain linguistic phenomena (such as phrase structure and 
constraints on long distance dependencies) in terms of the way 
conversation facilitates accurate information-tracking and 
perspective-switching.</p>

<p>

Functionalist research programs generally fall within the broad 
tendency to approach the study of language as an Emergentist. 
According to one proponent:</p>

<blockquote>
The functionalist view of language [is] as a system of communicative
social interaction&#8230; Syntax is not radically arbitrary, in this
view, but rather is <em>relatively motivated</em> by semantic,
pragmatic, and cognitive concerns. (Van Valin 1991, quoted in Newmeyer
1991: 4; emphasis in original)
</blockquote>

<p>

And according to Russ Tomlin, a linguist who takes a functionalist 
approach:</p>

<blockquote>
Syntax is not autonomous from semantics or pragmatics&#8230;the
rejection of autonomy derives from the observation that the use of
particular grammatical forms is strongly linked, even
deterministically linked, to the presence of particular semantic or
pragmatic functions in discourse. (Tomlin 1990, quoted by Newmeyer
(1991): 4)
</blockquote>

<p>

The idea that linguistic form is autonomous, and more specifically 
that syntactic form (rather than, say, phonological form) is 
autonomous, is a characteristic theme of the Essentialists. And the 
claims of Van Valin and Tomlin to the effect that syntax is 
<em>not</em> independent of semantics and pragmatics might tempt some
to think that Emergentism and Essentialism are logically 
incompatible. But this would be a mistake, since there are a large 
number of nonequivalent autonomy of form theses.</p>

<p>

Even in the context of trying to explain what the autonomy thesis is,
Newmeyer (1991: 3) talks about five formulations of the thesis, each 
of which can be found in some Essentialists&#8217; writings, without 
(apparently) realizing that they are non-equivalent. One is the 
relatively strong claim that the central properties of linguistic 
form <em>must not</em> be defined with essential reference to 
&#8220;concepts outside the system&#8221;, which suggests that no 
primitives in linguistics could be defined in psychological or 
biological terms. Another takes autonomy of form to be a 
<em>normative</em> claim: that linguistic concepts <em>ought</em> not
to be defined or characterized in terms of non-linguistic concepts. 
The third and fourth versions are ontological: one denies that 
central linguistic concepts <em>should</em> be ontologically reduced 
to non-linguistic ones, and the other denies that they <em>can</em> 
be. And in the fifth version the autonomy of syntax is taken to deny 
that syntactic patterning can be <em>explained</em> in terms of 
meaning or discourse functions.</p>

<p>

For each of these versions of autonomy, there are Essentialists who 
agree with it. Probably the paradigmatic Essentialist agrees with 
them all. But Emergentists need not disagree with them all. 
Paradigmatic functionalists like Tomlin, Van Valin and MacWhinney 
could in principle hold that the explanation of syntactic form, for 
example, will ultimately be in terms of discourse functions and 
semantics, but still accept that syntactic categories cannot be 
reduced to non-linguistic ones.</p>

<h3><a name="Ess">1.3  The Essentialists</a></h3>

<p>

If Leonard Bloomfield is the intellectual ancestor of Externalism, 
and Sapir the father of Emergentism, then Noam Chomsky is the 
intellectual ancestor of Essentialism. The researcher with 
predominantly Essentialist inclinations aims to identify the 
intrinsic properties of language that make it what it is. For a huge 
majority of practitioners of this approach&#8212;researchers in the 
tradition of <strong>generative grammar</strong> associated with 
Chomsky&#8212;this means postulating universals of human linguistic 
structure, unlearned but tacitly known, that permit and assist 
children to acquire human languages. This generative Essentialism has
a preference for finding surprising characteristics of languages that
cannot be inferred from the data of usage, and are not predictable 
from human cognition or the requirements of communication.</p>

<p>

Rather than being impressed with language variation, as are 
Emergentists and many Externalists, the generative Essentialists are 
extremely impressed with the idea that very young children of almost 
any intelligence level, and just about any social upbringing, acquire
language to the same high degree of mastery. From this it is inferred
that there must be unlearned features shared by all languages that 
somehow assist in language acquisition.</p>

<p>

A large number of contemporary Essentialists who follow Chomsky&#8217;s 
teaching on this matter claim that semantics and pragmatics are not a
central part of the study of language.
In Chomsky&#8217;s view, &#8220;it is possible that natural language has 
only syntax and pragmatics&#8221; (Chomsky 1995: 26); that is, only 
&#8220;internalist computations and performance systems that access 
them&#8221;; semantic theories are merely &#8220;part of an interface
level&#8221; or &#8220;a form of syntax&#8221; (Chomsky 1992: 223).</p>

<p>

Thus, while Bloomfield understood it to be a sensible practical 
decision to assign semantics to some field other than linguistics 
because of the underdeveloped state of semantic research, Chomsky 
appears to think that semantics as standardly understood is not part 
of the essence of the language faculty at all. (In broad outline, 
this exclusion of semantics from linguistics comports with Sapir&#8217;s 
view that form is linguistic but content is cultural.)</p>

<p>

Although Chomsky is an Essentialist in his approach to the study of 
language, excluding semantics as a central part of linguistic theory 
clearly does not follow from linguistic Essentialism (Katz 1980 
provides a detailed discussion of Chomsky&#8217;s views on semantics). 
Today there are many Essentialists who <em>do</em> hold that 
semantics is a component of a full linguistic theory.</p>

<p>

For example, many linguists today are interested in the 
syntax-semantics interface&#8212;the relationship between the surface
syntactic structure of sentences and their semantic interpretation. 
This area of interest is generally quite alien to philosophers who 
are primarily concerned with semantics only, and it falls outside of 
Chomsky&#8217;s syntactocentric purview as well. Linguists who work in the 
kind of semantics initiated by Montague (1974) certainly focus on the
essential features of language (most of their findings appear to be 
of universal import rather than limited to the semantic rules of 
specific languages). Useful works to consult to get a sense of the 
modern style of investigation of the syntax-semantics interface would
include Partee (1975), Jacobson (1996), Szabolcsi (1997), Chierchia 
(1998), Steedman (2000).</p>

<h3><a name="ComThrApp">1.4  Comparing the three approaches</a></h3>

<p>

The discussion so far has been at a rather high level of abstraction.
It may be useful to contrast the three tendencies by looking at how 
they each would analyze a particular linguistic phenomenon. We have 
selected the syntax of <strong>double-object</strong> clauses like 
<i>Hand the guard your pass</i> (also called 
<strong>ditransitive</strong> clauses), in which the verb is 
immediately followed by a sequence of two noun phrases, the first 
typically denoting a recipient and the second something transferred. 
For many such clauses there is an alternative way of expressing 
roughly the same thing: for <i>Hand the guard your pass</i> there is 
the alternative <i>Hand your pass to the guard</i>, in which the verb
is followed by a single object noun phrase and the recipient is 
expressed after that by a preposition phrase with <i>to</i>. We will 
call these <strong>recipient-PP</strong> clauses.</p>

<h4>1.4.1  A typical Essentialist analysis</h4>

<p>

Larson (1988) offers a generative Essentialist approach to the syntax
of double-object clauses. In order to provide even a rough outline of
his proposals, it will be very useful to be able to use <strong>tree 
diagrams</strong> of syntactic structure. A tree is a mathematical 
object consisting of a set of points called <strong>nodes</strong> 
between which certain relations hold. The nodes correspond to 
syntactic units; left-right order on the page corresponds to temporal
order of utterance between them; and upward connecting lines 
represent the relation &#8216;is an immediate subpart of&#8217;. 
Nodes are labeled to show categories of phrases and words, such as 
noun phrase (NP); preposition phrase (PP); and verb phrase (VP). When
the internal structure of some subpart of a tree is basically 
unimportant to the topic under discussion, it is customary to mask 
that part with an empty triangle. Consider a simple example: an 
active transitive clause like (Ai) and its passive equivalent (Aii).</p>

<dl class="sentag">
<dt>(A)</dt>
<dd>
  <dl class="sentag">
  <dt>i.</dt>
  <dd><i>The guard checked <u>my pass</u>.</i>
      &#160; [active clause]</dd>

  <dt>ii.</dt>
  <dd><i><u>My pass</u> was checked by the guard.</i>
      &#160; [passive clause]</dd>
  </dl></dd>
</dl>

<p>

A tree structure for (Ai) is shown in (T1).</p>

<div class="figure">
  <img src="https://plato.stanford.edu/entries/linguistics/T1.png" class="nocaption" title="A tree structure for sentence (Ai)" alt="A tree structure for sentence (Ai)">
</div>

<p>

In analyses of the sort Larson exemplifies, the structure of an 
expression is given by a <strong>derivation</strong>, which consists 
of a sequence of successively modified trees. Larson calls the 
earliest ones <strong>underlying</strong> structures. The last (and 
least abstract) in the derivation is the surface structure, which 
captures properties relevant to the way the expression is written and
pronounced. The underlying structures are posited in order to better 
identify syntactic generalizations. They are related to surface 
structures by a series of operations called 
<strong>transformations</strong> (which generative Essentialists 
typically regard as mentally real operations of the human language 
faculty).</p>

<p>

One of the fundamental operations that a transformation can effect is
<strong>movement</strong>, which involves shifting a part of the 
syntactic structure of a tree to another location within it. For 
example, it is often claimed that passive clauses have very much the 
same kinds of underlying structures as the synonymous active clauses,
and thus a passive clause like (Aii) would have an underlying 
structure much like (T1). A movement transformation would shift 
<i>the guard</i> toward the end of the clause (and add <i>by</i>), 
and another would shift <i>my pass</i> into the position before the 
verb. In other words, passive clauses look much more like their 
active counterparts in underlying structure.</p>

<p>

In a similar way, Larson proposes that a double-object clause like 
(B.ii) has the same underlying structure as (B.i).</p>

<dl class="sentag">
<dt>(B)</dt>
<dd>
  <dl class="sentag">
  <dt>i.</dt>
  <dd><i>I showed my pass to the guard.</i>
      &#160; [recipient-PP]</dd>

  <dt>ii.</dt>
  <dd><i>I showed the guard my pass.</i>
      &#160; [double object]</dd>
  </dl></dd>
</dl>

<p>

Moreover, he proposes that the transformational operation of deriving
the surface structure of (B.ii) from the underlying structure of 
(B.i) is essentially the same as the one that derives the surface 
structure of (A.ii) from the underlying structure of (A.i).</p>

<p>

Larson adopts many assumptions from Chomsky (1981) and subsequent 
work. One is that all NPs have to be assigned <strong>Case</strong> 
in the course of a derivation. (Case is an abstract syntactic 
property, only indirectly related to the morphological case forms 
displayed by nominative, accusative, and genitive pronouns. Objective
Case is assumed to be assigned to any NP in direct object position, 
e.g., <i>my pass</i> in (T1), and Nominative Case is assigned to an 
NP in the subject position of a tensed clause, e.g., <i>the guard</i>
in (T1).)</p>

<p>

He also makes two specific assumptions about the derivation of 
passive clauses. First, Case assignment to the position immediately 
after the verb is &#8220;suppressed&#8221;, which entails that the NP
there will not get Case unless it moves to some other position. (The 
subject position is the obvious one, because there it will receive 
Nominative Case.) Second, there is an unusual assignment of semantic 
role to NPs: instead of the subject NP being identified as the agent 
of the action the clause describes, that role is assigned to an 
adjunct at the end of the VP (the <i>by</i>-phrase in (A.ii); an 
adjunct is a constituent with an optional modifying role in its 
clause rather than a grammatically obligatory one like subject or 
object).</p>

<p>

Larson proposes that both of these points about passive clauses have 
analogs in the structure of double-object VPs. First, Case assignment
to the position immediately after the verb is suppressed; and since 
Larson takes the preposition to to be the marker of Case, this means 
in effect that <i>to</i> disappears. This entails that the NP after 
<i>to</i> will not get Case unless it moves to some other position. 
Second, there is an unusual assignment of semantic role to NPs: 
instead of the direct object NP being identified as the entity 
affected by the action the clause describes, that role is assigned to
an adjunct at the end of the VP.</p>

<p>

Larson makes some innovative assumptions about VPs. First, he 
proposes that in the underlying structure of a double-object clause 
<i>the direct object precedes the verb</i>, the tree diagram being 
(T2).</p>

<div class="figure">
  <img src="https://plato.stanford.edu/entries/linguistics/T2.png" class="nocaption" title="A tree diagram for the sentence (B.ii)" alt="A tree diagram for the sentence (B.ii)">
</div>

<p>

This does not match the surface order of words (<i>showed my pass to 
the guard</i>), but it is not intended to: it is an underlying 
structure. A transformation will move the verb to the left of <i>my 
pass</i> to produce the surface order seen in (B.i).</p>

<p>

Second, he assumes that there are two nodes labeled VP in a 
double-object clause, and two more labeled V&#8242;, though there is 
only one word of the verb (V) category. (Only the smaller VP and 
V&#8242; are shown in the partial structure (T2).)</p>

<p>

What is important here is that (T2) is the basis for the 
double-object surface structure as well. To produce that, the 
preposition <i>to</i> is erased and an additional NP position (for 
<i>my pass</i>) is attached to the V&#8242;, thus:</p>

<div class="figure">
  <img src="https://plato.stanford.edu/entries/linguistics/T3.png" class="nocaption" alt="Transforming the tree T2 by erasing a preposition and adding an new NP">
</div>

<p>

The additional NP is assigned the affected-entity semantic role. The 
other NP (<i>the guard</i>) does not yet have Case; but Larson 
assumes that it moves into the NP position before the verb. The 
result is shown in (T4), where &#8216;<i>e</i>&#8217; marks the empty
string left where some words have been moved away:</p>

<div class="figure">
  <img src="https://plato.stanford.edu/entries/linguistics/T4.png" class="nocaption" alt="Positioning the new NP before the verb">
</div>

<p>

Larson assumes that in this position <i>the guard</i> can receive 
Case. What remains is for the verb to move into a higher V position 
further to its left, to obtain the surface order:</p>

<div class="figure">
  <img src="https://plato.stanford.edu/entries/linguistics/T5.png" class="nocaption" alt="Moving the verb to a higher V position">
</div>

<p>

The complete sequence of transformations is taken to give a deep 
theoretical explanation of many properties of (B.i) and (B.ii), 
including such things as what could be substituted for the two NPs, 
and the fact there is at least rough truth-conditional equivalence 
between the two clauses.</p>

<p>

The reader with no previous experience of generative linguistics will
have many questions about the foregoing sketch (e.g., whether it is 
really necessary to have <i>the guard</i> after <i>showed</i> in 
(T3), then the opposite order in (T4), and finally the same order 
again in (T5)). We cannot hope to answer such questions here; 
Larson&#8217;s paper is extremely rich in further assumptions, links to the
previous literature, and additional classes of data that he aims to 
explain. But the foregoing should suffice to convey some of the 
flavor of the analysis.</p>

<p>

The key point to note is that Essentialists seek underlying 
symmetries and parallels whose operation is not manifest in the data 
of language use. For Essentialists, there is positive explanatory 
virtue in hypothesizing abstract structures that are very far from 
being inferrable from performance; and the posited operations on 
those structures are justified in terms of elegance and formal 
parallelism with other analyses, not through observation of language 
use in communicative situations.</p>

<h4>1.4.2  A typical Emergentist analysis</h4>

<p>

Many Emergentists are favorably disposed toward the kind of 
<strong>construction grammar</strong> expounded in Goldberg (1995). 
We will use her work as an exemplar of the Emergentist approach. The 
first thing to note is that Goldberg does not take double-object 
clauses like (B.ii) to be derived alternants of recipient-PP 
structures like (B.i), the way Larson does. So she is not looking for
a regular syntactic operation that can relate their derivations; 
indeed, she does not posit derivations at all. She is interested in 
explaining correlations between syntactic, semantic, and pragmatic 
aspects of clauses; for example, she asks this question:</p>

<blockquote>
How are the semantics of independent constructions related such that
the classes of verbs associated with one overlap with the classes of
verbs associated with another?
(Goldberg 1995: 89)
</blockquote>

<p>

Thus she aims to explain why some verbs occur in both the 
double-object and recipient-PP kinds of expression and some do not.</p>

<p>

The fundamental notion in Goldberg&#8217;s linguistic theory is that of a 
<strong>construction</strong>. A construction can be defined very 
roughly as a way of structurally composing words or phrases&#8212;a 
sort of template&#8212;for expressing a certain class of meanings. 
Like Emergentists in general, Goldberg regards linguistic theory as 
continuous with a certain part of general cognitive psychological 
theory; linguistics emerges from this more general theory, and 
linguistic matters are rarely fully separate from cognitive matters. 
So a construction for Goldberg has a mental reality: it corresponds 
to a generalized concept or scenario expressible in a language, 
annotated with a guide to the linguistic structure of the expression.</p>

<p>

Many words will be trivial examples of constructions: a single concept paired 
with a way of pronouncing and some details about grammatical 
restrictions (category, inflectional class, etc.); but constructions 
can be much more abstract and internally complex. The double-object 
construction, which Goldberg calls the Ditransitive Construction, is 
a moderately abstract and complex one; she diagrams it thus (p. 50):</p>

<div class="figure">
  <img src="https://plato.stanford.edu/entries/linguistics/fig1.png" class="nocaption" alt="Goldberg's Ditransitive Construction">
</div>

<p>

This expresses a set of constraints on how to use English to 
communicate the idea of a particular kind of scenario. The scenario 
involves a ternary relation CAUSE-RECEIVE holding between an
agent (<strong>agt</strong>), a recipient (<strong>rec</strong>), and
a patient (<strong>pat</strong>). PRED is a variable that is filled 
by the meaning of a particular verb when it is employed in this 
construction.</p>

<p>

The solid vertical lines downward from <strong>agt</strong> and 
<strong>pat</strong> indicate that for any verb integrated into this 
construction it is required that its subject NP should express the 
agent participant, and the direct object (OBJ<sub>2</sub>) should 
express the patient participant. The dashed vertical line downward 
from rec signals that the first object (OBJ) may express the 
recipient but it does not have to&#8212;the necessity of there being 
a recipient is a property of the construction itself, and not every 
verb demands that it be made explicit who the recipient is. But if 
there are two objects, the first is obligatorily associated with the 
recipient role: <i>We sent the builder a carpenter</i> can only 
express a claim about the sending of a carpenter over to the builder,
never the sending of the builder over to where a carpenter is.</p>

<p>

When a particular verb is used in this construction, it may have 
obligatory accompanying NPs denoting what Goldberg calls 
&#8220;profiled participants&#8221; so that the match between the 
participant roles (<strong>agt</strong>, <strong>rec</strong>, 
<strong>pat</strong>) is one-to-one, as with the verb <i>hand</i>. 
When this verb is used, the agent (&#8216;hander&#8217;), recipient 
(&#8216;handee&#8217;), and item transferred (&#8216;handed&#8217;) 
must all be made explicit. Goldberg gives the following diagram of 
the &#8220;composite structure&#8221; that results when <i>hand</i> 
is used in the construction:</p>

<div class="figure">
  <img src="https://plato.stanford.edu/entries/linguistics/fig2.png" class="nocaption" alt="Instance of Goldberg's Ditransitive Construction">
</div>

<p>

Because of this requirement of explicit presence, <i>Hand him your 
pass</i> is grammatical, but <sup>*</sup><i>Hand him</i> is not, and 
neither is <sup>*</sup><i>Hand your pass</i>. The verb <i>send</i>, 
on the other hand, illustrates the optional syntactic expression of 
the recipient role: we can say <i>Send a text message</i>, which is 
understood to involve some recipient but does not make the recipient 
explicit.</p>

<p>

The R notation relates to the fact that particular verbs may express 
either an <em>instance</em> of causing someone to receive something, 
as with <i>hand</i>, or a <em>means</em> of causing someone to 
receive something, as with <i>kick</i>: what <i>Joe kicked Bill the 
ball</i> means is that Joe caused Bill to receive the ball by means 
of a kicking action.</p>

<p>

Goldberg&#8217;s discussion covers many subtle ways in which the scenario 
communicated affects whether the use of a construction is grammatical
and appropriate. For example, there is something odd about 
<sup>?</sup><i>Joe kicked Bill the ball he was trying to kick to 
Sam</i>: the Ditransitive Construction seems best suited to cases of 
volitional transfer (rather than transfer as an unexpected side 
effect of a blunder). However, an exception is provided by a class of
cases in which the transfer is not of a physical object but is only 
metaphorical: <i>That guy gives me the creeps</i> does not imply any 
volitional transfer of a physical object.</p>

<p>

Metaphorical cases are distinguished from physical transfers in other
ways as well. Goldberg notes sentences like <i>The music lent the 
event a festive air</i>, where <i>the music</i> is subject of the 
verb lend despite the fact that music cannot literally lend anything 
to anyone.</p>

<p>

Goldberg discusses many topics such as metaphorical extension, 
shading, metonymy, cutting, role merging, and also presents various 
general principles linking meanings and constructions. One of these 
principles, the No Synonymy Principle, says that no two syntactically
distinct constructions can be both semantically and pragmatically 
synonymous. It might seem that if any two sentences are synonymous, 
pairs like this are:</p>

<dl class="sentag">
<dt>(C)</dt>
<dd>
  <dl class="sentag">
  <dt>i.</dt>
  <dd><i>She gave her husband an iPod.</i>
   &#160; [double object]</dd>

  <dt>ii.</dt>
  <dd><i>She gave an iPod to her husband.</i>
   &#160; [recipient-PP]</dd>
  </dl></dd>
</dl>

<p>

Yet the two constructions cannot be fully synonymous, both 
semantically and pragmatically, if the No Synonymy Principle is 
correct. And to support the principle, Goldberg notes purported 
contrasts such as this:</p>

<dl class="sentag">
<dt>(D)</dt>
<dd>
  <dl class="sentag">
  <dt>i.</dt>
  <dd><i>She gave her husband a new interest in music.</i>
   &#160; <span class="nw">[double object]</span></dd>

  <dt>ii.</dt>
  <dd><sup>?</sup><i>She gave a new interest in music to her husband.</i>
   &#160; <span class="nw">[recipient-PP]</span></dd>
  </dl></dd>
</dl>

<p>

There is a causation-as-transfer metaphor here, and it seems to be 
compatible with the double object construction but not with the 
recipient-PP. So (in Goldberg&#8217;s view) the two are not fully 
synonymous.</p>

<p>

It is no part of our aim here to provide a full account of the 
content of Goldberg&#8217;s discussion of double-object clauses. But what 
we want to highlight is that the focus is not on finding abstract 
elements or operations of a purely syntactic nature that are 
candidates for being essential properties of language per se. The 
focus for Emergentists is nearly always on the ways in which meaning 
is conveyed, the scenarios that particular constructions are used to 
communicate, and the aspects of language that connect up with 
psychological topics like cognition, perception, and 
conceptualization.</p>

<h4>1.4.3  A typical Externalist analysis</h4>

<p>

One kind of work that is representative of the Externalist tendency is
nicely illustrated by Bresnan et al. (2007) and Bresnan and Ford
(2010). Bresnan and her colleagues defend the use of
corpora&#8212;bodies of attested written and spoken texts. One of
their findings is that a number of types of expressions that linguists
have often taken to be ungrammatical do in fact turn up in actual
use. Essentialists and Emergentists alike have often, purely on the
basis of intuition, asserted that sentences like <i>John gave Mary a
kiss</i> are grammatical but sentences like <i>John gave a kiss to
Mary</i> are no, as we see above with Goldberg&#8217;s (D)(ii). Bresnan and
her colleagues find numerous occurrences of the latter sort on the
World Wide Web, and conclude that they are not ungrammatical or even
unacceptable, but merely dispreferred.</p>

<p>

Bresnan and colleagues used a three-million-word collection of 
recorded and transcribed spontaneous telephone conversations known as
the Switchboard corpus to study the double-object and recipient-PP 
constructions. They first annotated the utterances with indications 
of a number of factors that they thought might influence the choice 
between the double-object and recipient-PP constructions:</p>

<ul>

<li>Discourse accessibility of NPs: does a particular NP refer to
  something already mentioned, or to something new to the
  discourse?</li>

<li>Relative lengths of NPs: what is the difference in number of
  words between the recipient NP and the transferred-item NP?</li>

<li>Definiteness: are the recipient and transferred-item NPs
  definite like <i>the bishop</i> or indefinite like <i>some
  members</i></li>

<li>Animacy: do the recipient and transferred-item NPs denote
  animate beings or inanimate things?</li>

<li>Pronominality: are the recipient and transferred-item NPs
  pronouns?</li>

<li>Number: are the recipient and transferred-item NPs singular or
  plural?</li>

<li>person: are the recipient and transferred-item NPs first-person
  or second-person pronouns, or third person?</li>
</ul>

<p>

They also coded the verb meanings by assigning them to half a dozen 
semantic categories:</p>

<ul>

<li>Abstract senses (<i>give it some thought</i>);</li>

<li>Transfer of possession (<i>give him an armband</i>);</li>

<li>Future transfer of possession (<i>I owe you a dollar</i>);</li>

<li>Prevention of possession (<i>They denied me my rights</i>);</li>

<li>Communication verb sense (<i>tell me your name</i>).</li>
</ul>

<p>

They then constructed a statistical model of the corpus: a 
mathematical formula expressing, for each combination of the factors 
listed above, the ratio of the probabilities of the double object and
the recipient-PP. (To be precise, they used the natural logarithm of 
the ratio of <i>p</i> to 1 &#8722; <i>p</i>, where <i>p</i> is the 
probability of a double-object or recipient-PP in the corpus being of
the double-object form.) They then used logistic regression to 
predict the probability of fit to the data.</p>

<p>

To determine how well the model generalized to unseen data, they 
divided the data randomly 100 times into a training set and a testing
set, fit the model parameters on each training set, and scored its 
predictions on the unseen testing set. The average percent of correct
predictions on unseen data was 92%. All components of the model 
except number of the recipient NP made a statistically significant 
difference&#8212;almost all at the 0.001 level.</p>

<p>

What this means is that knowing only the presence or absence of the 
sort of factors listed above they were reliably able to predict 
whether double-object or recipient-PP structures would be used in a 
given context, with a 92% score accuracy rate.</p>

<p>

The implication is that the two kinds of structure are not 
interchangeable: they are reliably differentiated by the presence of 
other factors in the texts in which they occur.</p>

<p>

They then took the model they had generated for the telephone speech 
data and applied it to a corpus of written material: the <em>Wall 
Street Journal</em> corpus (WSJ), a collection of 1987&#8211;9 
newspaper copy, only roughly edited. The main relevant difference 
with written language is that the language producer has more 
opportunity to reflect thoughtfully on how they are going to phrase 
things. It was reasonable to think that a model based on speech data 
might not transfer well. But instead the model had 93.5% accuracy. 
The authors conclude is that &#8220;the model for spoken English 
transfers beautifully to written&#8221;. The main difference between 
the corpora was found to be a slightly higher probability of the 
recipient-PP structure in written English.</p>

<p>

In a very thorough subsequent study, Bresnan and Ford (2010) show 
that the results also correlate with native speakers&#8217; metalinguistic 
judgments of naturalness for sentence structures, and with lexical 
decision latencies (speed of deciding whether the words in a text 
were genuine English words or not), and with a sentence completion 
task (choosing the most natural of a list of possible completions of 
a partial sentence). The results of these experiments confirmed that 
their model predicted participants&#8217; performance.</p>

<p>

Among the things to note about this work is that it was all done on 
directly recorded performance data: transcripts of people speaking to
each other spontaneously on the phone in the case of the Switchboard 
corpus, stories as written by newspaper journalists in the case of 
WSJ, measured responses of volunteer subjects in a laboratory in the 
case of the psycholinguistic experiments of Bresnan and Ford (2010). 
The focus is on identifying the factors in linguistic performance 
that permit accurate prediction of future performance, and the 
methods of investigation have a replicability and checkability that 
is familiar in the natural sciences.</p>

<p>

However, we should make it clear that the work is not some kind of 
close-to-the-ground collecting and classifying of instances. The 
models that Bresnan and her colleagues develop are sophisticated 
mathematical abstractions, very far removed from the records of 
utterance tokens. They claim that these models &#8220;allow 
linguistic theory to solve more difficult problems than it has in the
past, and to build convergent projects with psychology, computer 
science, and allied fields of cognitive science&#8221; (Bresnan et 
al. 2007: 69).</p>

<h4>1.4.4  Conclusion</h4>

<p>

It is important to see that the contrast we have drawn here is not 
just between three pieces of work that chose to look at different 
aspects of the phenomena associated with double-object sentences. It 
is true that Larson focuses more on details of tree structure, 
Goldberg more on subtle differences in meaning, and Bresnan et al. on
frequencies of occurrence. But that is not what we are pointing to. 
What we want to stress is that we are illustrating three different 
broad approaches to language that regard different facts as likely to
be relevant, and make different assumptions about what needs to be 
accounted for, and what might count as an explanation.</p>

<p>

Larson looks at contrasts between different kinds of clause with 
different meanings and see evidence of abstract operations affecting 
subtle details of tree structure, and parallelism between 
derivational operations formerly thought distinct.</p>

<p>

Goldberg looks at the same facts and sees evidence not for anything 
to do with derivations but for the reality of specific 
constructions&#8212;roughly, packets of syntactic, semantic, and 
pragmatic information tied together by constraints.</p>

<p>

Bresnan and her colleagues see evidence that readily observable facts
about speaker behavior and frequency of word sequences correlate 
closely with certain lexical, syntactic, and semantic properties of 
words.</p>

<p>

Nothing precludes defenders of any of the three approaches from 
paying attention to any of the phenomena that the other approaches 
attend to. There is ample opportunity for linguists to mix aspects of
the three approaches in particular projects. But in broad outline 
there are three different tendencies exhibited here, with 
stereotypical views and assumptions roughly as we laid them out in 
Table 1.</p>

<h2><a name="SubMatEssThe">2.  The Subject Matter of Linguistic Theories</a></h2>

<p>

The complex and multi-faceted character of linguistic phenomena means
that the discipline of linguistics has a whole complex of 
distinguishable subject matters associated with different research 
questions. Among the possible topics for investigation are these:</p>

<ol type="i">

<li>the capacity of humans to acquire, use, and invent languages;</li>

<li>the abstract structural patterns (phonetic, morphological,
  syntactic, or semantic) found in a particular language under some
  idealization;</li>

<li>systematic structural manifestations of the use of some
  particular language;</li>

<li>the changes in a language or among languages across time;</li>

<li>the psychological functioning of individuals who have
  successfully acquired particular languages;</li>

<li>the psychological processes underlying speech or linguistically
  mediated thinking in humans;</li>

<li>the evolutionary origin of (i), and/or (ii).</li>

</ol>

<p>

There is no reason for all of the discipline of linguistics to 
converge on a single subject matter, or to think that the entire 
field of linguistics cannot have a diverse range of subject matters. 
To give a few examples:</p>

<ul class="jfy">

<li>The influential Swiss linguist Ferdinand de Saussure (1916)
  distinguished between <em>langue</em>, a socially shared set of
  abstract conventions (compare with (ii)) and <em>parole</em>, the
  particular choices made by a speaker deploying a language (compare
  (iii)).</li>

<li>The anthropological linguist Edward Sapir (1921, 1929) thought
  that human beings have a seemingly species-universal capacity to
  acquire and use languages (compare (i)), but his own interest was
  limited to the systematic structural features of particular
  languages (compare (ii)) and the psychological reality of linguistic
  units such as the phoneme (an aspect of (vi)), and the psychological
  effects of language and thought (an aspect of (v)).</li>

<li>Bloomfield (1933) showed a strong interest in historical
  linguistic change (compare (iv)), distinguishing that sharply (much
  as Saussure did) from synchronic description of language structure
  ((ii) again) and language use (compare (iii)), arguing that the
  study of (iv) presupposed (vi).</li>

<li>Bloomfield famously eschewed all dualistic mentalistic
  approaches to the study of language, but since he rejected them on
  materialist ontological grounds, his rejection of mentalism was not
  clearly a rejection of (vi) or (vii): his attempt to cast
  linguistics in terms of stimulus-response psychology indicates that
  he was sympathetic to the Weissian psychology of his time and
  accepted that linguistics might have psychological subject
  matter.</li>

<li>Zellig Harris, on the other hand, showed little interest in the
  psychology of language, concentrating on mathematical techniques for
  tackling (ii).</li>
</ul>

<p>

Most saliently of all, Harris&#8217;s student Chomsky reacted strongly 
against indifference toward the mind, and insisted that the principal
subject matter of linguistics was, and had to be, a narrow 
psychological version of (i), and an individual, non-social, and 
internalized conception of (ii).</p>

<p>

In the course of advancing his view, Chomsky introduced a number of 
novel pairs of terms into the linguistics literature: competence vs. 
performance (Chomsky 1965); &#8216;I-language&#8217; vs. 
&#8216;E-language&#8217; (Chomsky 1986); the faculty of language in 
the narrow sense vs. the and faculty of language in the broad sense 
(the &#8216;FLN&#8217; and &#8216;FLB&#8217; of Hauser et al. 2002). 
Because Chomsky&#8217;s terminological innovations have been adopted so 
widely in linguistics, the focus of sections 2.1&#8211;2.3 will be to
examine the use of these expressions as they were introduced into the
linguistics literature and consider their relation to (i)-(vii).</p>

<h3><a name="ComPer">2.1  Competence and performance</a></h3>

<p>

Essentialists invariably distinguish between what Chomsky (1965) 
called <strong>competence</strong> and <strong>performance</strong>. 
Competence is what knowing a language confers: a tacit grasp of the 
structural properties of all the sentences of a language. Performance
involves actual real-time use, and may diverge radically from the 
underlying competence, for at least two reasons: (a) an attempt to 
produce an utterance may be perturbed by non-linguistic factors like 
being distracted or interrupted, changing plans or losing attention, 
being drunk or having a brain injury; or (b) certain capacity limits 
of the mechanisms of perception or production may be overstepped.</p>

<p>

Emergentists tend to feel that the competence/performance distinction
sidelines language use too much. Bybee and McClelland put
it this way:</p>

<blockquote>
One common view is that language has an essential and unique inner
structure that conforms to a universal ideal, and what people say is a
potentially imperfect reflection of this inner essence, muddied by
performance factors. According to an opposing view&#8230;language use
has a major impact on language structure. The experience that users
have with language shapes cognitive representations, which are built
up through the application of general principles of human cognition to
linguistic input. The structure that appears to underlie language use
reflects the operation of these principles as they shape how
individual speakers and hearers represent form and meaning and adapt
these forms and meanings as they speak.
(Bybee and McClelland 2005: 382)
</blockquote>

<p>

And Externalists are often concerned to describe and explain not only
language structure, but also the workings of processing mechanisms 
and the etiology of performance errors.</p>

<p>

However, every linguist accepts that some idealization away from the 
speech phenomena is necessary. Emergentists and Externalists are 
almost always happy to idealize away from sporadic speech errors. 
What they are not so keen to do is to idealize away from limitations 
on linguistic processing and the short-term memory on which it 
relies. Acceptance of a thoroughgoing competence/performance 
distinction thus tends to be a hallmark of Essentialist approaches, 
which take the nature of language to be entirely independent of other
human cognitive processes (though of course capable of connecting to 
them).</p>

<p>

The Essentialists&#8217; practice of idealizing away from even 
psycholinguistically relevant factors like limits on memory and 
processing plays a significant role in various important debates 
within linguistics. Perhaps the most salient and famous is the issue 
of whether English is a finite-state language.</p>

<p>

The claim that English is not accepted by any finite-state automaton 
can only be supported by showing that every grammar for English has 
center- embedding to an unbounded depth (see Levelt 2008: 20&#8211;23
for an exposition and proof of the relevant theorem, originally from 
Chomsky 1959). But even depth-3 center-embedding of clauses (a clause
interrupting a clause that itself interrupts a clause) is in practice
extraordinarily hard to process. Hardly anyone can readily understand
even semantically plausible sentences like <i>Vehicles that engineers
who car companies trust build crash every day</i>. And such sentences
virtually never occur, even in writing. Karlsson (2007) undertakes an
extensive examination of available textual material, and concludes 
that depth-3 center-embeddings are vanishingly rare, and no genuine 
depth-4 center-embedding has ever occurred at all in naturally 
composed text. He proposes that there is no reason to regard 
center-embedding as grammatical beyond depth 3 (and for spoken 
language, depth 2). Karlsson is proposing a grammar that stays close 
to what performance data can confirm; the standard Essentialist view 
is that we should project massively from what is observed, and say 
that depth-<i>n</i> center-embedding is fully grammatical for all 
<i>n</i>.</p>

<h3><a name="ILanELan">2.2  &#8216;I-Language&#8217; and &#8216;E-Language&#8217;</a></h3>

<p>

Chomsky (1986) introduced into the linguistics literature two 
technical notions of a language: &#8216;E-Language&#8217; and 
&#8216;I-Language&#8217;. He deprecates the former as either 
undeserving of study or as a fictional entity, and promotes the latter 
as the only scientifically respectable object of study for a serious 
linguistics.</p>

<h4>2.2.1  &#8216;E-language&#8217;</h4>

<p>

Chomsky&#8217;s notion &#8216;E-language&#8217; is supposed to suggest by 
its initial &#8216;E&#8217; both &#8216;extensional&#8217; (concerned
with which sentences happen to satisfy a definition of a language 
rather than with what the definition says) and &#8216;external&#8217;
(external to the mind, that is, non-mental). The dismissal of 
E-language as an object of study is aimed at critics of 
Essentialism&#8212;many but not all of those critics falling within 
our categories of Externalists and Emergentists.</p>

<p>

<strong>Extensional</strong>. First, there is an attempt to impugn the
extensional notion of a language that is found in two radically
different strands of Externalist work. Some Externalist investigations
are grounded in the details of attested utterances (as collected in
corpora), external to human minds. Others, with mathematical or
computational interests, sometimes idealize languages as extensionally
definable objects (typically infinite sets of strings) with a certain
structure, independently of whatever device might be employed to
characterize them. A set of strings of words either is or is not
regular (finite-state), either is or is not recursive (decidable),
etc., independently of forms of grammar statement. Chomsky (1986)
basically dismissed both corpus-based work and mathematical
linguistics simply on the grounds that they employ an extensional
conception of language that is, a conception that removes the object
of study from having an essential connection with the mental.</p>

<p>

<strong>External</strong>. Second, a distinct meaning based on 
&#8216;external&#8217; was folded into the neologism 
&#8216;E-language&#8217; to suggest criticism of any view that 
conceives of a natural language as a public, intersubjectively 
accessible system used by a community of people (often millions of 
them spread across different countries). Here, the objection is that 
languages as thus conceived have no clear criteria of individuation 
in terms of necessary and sufficient conditions. On this conception, 
the subject matter of interest is a historico-geographical entity 
that changes as it is transmitted over generations, or over mountain 
ranges. Famously, for example, there is a gradual valley-to-valley 
change in the language spoken between southeastern France and 
northwestern Italy such that each valley&#8217;s speakers can understand 
the next. But the far northwesterners clearly speak French and the 
far southeasterners clearly speak Italian. It is the politically 
defined geographical border, not the intrinsic properties of the 
dialects, that would encourage viewing this continuum as two 
different languages.</p>

<p>

Perhaps the most famous quotation by any linguist is standardly 
attributed to Max Weinreich (1945): &#8216;A shprakh iz a dialekt mit
an armey un flot&#8217; (&#8216;A language is a dialect with an army 
and navy&#8217;; he actually credits the remark to an unnamed 
student). The implication is that E-languages are defined in terms of
non-linguistic, non-essential properties. Essentialists object that a
scientific linguistics cannot tolerate individuating French and 
Italian in a way that is subject to historical contingencies of wars 
and treaties (after all, the borders could have coincided with a 
different hill or valley had some battle had a different outcome).</p>

<p>

Considerations of intelligibility fare no better. Mutual 
intelligibility between languages is not a transitive relation, and 
sometimes the intelligibility relation is not even symmetric 
(smaller, more isolated, or less prestigious groups often understand 
the dialects of larger, more central, or higher-prestige groups when 
the converse does not hold). So these sociological facts cannot 
individuate languages either.</p>

<p>

Chomsky therefore concludes that languages cannot be defined or
individuated extensionally or mind-externally, and hence the only
scientifically interesting conception of a &#8216;language&#8217; is
the &#8216;I-language&#8217; view (see for example Chomsky 1986: 25;
1992; 1995 and elsewhere). Chomsky says of E-languages that &#8220;all
scientific approaches have simply abandoned these elements of what is
called &#8216;language&#8217; in common usage&#8221; (Chomsky 1988,
37); and &#8220;we can define E-language in one way or another or not
at all, since the concept appears to play no role in the theory of
language&#8221; (Chomsky 1986: 26; in saying that it appears to play
no role in the theory of language, here he means that it plays no role
in the theory he favours).</p>

<p>

This conclusion may be bewildering to non-linguists as well as 
non-Essentialists. It is at odds with what a broad range of 
philosophers have tacitly assumed or explicitly claimed about 
language or languages: &#8216;[A language] is a practice in which 
people engage&#8230;it is constituted by rules which it is part of 
social custom to follow&#8217; (Dummett 1986: 473&#8211;473); 
&#8216;Language is a set of rules existing at the level of common 
knowledge&#8217; and these rules are &#8216;norms which govern 
intentional social behavior&#8217; (Itkonen 1978: 122), and so on. 
Generally speaking, those philosophers influenced by Wittgenstein 
also take the view that a language is a social-historical entity. But
the opposite view has become a part of the conceptual underpinning of
linguistics for many Essentialists.</p>

<p>

Failing to have precise individuation conditions is surely not a 
sufficient reason to deny that an entity can be studied 
scientifically. &#8216;Language&#8217; as a count noun in the 
extensional and socio-historical sense is vague, but this need not be
any greater obstacle to theorizing about them than is the vagueness 
of other terms for historical entities without clear individuation 
conditions, like &#8216;species&#8217; and &#8216;individual 
organism&#8217; in biology.</p>

<p>

At least some Emergentist linguists, and perhaps some Externalists,
would be content to say that languages are collections of social
conventions, publicly shared, and some philosophers would agree (see
Millikan 2003, for example, and Chomsky 2003 for a reply). Lewis
(1969) explicitly defends the view that language can be understood in
terms of public communications, functioning to solve coordination
problems within a group (although he acknowledges that the
coordination could be between different temporal stages of one
individual, so language use by an isolated person is also
intelligible; see the appendix &#8220;Lewis&#8217;s Theory of Languages
as Conventions&#8221; in the entry on
 <a href="https://plato.stanford.edu/entries/idiolects/appendix.html">idiolects</a>,
 for further discussion of Lewis). What Chomsky calls E-languages, 
then, would be perfectly amenable to linguistic or philosophical 
study.</p>

<h4>2.2.2  &#8216;I-language&#8217;</h4>

<p>

Chomsky (1986) introduced the neologism &#8216;I-language&#8217; in 
part to disambiguate the word &#8216;grammar&#8217;. In earlier 
generative Essentialist literature, &#8216;grammar&#8217; was 
(deliberately) ambiguous between (i) the linguist&#8217;s generative theory
and (ii) what a speaker knows when they know a language. 
&#8216;I-language&#8217; can be regarded as a replacement for Bever&#8217;s
term &#8216;psychogrammar&#8217; (see also George 1989): it denotes a
mental or psychological entity (not a grammarian&#8217;s description of a 
language as externally manifested).</p>

<p>

I-language is first discussed under the sub-heading of 
&#8216;internalized language&#8217; to denote linguistic knowledge. 
Later discussion in Chomsky 1986 and 1995 makes it clear that the 
&#8216;I&#8217; of &#8216;I-language&#8217; is supposed to suggest at
least three English words: &#8216;individual&#8217;, 
&#8216;internal&#8217;, and &#8216;intensional&#8217;. And Chomsky 
emphasizes that the neologism also implies a kind of realism about 
speakers&#8217; knowledge of language.</p>

<p>

<strong>Individual</strong>. A language is claimed to be strictly a 
property of individual human beings&#8212;not groups. The contrast is
between the idiolect of a single individual, and a dialect or 
language of a geographical, social, historical, or political group. 
I-languages are properties of the minds of individuals who know them.</p>

<p>

<strong>Internal</strong>. As generative Essentialists see it, your 
I-language is a state of your mind/brain. Meaning is 
internal&#8212;indeed, on Chomsky&#8217;s conception, an I-language</p>

<blockquote>
is a strictly internalist, individualist approach to language,
analogous in this respect to studies of the visual system. If the
cognitive system of Jones&#8217;s language faculty is in state L, we will
say that Jones has the I-language L. (Chomsky 1995: 13)
</blockquote>

<p>

And he clarifies the sense in which an I-language is internal by 
appealing to an analogy with the way the study of vision is internal:</p>

<blockquote>
The same considerations apply to the study of visual perception along
lines pioneered by David Marr, which has been much discussed in this
connection. This work is mostly concerned with operations carried out
by the retina; loosely put, the mapping of retinal images to the
visual cortex. Marr&#8217;s famous three levels of
analysis&#8212;computational, algorithmic, and
implementation&#8212;have to do with ways of construing such
mappings. Again, the theory applies to a brain in a vat exactly as it
does to a person seeing an object in motion. (Chomsky 1995: 52)
</blockquote>

<p>

Thus, while the speaker&#8217;s I-language may be involved in performing 
operations over representations of distal 
stimuli&#8212;representations of other speaker&#8217;s 
utterances&#8212;I-languages can and should be studied in isolation 
from their external environments.</p>

<p>

Although Chomsky sometimes refers to this narrow individuation of 
I-languages as &#8216;individual&#8217;, he clearly claims that 
I-languages are individuated in isolation from both speech 
communities and other aspects of the broadly conceived natural 
environment:</p>

<blockquote>
Suppose Jones is a member of some ordinary community, and J is
indistinguishable from him except that his total experience derives
from some virtual reality design; or let J be Jones&#8217;s Twin in a
Twin-Earth scenario. They have had indistinguishable experiences and
will behave the same way (in so far as behavior is predictable at
all); they have the same internal states. Suppose that J replaces
Jones in the community, unknown to anyone except the observing
scientist. Unaware of any change, everyone will act as before,
treating J as Jones; J too will continue as before. The scientist
seeking the best theory of all of this will construct a narrow
individualist account of Jones, J, and others in the community. The
account omits nothing&#8230; (Chomsky 1995: 53&#8211;54)
</blockquote>

<p>

This passage can also be seen as suggesting a radically 
intensionalist conception of language.</p>

<p>

<strong>Intensional</strong>. The way in which I-languages are 
&#8216;intensional&#8217; for Chomsky needs a little explication. The
concept of intension is familiar in logic and semantics, where 
&#8216;intensional&#8217; contrasts with &#8216;extensional&#8217;. 
The extension of a predicate like blue is simply the set of all blue 
objects; the intension is the function that picks out in a given 
world the blue objects contained therein. In a similar way, the 
extension of a set can be distinguished from an intensional 
description of the set in terms of a function: the set of integer 
squares is {1, 4, 9, 16, 25, 36, &#8230;}, and the intension
could be given in terms of the one-place function <i>f</i> such that 
<i>f</i>(<i>n</i>) = <i>n</i> &#215; <i>n</i>. One difference between
the two accounts of squaring is that the intensional one could be
applied to a different domain (any domain on which the
&#8216;&#215;&#8217; operation is defined: on the rationals rather
than the integers, for example, the extension of the identically
defined function is a different and larger set containing infinitely
many fractions).</p>

<p>

In an analogous way, a language can be identified with the set of all
and only its expressions (regardless of what sort of object an 
expression is: a word sequence, a tree structure, a complete 
derivation, or whatever), which is the extensional view; but it can 
also be identified intensionally by means of a recipe or formal 
specification of some kind&#8212;what linguists call a grammar.</p>

<p>

In natural language semantics, an intensional context is one where 
substitution of co-extensional terms fails to preserve truth value 
(<i>Scott is Scott</i> is true, and <i>Scott is the author of 
Waverley</i> is true, but the truth of <i>George knows that Scott is 
Scott</i> doesn&#8217;t guarantee the truth of <i>George knows that Scott 
is the author of Waverly</i>, so <i>knows that</i> establishes an 
intensional context).</p>

<p>

Chomsky claims that the truth of an I-language attribution is not 
preserved by substituting terms that have the same extension. That 
is, even when two human beings do not differ at all on what 
expressions are grammatical, it may be false to say that they have 
the same I-language. Where H is a human being and L is a language (in
the informal sense) and R is the relation of knowing (or having, or 
using) that holds between a human being and a language, Chomsky 
holds, in effect, that R establishes an intensional context in 
statements of the theory:</p>

<blockquote>
[F]or H to know L is for H to have a certain I-language. The
statements of the grammar are statements of the theory of mind about
the I-language, hence structures of the brain formulated at a certain
level of abstraction from mechanisms. These structures are specific
things in the world, with their properties&#8230; The I-language L may
be the one used by a speaker but not the I-language
L&#8242; even if the two generate the same class of
expressions (or other formal objects) &#8230; L&#8242;
may not even be a possible human I-language, one attainable by the
language faculty. (Chomsky 1986: 23)
</blockquote>

<p>

The idea is that two individuals can know (or have, or use) different
I-languages that generate exactly the same strings of words, and even
give them exactly the same structures.</p>

<p>

The generative Essentialist conception of an I-language is 
antithetical to Emergentist research programs. If the fundamental 
explanandum of scientific linguistics is how actual linguistic 
communication takes place, one must start by looking at both internal
(psychological) and external (public) practices and conventions in 
virtue of which it occurs, and consider the effect of historical and 
geographic contingencies on the relevant underlying processes. That 
would not rule out &#8216;I-language&#8217; as part of the explanans;
but some Emergentists seem to be <em>fictionalists</em> about 
I-languages, in an analogous sense to the way that Chomsky is a 
fictionalist about E-languages. Emergentists do not see a child as 
learning a generative grammar, but as learning how to use a symbolic 
system for propositional communication. On this view grammars are 
mere artifacts that are developed by linguists to codify aspects of 
the relevant systems, and positing an I-language amounts to 
projecting the linguist&#8217;s codification illegitimately onto human 
minds (see, for example, Tomasello 2003).</p>

<p>

The I-language concept brushes aside certain phenomena of interest to
the Externalists, who hold that the forms of actually attested 
expressions (sentences, phrases, syllables, and systems of such 
units) are of interest for linguistics. For example, computational 
linguistics (work on speech recognition, machine translation, and 
natural language interfaces to databases) must rely on a conception 
of language as public and extensional; so must any work on the 
utterances of young children, or the effects of word frequency on 
vowel reduction, or misunderstandings caused by road sign wordings. 
At the very least, it might be said on behalf of this strain of 
Externalism (along the lines of Soames 1984) that linguistics will 
need careful work on languages as intersubjectively accessible 
systems before hypotheses about the I-language that purportedly 
produces them can be investigated.</p>

<p>

It is a highly biased claim that the E-language concept 
&#8220;appears to play no role in the theory of language&#8221; 
(Chomsky 1986: 26). Indeed, the terminological contrast seems to have
been invented not to clarify a distinction between concepts but to 
nudge linguistic research in a particular direction.</p>

<h3><a name="FacLanNarBroSen">2.3  The faculty of language in narrow and broad senses</a></h3>

<p>

In Hauser et al. (2002) (henceforth HCF) a further pair of 
contrasting terms is introduced. They draw a distinction quite 
separate from the competence/performance and 
&#8216;I-language&#8217;/&#8216;E-language&#8217; distinctions: the 
&#8220;language faculty in the narrow sense&#8221; (FLN) is 
distinguished from the &#8220;language faculty in the broad 
sense&#8221; (FLB). According to HCF, FLB &#8220;excludes other 
organism-internal systems that are necessary but not sufficient for 
language (e.g., memory, respiration, digestion, circulation, 
etc.)&#8221; but includes whatever is involved in language, and FLN 
is some limited part of FLB (p. 1571) This is all fairly vague, but 
it is clear that FLN and FLB are both internal rather than external, 
and individual rather than social.</p>

<p>

The FLN/FLB distinction apparently aims to address the uniqueness of 
one component of the human capacity for language rather than (say) 
the content of human grammars. HCF say (p. 1573) that &#8220;Only FLN
is uniquely human&#8221;; they &#8220;hypothesize that most, if not 
all, of FLB is based on mechanisms shared with nonhuman 
animals&#8221;; and they say:</p>

<blockquote>
[T]he computations underlying FLN may be quite limited. In fact, we
propose in this hypothesis that FLN comprises only the core
computational mechanisms of recursion as they appear in narrow syntax
and the mappings to the interfaces. (ibid.)
</blockquote>

<p>

The components of FLB that HCF hypothesize are not part of FLN are 
the &#8220;sensory-motor&#8221; and 
&#8220;conceptual-intentional&#8221; systems. The study of the 
conceptual-intentional system includes investigations of things like 
the theory of mind; referential vocal signals; whether imitation is 
goal directed; and the field of pragmatics. The study of the sensory 
motor system, by contrast, includes &#8220;vocal tract length and 
formant dispersion in birds and primates&#8221;; learning of songs by
songbirds; analyses of vocal dialects in whales and spontaneous 
imitation of artificially created sounds in dolphins; &#8220;primate 
vocal production, including the role of mandibular 
oscillations&#8221;; and &#8220;[c]ross-modal perception and sign 
language in humans versus unimodal communication in animals&#8221;.</p>

<p>

It is presented as an empirical hypothesis that a core property of 
the FLN is &#8220;recursion&#8221;:</p>

<blockquote>
All approaches agree that a core property of FLN is recursion,
attributed to narrow syntax&#8230;FLN takes a finite set of elements
and yields a potentially infinite array of discrete expressions. This
capacity of FLN yields discrete infinity (a property that also
characterizes the natural numbers). (HCF, p. 1571)
</blockquote>

<p>

HCF leave open exactly what the FLN includes in addition to 
recursion. It is not ruled out that the FLN incorporates substantive 
universals as well as the formal property of &#8220;recursion&#8221;.
But whatever &#8220;recursion&#8221; is in this context, it is 
apparently not domain-specific in the sense of earlier discussions by
generative Essentialists, because it is not unique to human natural 
language or defined over specifically linguistic inputs and outputs: 
it is the basis for humans&#8217; grasp of the formal and arguably 
non-natural language of arithmetic (counting, and the successor 
function), and perhaps also navigation and social relations. It might
be more appropriate to say that HCF identify recursion as a cognitive
universal, not a linguistic one. And in that case it is difficult to 
see how the so-called &#8216;language faculty&#8217; deserves that 
name: it is more like a faculty for cognition and communication.</p>

<p>

This abandonment of linguistic domain-specificity contrasts very 
sharply with the picture that was such a prominent characteristic of 
the earlier work on linguistic nativism, popularized in different 
ways by Fodor (1983), Barkow et al. (1992), and Pinker (1994). And 
yet the HCF discussion of FLN seems to incline to the view that human
language capacities have a unique human (though not uniquely 
linguistic) essence.</p>

<p>

The FLN/FLB distinction provides earlier generative Essentialism with
an answer (at least in part) to the question of what the singularity 
of the human language faculty consists in, and it does so in a way 
that subsumes many of the empirical discoveries of paleoanthropology,
primatology, and ethnography that have been part of highly 
influential in Emergentist approaches as well as neo-Darwinian 
Essentialist approaches. A neo-Darwinian Essentialist like Pinker 
will accept that the language faculty involves recursion, but also 
will also hold (with Emergentists) that human language capacities 
originated, via natural selection, for the purpose of linguistic 
communication.</p>

<p>

Thus, over the years, those Essentialists who follow Chomsky closely
have changed the term they use for their core subject matter from
&#8216;linguistic competence&#8217; to &#8216;I-language&#8217; to
&#8216;FLN&#8217;, and the concepts expressed by these terms are all
slightly different.  In particular, what they are counterposed to
differs in each case.</p>

<p>

The challenge for the generative Essentialist adopting the FLN/FLB 
distinction as characterized by HCF is to identify empirical data 
that can support the hypothesis that the FLN &#8220;yields discrete 
infinity&#8221;. That will mean answering the question: discrete 
infinity of what? HCF write that FLN &#8220;takes a finite set of 
elements and yields a potentially infinite array of discrete 
expressions&#8221; (p. 1571), which makes it clear that there must be
a recursive procedure in the mathematical sense, perhaps putting 
atomic elements such as words together to make internally complex 
elements like sentences (&#8220;array&#8221; should probably be 
understood as a misnomer for &#8216;set&#8217;). But then they say, 
somewhat mystifyingly:</p>

<blockquote>
Each of these discrete expressions is then passed to the sensory-motor
and conceptual-intentional systems, which process and elaborate this
information in the use of language. Each expression is, in this sense,
a pairing of sound and meaning.
(HCF, p. 1571)
</blockquote>

<p>

But the sensory-motor and conceptual-intentional systems are concrete
parts of the organism: muscles and nerves and articulatory organs and
perceptual channels and neuronal activity. How can each one of a 
&#8220;potentially infinite array&#8221; be &#8220;passed to&#8221; 
such concrete systems without it taking a potentially infinite amount
of time? HCF may mean that for any one of the expressions that FLN 
defines as well-formed (by generating it) there is a possibility of 
its being used as the basis for a pairing of sound and meaning. This 
would be closer to the classical generative Essentialist view that 
the grammar generates an infinite set of structural descriptions; but
it is not what HCF say.</p>

<p>

At root, HCF is a polemical work intended to identify the view it 
promotes as valuable and all other approaches to linguistics as 
otiose.</p>

<blockquote>
In the varieties of modern linguistics that concern us here, the term
&#8220;language&#8221; is used quite differently to refer to an
internal component of the mind/brain (sometimes called internal
language or I-language).&#8230; However, this biologically and
individually grounded usage still leaves much open to interpretation
(and misunderstanding). For example, a neuroscientist might ask: What
components of the human nervous system are recruited in the use of
language in its broadest sense? Because any aspect of cognition
appears to be, at least in principle, accessible to language, the
broadest answer to this question is, probably, &#8220;most of
it.&#8221; Even aspects of emotion or cognition not readily verbalized
may be influenced by linguistically based thought processes. Thus,
this conception is too broad to be of much use. (HCF,
p. 1570)
</blockquote>

<p>

It is hard to see this as anything other than a claim that approaches
to linguistics focusing on anything that could fall under the label 
&#8216;E-language&#8217; are to be dismissed as useless.</p>

<p>

Some Externalists and Emergentists actually reject the idea that the 
human capacity for language yields &#8220;a potentially infinite 
array of expressions&#8221;. It is often pointed out by empirically 
inclined computational linguists that in practice there will only 
ever be a finite number of sentences to be dealt with (though the 
people saying this may underestimate the sheer vastness of the finite
set involved). And naturally, for those who do not believe there are 
generative grammars in speakers&#8217; heads at all, it holds a fortiori 
that speakers do not have grammars in their heads generating infinite
languages. Externalists and Emergentists tend to hold that the 
&#8220;discrete infinity&#8221; that HCF posits is more plausibly a 
property of the generative Essentialists&#8217; model of linguistic 
competence, I-language, or FLN, than a part of the human mind/brain. 
This does not mean that non-Essentialists deny that actual language 
use is creative, or (of course) that they think there is a longest 
sentence of English. But they may reject the link between linguistic 
productivity or creativity and the mathematical notion of recursion 
(see Pullum and Scholz 2010).</p>

<p>

HCF&#8217;s remarks about how FLN &#8220;yields&#8221; or 
&#8220;generates&#8221; a specific &#8220;array&#8221; assume that 
languages are clearly and sharply individuated by their generators. 
They appear to be committed to the view that there is a fact of the 
matter about exactly which generator is in a given speaker&#8217;s head. 
Emergentists tend not to individuate languages in this way, and may 
reject generative grammars entirely as inappropriately or 
unacceptably &#8216;formalist&#8217;. They are content with the 
notion that the common-sense concept of a language is vague, and it 
is not the job of linguistic theory to explain what a language is, 
any more than it is the job of physicists to explain what material 
is, or of biologists to explain what life is. Emergentists, in 
particular, are interested not so much in identifying generators, or 
individuating languages, but in exploring the component capacities 
that facilitate linguistic communication, and finding out how they 
interact.</p>

<p>

Similarly, Externalists are interested in the linguistic structure of
expressions, but have little use for the idea of a discrete infinity 
of them, a view that is not, and cannot be empirically supported, unless one
thinks of simplicity and elegance of theory as empirical matters. 
They focus on the outward manifestations of language, not on a set of
expressions regarded as a whole language&#8212;at least not in any 
way that would give a language a definite cardinality. Zellig Harris,
an archetypal Externalist, is explicit that the reason for not 
regarding the set of utterances as finite concerns the elegance of 
the resulting grammar: &#8220;If we were to insist on a finite 
language, we would have to include in our grammar several highly 
arbitrary and numerical conditions&#8221; (Harris 1957: 208). 
Infinitude, on his view is an unimportant side consequence of setting
up a sentence-generating grammar in an uncluttered and maximally 
elegant way, not a discovered property of languages (see Pullum and 
Scholz 2010 for further discussion).</p>

<h3><a name="KatPla">2.4  Katzian Platonism</a></h3>

<p>

Not all Essentialists agree that linguistics studies aspects of what 
is in the mind or aspects of what is human. There are some who do not
see language as either mental or human, and certainly do not regard 
linguists as working on a problem within cognitive psychology or 
neurophysiology. Montague (1974), for example, is deeply concerned 
with using powerful higher-order quantified modal logics and possible
worlds to formalize aspects of natural language semantics, but 
eschews psychologism. His leanings are toward Frege, and his ontology
inclines toward platonism rather than psychologism.</p>

<p>

Katz (1981) is an explicit defense of the Fregean view that natural 
languages are timeless, locationless, and necessarily existent. The 
primary essential property that Katz finds in natural languages is 
<em>effability</em>, the property of providing semantic expression 
for absolutely every Fregean proposition. On the platonist view the 
fact that non-spatiotemporally located languages are grasped and used
by human beings raises major epistemological issues (see the section 
titled &#8216;The Epistemological Argument Against Platonism&#8217;
 in the entry on <a href="https://plato.stanford.edu/entries/platonism/index.html#5">Platonism in metaphysics</a>).
 Katz (1998) attempts to address these issues.</p>

<p>

Katz&#8217;s own tripartite classification of linguistic theories, derived 
from medieval solutions to the problem of universals (and used as the
structure of his book of readings, Katz 1985), is orthogonal to our 
classification. Katz sees three ontological conceptions of the 
subject matter of linguistics:</p>

<ul class="jfy">

<li><strong>nominalism</strong> claims that linguistics is about
  physical, non-mental objects;</li>

<li><strong>conceptualism</strong> claims it is about objects that
  are physical but mental (which Katz takes to mean
  neurophysiological); and</li>

<li><strong>platonism</strong> claims that linguistics is about
  abstract objects that are not physical (hence, for him, not mental
  either).</li>
</ul>

<p>

Katz took nominalism to have been refuted by Chomsky in his critiques
of American structuralists in the 1960s. But, in Katz&#8217;s opinion, 
Chomsky had failed to notice that conceptualism was infected with 
many of the same faults as nominalism, because it too localized 
language spatiotemporally (in contingently existing, finite, human 
brains). Through an argument by elimination, Katz concluded that only
platonism remained, and must be the correct view to adopt.</p>

<p>

Katz&#8217;s argument by elimination should probably be taken as another 
example of an effort not to separate and clarify concepts used in 
different kinds of linguistic theorizing, but rather to dismiss and 
exclude certain types of research from the theory of language (see 
Pullum and Scholz 1997 for detailed discussion). But regardless of 
that, his typology of linguists should certainly not be thought to 
relate directly to the distinctions between centers of interest in 
linguistic theorizing around which this article is structured. No 
particular metaphysical view unifies any of our three groupings. For 
example, not all Externalists incline toward nominalism; numerous 
Emergentists as well as most Essentialists take linguistics to be 
about mental phenomena; and our Essentialists include Katz&#8217;s 
platonists alongside the Chomskyan &#8216;I-language&#8217; 
advocates.</p>

<h3><a name="ComLinThe">2.5  Components of linguistic theories</a></h3>

<p>

Linguists&#8217; conception of the components of the study of language 
contrast with philosophers&#8217; conceptions (even those of philosophers 
of language) in at least three ways. First, linguists are often 
intensely interested in small details of linguistic form in their own
right. Second, linguists take an interest in whole topic areas like 
the internal structure of phrases, the physics of pronunciation, 
morphological features such as conjugation classes, lexical 
information about particular words, and so on&#8212;topics in which 
there is typically little philosophical payoff. And third, linguists 
are concerned with relations between the different subsystems of 
languages: the exact way the syntax meshes with the semantics, the 
relationship between phonological and syntactic facts, and so on.</p>

<p>

With regard to form, philosophers broadly follow Morris (1938), a 
foundational work in semiotics, and to some extent Peirce (see SEP 
entry: Peirce, semiotics), in thinking of the theory of language as 
having three main components:</p>

<ul>

<li><i>syntax</i>, which treats of the form of signs;</li>

<li><i>semantics</i>, which deals with the relations of signs to
  their denotations; and</li>

<li><i>pragmatics</i>, which concerns the contextualized use of
  interpreted signs.</li>
</ul>

<p>

Linguists, by contrast, following both Sapir (1921) and Bloomfield 
(1933), treat the syntactic component in a more detailed way than 
Morris or Peirce, and distinguish between at least three kinds of 
linguistic form: the form of speech sounds (phonology), the form of 
words (morphology), and the form of sentences. (If syntax is about 
the form of expressions in general, then each of these would be an 
element of Morris&#8217;s syntax.)</p>

<p>

Emergentists in general deny that there is a distinction between 
semantics and pragmatics&#8212;a position that is familiar enough in 
philosophy: Quine (1987: 211), for instance, holds that &#8220;the 
separation between semantics and pragmatics is a pernicious 
error.&#8221; And generally speaking, those theorists who, like the 
later Wittgenstein, focus on meaning as use will deny that one can 
separate semantics from pragmatics. Emergentists such as Paul Hopper 
&amp; Sandra Thompson agree:</p>

<blockquote>
[W]hat is called semantics and what is called pragmatics are an
integrated whole. (Hopper and Thompson 1993: 372)
</blockquote>

<p>

Some Essentialists&#8212;notably Chomsky&#8212;also deny that 
semantics can be separated from pragmatics, but unlike the 
Emergentists (who think that semantics-pragmatics is a starting point
for linguistic theory), Chomsky (as we noted briefly in section 1.3) 
denies that semantics and pragmatics can have any role in 
linguistics:</p>

<blockquote>
<p>It seems that other cognitive systems&#8212;in particular, our system
of beliefs concerning things in the world and their
behavior&#8212;play an essential part in our judgments of meaning and
reference, in an extremely intricate manner, and it is not at all
clear that much will remain if we try to separate the purely
linguistic components of what in informal usage or even in technical
discussion we call &#8216;the meaning of [a] linguistic expression.&#8217;
(Chomsky 1979; 142)
</p>
</blockquote>

<p>
Regarding the theoretical account of the relation between words or
phrases and what speakers take them to refer to, Chomsky says,
&#8220;I think such theories should be regarded as a variety of
syntax&#8221; (Chomsky 1992: 223).
</p>

<p>

Not every Essentialist agrees with Chomsky on this point. Many 
believe that every theory should incorporate a linguistic component 
that yields meanings, in much the same way that many philosophers of 
language believe there to be such a separate component. Often, 
although not always, this component amounts to a truth-theoretic 
account of the values of syntactically-characterized sentences. This 
typically involves a translation of the natural language sentence 
into some representation that is &#8220;intermediate&#8221; between 
natural language and a truth-theory&#8212;perhaps an augmented 
version of first-order logic, or perhaps a higher-order intensional 
language. The Essentialists who study semantics in such ways usually 
agree with Chomsky in seeing little role for pragmatics within 
linguistic theory. But their separation of semantics from pragmatics 
allows them to accord semantics a legitimacy within linguistics 
itself, and not just in psychology or sociology.</p>

<p>

Such Essentialists, as well as the Emergentists, differ in important 
ways from classical philosophical logic in their attitudes towards 
&#8220;the syntactic-semantic interface&#8221;, however. Philosophers
of language and logic who are not also heavily influenced by 
linguistics tend to move directly&#8212;perhaps by means of a 
&#8220;semantic intuition&#8221; or perhaps from an intuitive 
understanding of the truth conditions involved&#8212;from a natural 
language sentence to its &#8220;deep, logical&#8221; representation. 
For example, they may move directly from (EX1) to (LF1):</p>

<dl class="sentag tag4em">

<dt>(EX1)</dt>
<dd>Every linguist admires P&#257;nini.</dd>

<dt>(LF1)</dt>
<dd><i>x</i>(Linguist(<i>x</i>) &#8835;
  Admires(<i>x</i>, <strong><i>p</i></strong>))</dd>

</dl>

<p>

And from there perhaps to a model-theoretic description of its 
truth-conditions. A linguist, on the other hand, would aim to 
describe how (EX1) and (LF1) are related. From the point of view of a
semantically-inclined Essentialist, the question is: how should the 
syntactic component of linguistic theory be written so that the 
semantic value (or, &#8220;logical form representation&#8221;) can be
assigned? From some Emergentist points of view, the question is: how 
can the semantic properties and communicative function of an 
expression explain its syntactic properties?</p>

<p>

Matters are perhaps less clear with the Externalists&#8212;at least 
with those who identify semantic value with distribution in terms of 
neighboring words (there is a tradition stemming from the 
structuralists of equating synonymy with the possibility of 
substitution in all contexts without affecting acceptability).</p>

<p>

Matters are in general quite a bit more subtle and tricky than (EX1) 
might suggest. Philosophers have taken the natural language sentence 
(EX2) to have two logical forms, (LF2a) and (LF2b):</p>

<dl class="sentag tag4em">

<dt>(EX2)</dt>
<dd>Every linguist admires a philosopher.</dd>

<dt>(LF2a)</dt>
<dd>&#8704;<i>x</i>(Linguist(<i>x</i>)
  &#8835; &#8707;<i>y</i>(Philosopher(<i>y</i>) &#8743;
  Admires(<i>x</i>,<i>y</i>)))</dd>

<dt>(LF2b)</dt>
<dd>&#8707;<i>x</i>(Philosopher(<i>x</i>) &#8743;
  &#8704;<i>y</i>(Linguist(<i>y</i>) &#8835;
  Admires(<i>y</i>,<i>x</i>)))</dd>
</dl>

<p>

But for the linguist interested in the syntax-semantics interface, 
there needs to be some explanation of how (LF2a) and (LF2b) are 
associated with (EX2). It could be a way in which rules can derive 
(LF2a) and (LF2b) from the syntactic representation of (EX2), as some
semantically-inclined Essentialists would propose, or a way to 
explain the syntactic properties of (EX2) from facts about the 
meanings represented by (LF2a) and (LF2b), as some Emergentists might
want. But that they should be connected up in some way is something 
that linguists would typically count as non-negotiable.</p>

<h2><a name="LinMetDat">3.  Linguistic Methodology and Data</a></h2>

<p>

The strengths and limitations of different data gathering methods 
began to play an important role in linguistics in the early to 
mid-20th century. Voegelin and Harris (1951: 323) discuss several 
methods that had been used to distinguish Amerindian languages and 
dialects:</p>

<ul class="jfy">

<li><strong>Informal elicitation</strong>: asking an informant for a
  metalinguistic judgment on an expression. [E.g., &#8220;Is this
  sentence grammatical?&#8221; &#8220;Do these two sentences mean the
  same thing?&#8221;]</li>

<li><strong>Corpus collection</strong>: gathering a body of
  naturally occurring utterances.</li>

<li><strong>Controlled experimentation</strong>: testing informants
  in some way that directly gauges their linguistic capacities.</li>
</ul>

<p>

They note that the anthropological linguists Boas and Sapir (who we 
take to be proto-Emergentists) used the &#8216;ask the 
informant&#8217; method of informal elicitation, addressing questions
&#8220;to the informant&#8217;s perception rather than to the data 
directly&#8221; (1951: 324). Bloomfield (the proto-Externalist), on 
the other hand, worked on Amerindian languages mostly by collecting 
corpora, with occasional use of monolingual elicitation.</p>

<p>

The preferred method of Essentialists today is informal elicitation, 
including elicitation from oneself. Although the techniques for 
gathering data about speakers and their language use have changed 
dramatically over the past 60 or more years, the general strategies 
have not: data is still gathered by elicitation of metalinguistic 
judgments, collection of corpus material, or direct psychological 
testing of speakers&#8217; reactions and behaviors. Different linguists 
will have different preferences among these techniques, but it is 
important to understand that data could be gathered in any of the 
three ways by advocates of any tendency. Essentialists, Emergentists,
and Externalists differ as much on how data is interpreted and used 
as on their views of how it should be gathered.</p>

<p>

A wide range of methodological issues about data collection have been
raised in linguistics. Since gathering data by direct objective 
experimental testing of informants is a familiar practice throughout 
the social, psychological, medical, and biological sciences, we will 
say little about it here, focusing instead on these five issues about
data:</p>

<ol type="i">

<li>Disputes over the use of linguistic intuitions as linguistic data;</li>

<li>Differences between grammaticality and acceptability judgments;</li>

<li>Differences between scales for measuring acceptability judgments;</li>

<li>Debates about the reliability of informal judgment elicitation methods; and</li>

<li>Issues concerning the relevance and reliability of corpus evidence.</li>
</ol>

<h3><a name="AcrOveLinInt">3.1  Acrimony over linguistic intuitions</a></h3>

<p>

The debate in linguistics over the use of linguistic intuitions 
(elicited metalinguistic judgments) as data, and how that data should
be collected has resulted in enduring, rancorous, often ideologically
tinged disputes over the past 45 years. The disputes are remarkable, 
if only for their fairly consistent venomous tone.</p>

<p>

At their most extreme, many Emergentists and some Externalists cast 
the debate in terms of whether linguistic intuitions should ever 
count as evidence for linguistic theorizing. And many Essentialists 
cast it in terms of whether anything but linguistic intuitions are 
ever really needed to support linguistic theorizing.</p>

<p>

The debate focuses on the Essentialists&#8217; notion of a mental grammar, 
since linguistic intuitions are generally understood to be a 
consequence of tacit knowledge of language. Emergentists who deny 
that speakers have innate domain-specific grammars (competence, 
I-languages, or FLN) have raised a diverse range of objections to the
use of reports of intuitions as linguistic data. (But see Devitt 2006
for an understanding of linguistic intuitions that does not base them
on inferred tacit knowledge of competence grammars.) The following 
passages are representative Emergentist critiques of intuitions 
(elicited judgments):</p>

<blockquote>
<p>Generative linguists typically respond to calls for evidence for
the reality of their theoretical constructs by claiming that no
evidence is needed over and above the theory&#8217;s ability to account for
patterns of grammaticality judgments elicited from native
speakers. This response is unsatisfactory on two accounts. First, such
judgments are inherently unreliable because of their unavoidable
meta-cognitive overtones&#8230; Second, the outcome of a judgment (or
the analysis of an elicited utterance) is invariably brought to bear
on some distinction between variants of the current generative theory,
never on its foundational assumptions. (Edelman and Christiansen 2003:
60)</p>

<p>
The data that are actually used toward this end in Generative Grammar
analyses are almost always disembodied sentences that analysts have
made up ad hoc, &#8230; rather than utterances produced by real people
in real discourse situations&#8230; In diametric opposition to these
methodological assumptions and choices, cognitive-functional linguists
take as their object of study all aspects of natural language
understanding and use&#8230; They (especially the more functionally
oriented analysts) take as an important part of their data not
disembodied sentences derived from introspection, but rather
utterances or other longer sequences from naturally occurring
discourse. (Tomasello 1998: xiii)</p>

<p>
[T]he journals are full of papers containing highly questionable data,
as readers can verify simply by perusing the examples in nearly any
syntax article about a familiar language. (Wasow and Arnold 2005:
1484)</p>
</blockquote>

<p>

It is a common Emergentist objection that linguistic intuitions 
(taken to be reports of elicited judgments of the acceptability of 
expressions not their grammaticality) are bad data points because not
only are they not usage data, i.e., they are metalinguistic, but also
because they are judgments about linguist&#8217;s invented example 
sentences. On neither count would they be clear and direct evidence 
of language use and human communicative capacities&#8212;the subject 
matter of linguistics on the Emergentist view. A further objection is
to their use by theorists to the exclusion of all other kinds of 
evidence. For example,</p>

<blockquote>
[Formal linguistics] continues to insist that its method for gathering
data is not only appropriate, but is superior to others. Occasionally
a syntactician will acknowledge that no one type of data is
privileged, but the actual behavior of people in the field belies this
concession. Take a look at any recent article on formal syntax and see
whether anything other than the theorist&#8217;s judgments constitute the
data on which the arguments are based. (Ferreira 2005: 372)
</blockquote>

<p>

&#8220;Formal&#8221; is Emergentist shorthand for referring to 
generative linguistics. And it should be noted that the practice by 
Essentialists of collapsing various kinds of acceptability judgments 
under the single label &#8216;intuitions&#8217; masks important 
differences. In principle there might be significant differences 
between the judgments of (i) linguists with a stake in what the 
evidence shows; (ii) linguists with experience in syntactic theory 
but no stake in the issue at hand; (iii) non-linguist native speakers
who have been tutored in how to provide the kinds of judgments the
linguist is interested in; and (iv) linguistically na&#239;ve native 
speakers.</p>

<p>

Many Emergentists object to all four kinds of reports of intuitions 
on the grounds that they are not direct evidence language use. For 
example, a common objection is based on the view that</p>

<blockquote>
[T]he primary object of study is the language people actually produce
and understand. Language in use is the best evidence we have for
determining the nature and specific organization of linguistic
systems. Thus, an ideal usage-based analysis is one that emerges from
observation of such bodies of usage data, called
corpora.&#8230; Because the linguistic system is so closely tied to
usage, it follows that theories of language should be grounded in an
observation of data from actual uses of language. (Barlow and Kemmer
2002, Introduction)
</blockquote>

<p>

But collections of linguists&#8217; reports of their own judgments are also
criticized by Emergentists as &#8220;arm-chair data 
collection,&#8221; or &#8220;data collection by introspection&#8221;.
All parties tend to call this kind of data collection 
&#8220;informal&#8221;&#8212;though they all rely on either formally 
or informally elicited judgments to some degree.</p>

<p>

On the other side, Essentialists tend to deny that usage data is 
adequate evidence by itself:</p>

<blockquote>
More than five decades of research in generative linguistics have
shown that the standard generative methodology of hypothesis formation
and empirical verification via judgment elicitation can lead to a
veritable goldmine of linguistic discovery and explanation. In many
cases it has yielded good, replicable results, ones that could not as
easily have been obtained by using other data-gathering methods, such
as corpus-based research&#8230;[C]onsider the fact that parasitic gap
constructions&#8230;are exceedingly rare in corpora&#8230;. [T]hese
distributional phenomena would have been entirely impossible to
distill via any non-introspective, non-elicitation based data
gathering method. Corpus data simply cannot yield such a detailed
picture of what is licit and, more crucially, what is not licit for a
particular construction in a particular linguistic environment. (den
Dikken et al. 2007: 336)
</blockquote>

<p>

And Essentialists often seem to deny that they are guilty of what the
Emergentist claims they are guilty of. For example, Chomsky appears 
to be claiming that acceptability judgments are performance data, 
i.e. evidence of use:</p>

<blockquote>
Acceptability is a concept that belongs to the study of performance,
whereas grammaticalness belongs to the study of competence&#8230;
Like acceptability, grammaticalness is, no doubt, a matter of
degree&#8230;but the scales of grammaticalness and acceptability do
not coincide. Grammaticalness is only one of many factors that
interact to determine acceptability. (Chomsky 1965: 11)
</blockquote>

<p>

Chomsky means to deny that acceptability judgments are direct 
evidence of <em>linguistic competence</em>. But it does not follow 
that elicited acceptability judgments are direct evidence of language
use.</p>

<p>

And as for the charge of &#8220;arm-chair&#8221; collection methods, 
some Essentialists claim to have shown that such methods are as good 
as more controlled experimental methods. For example, Sprouse and 
Almeida report:</p>

<blockquote>
  We formally tested all 469 data points from a popular generative
  syntax textbook (Adger 2003) on 440 na&#239;ve participants. Using
  three different statistical analysis approaches (traditional
  statistical tests, linear mixed-effects models, and Bayes factor
  analysis), and adopting the assumption of critics that formal
  results are more &#8216;true&#8217; than informal judgments, we
  estimated a maximum replication failure rate of 2% for the 469 data
  points in Adger (2003)&#8230;A replication rate of 98% suggests
  that the empirical foundation of generative syntactic theory is
  sound, at least for the topics covered in Adger (2003).  (p. 13 of
  Sprouse and Almeida, ms 2010: see Other Internet Resources)
</blockquote>

<p>

(When they say &#8220;formal results&#8221; they apparently mean 
&#8220;results obtained by controlled experiments&#8221;.) This can 
be read as either defending Essentialists&#8217; consulting of their own 
intuitions simpliciter, or their self-consultation of intuitions on 
uncontroversial textbook cases only. The former is much more 
controversial than the later.</p>

<p>

Finally, both parties of the debate engage in ad hominem attacks on 
their opponents. Here is one example of a classic ad hominem (tu 
quoque) attack on Emergentists in defense of constructed examples by 
Essentialists:</p>

<blockquote>
[The charge made concerning &#8220;armchair data collection&#8221;]
implies that there is something intrinsic to generative grammar that
invites partisans of that framework to construct syntactic theories on
the evidence of a single person&#8217;s judgments. Nothing could be farther
from the truth. The great bulk of publications in cognitive and
functional linguistics follow the same practice. Of
course, <em>rhetorically</em> many of the latter decry the use of
linguists&#8217; own intuitions as data. For example, in &#8230; an
important collections [sic] of papers in cognitive-functional
linguistics, &#8230; only two contributors to the volume
&#8230; present segments of natural discourse, neither filling even a
page of text. All of the other contributors employ examples
constructed by the linguists themselves. It is quite difficult to
find <em>any</em> work in cognitive linguistics (and functional
linguists are only slightly better) that uses multiple informants. It
seems almost disingenuous &#8230; to fault generativists for what (for
better or worse) is standard practice in the field, regardless of
theoretical allegiance. (Newmeyer 2007: 395)
</blockquote>

<p>

Clearly, the mere fact that some Emergentists may in practice have 
made use of invented examples in testing their theories does not tell
against any cogent general objections they may have offered to such 
practice. What is needed is a decision on the methodological point, 
not just a cry of &#8220;You did it too!&#8221;.</p>

<p>

Given the intolerance of each other&#8217;s views, and the crosstalk 
present in these debates, it is tempting to think that Emergentism 
and Essentialism are fundamentally incompatible on what counts as 
linguistic data, since their differences are based on their different
views of the subject matter of linguistics, and what the phenomena 
and goals of linguistic theorizing are. There is no doubt that the 
opposing sides think that their respective views are incompatible. 
But this conclusion may well be too hasty. In what follows, we try to
point to a way that the dispute could be ameliorated, if not 
adjudicated.</p>

<h3><a name="GraAccJud">3.2  Grammaticality and acceptability judgments</a></h3>

<p>

Essentialists who accept the competence/performance distinction of 
Chomsky (1965) traditionally emphasize elicited acceptability 
judgment data (although they need not reject data that is gathered 
using other methods). But as Cowart notes:</p>

<blockquote>
In this view, which exploits the distinction between competence and
performance, the act of expressing a judgment of acceptability is a
kind of linguistic performance. The grammar that a [generative
Essentialist] linguistic theory posits in the head of a speaker does
not exercise exhaustive control of judgments&#8230; While forming a
sentence judgment, a speaker draws on a variety of cognitive
resources&#8230; The resulting [acceptability] judgments could
pattern quite differently than the grammaticality values we might like
them to reflect. (Cowart 1997: 7)
</blockquote>

<p>

The grammaticality of an expression, on the standard generative 
Essentialist view, is the status conferred on it by the competence 
state of an ideal speaker. But competence can never be exercised or 
used without potentially interfering performance factors like memory 
being exercised as well. This means that judgments about 
grammaticality are never really directly available to the linguist 
through informant judgments: they have to be inferred from judgments 
of acceptability (along with any other relevant evidence). 
Nevertheless, Essentialists do take acceptability judgments to 
provide fairly good evidence concerning the character of linguistic 
competence. In fact the use of informally gathered acceptability 
judgment data is a hallmark of post-1965 Essentialist practice.</p>

<p>

It would be a mistake, however, to suppose that only Essentialists 
make use of such judgments. Many contemporary Externalists and 
Emergentists who reject the competence/performance distinction still 
use informally gathered acceptability judgments in linguistic 
theorizing, though perhaps not in theory testing. Emergentists tend 
to interpret experimentally gathered judgment data as performance 
data reflecting the interactions between learned features of 
communication systems and general learning mechanisms as deployed in 
communication. And Externalists use judgment data for corpus cleaning
(see below).</p>

<p>

It should be noted that sociolinguists and anthropological linguists 
(and we regard them as tending toward Emergentist views) often 
informally elicit informant judgments not only about acceptability 
but also about social and regional style and variation, and meaning. 
They may ask informants questions like, &#8220;Who would typically 
say that?&#8221;, or &#8220;What does X mean in context XYZ?&#8221;, 
or &#8220;If you can say WXY, can you say WXZ?&#8221; (see Labov 
1996: 77).</p>

<h3><a name="AssDegAcc">3.3  Assessing degrees of acceptability</a></h3>

<p>

A generative grammar gives a finite specification of a set of 
expressions. A psychogrammar, to the extent that it corresponds to a 
generative grammar, might be thought to equip a speaker to know (at 
least in principle) absolutely whether a string is in the language. 
However, elicited metalinguistic judgments are uncontroversially a 
matter of degree. A question arises concerning the scale on which 
these degrees of acceptability should be measured.</p>

<p>

Linguists have implicitly worked with a scale of roughly half a dozen
levels and types of acceptability, annotating them with prefixed 
symbols. The most familiar is the asterisk, originally used simply to
mark strings of words as ungrammatical, i.e., as not belonging to the
language at all. Other prefixed marks have gradually become current:</p>

<table class="hrules vert-top cellpad-small avoid-break centered">
<tr>
 <td><span class="sc">prefix</span></td>
 <td><span class="sc">approximate meaning</span></td>
</tr>

<tr>
 <td>#</td>
 <td>semantically anomalous: unacceptable in virtue of a bizarre meaning</td>
</tr>

<tr>
 <td>%</td>
 <td>subject to a &#8216;dialect&#8217; split: judged grammatical only by some speakers</td>
</tr>
</table>

<p>

But other annotations have been used to indicate a gradation in the 
extent to which some sentences are unacceptable. No scientifically 
validated or explicitly agreed meanings have been associated with 
these marks, but a tradition has slowly grown up of assigning 
prefixes such as those in Table 2 to signify degrees of 
unacceptability:</p>

<div class="figure wide">
<table class="hrules vert-top cellpad-small avoid-break centered">
<tr>
 <td><span class="sc">prefix</span></td>
 <td><span class="sc">approximate meaning</span></td>
</tr>
<tr>
 <td><span class="nw">(no prefix)</span></td>
 <td>acceptable and thus presumably grammatical</td>
</tr>
<tr>
 <td>?</td>
 <td>of dubious acceptability, though probably grammatical</td>
</tr>
<tr>
 <td>??</td>
 <td>clearly unacceptable but possibly grammatical</td>
</tr>
<tr>
 <td>?*</td>
 <td>unacceptable enough to suggest probable ungrammaticality</td>
</tr>
<tr>
 <td>*</td>
 <td>unacceptable enough to suggest clear ungrammaticality</td>
</tr>
<tr>
 <td>**</td>
 <td>grossly unacceptable, suggesting extreme ungrammaticality</td>
</tr>
</table>
<p class="center"><span class="figlabel">Table 2:</span> Prefixes used to mark levels of acceptability</p>
</div>

<p>

Such markings are often used in a way that suggests an 
<strong>ordinal scale</strong>, i.e. a partial ordering that is 
silent on anything other than equivalence in acceptability or ranking
in degree of unacceptability.</p>

<p>

By contrast, Bard et al. (1996: 39) point out, it is possible to use 
<strong>interval scales</strong>, which additionally measure distance
between ordinal positions. Interval scales of acceptability would 
measure <em>relative distances</em> between strings&#8212;how much 
more or less acceptable one is than another. <strong>Magnitude 
estimation</strong> is a method developed in psychophysics to measure
subjects&#8217; judgments of physical stimuli on an interval scale. Bard et
al. (1996) adapted these methods to linguistic acceptability 
judgments, arguing that interval scales of measurement are required 
for testing theoretical claims that rely on subtle judgments of 
comparative acceptability. An ordinal scale of acceptability can 
represent one expression as being less acceptable than another, but 
cannot support quantitative questions about how much less. Many 
generative Essentialist theorists had been suggesting that violation 
of different universal principles led to different degrees of 
unacceptability. According to Bard et al. (34&#8211;35), because 
there may be &#8220;disproportion between the fineness of judgments 
people can make and the symbol set available for recording 
them&#8221; it will not suffice to use some fixed scale such as this 
one:</p>

<p class="center">
  ? &lt; ?? &lt; ?* &lt; * &lt; **
</p>

<p>

indicating absolute degrees of unacceptability. Degrees of relative 
unacceptability must be measured. This is done by asking the 
informant how much less acceptable one string is than another.</p>

<p>

Magnitude estimation can be used with both informal and experimental 
methods of data collection. And data that is measured using interval 
scales can be subjected to much more mathematically sophisticated 
tests and analyses than data measured solely by an ordinal scale, 
provided that quantitative data are available.</p>

<p>

It should be noted that the value of applying magnitude estimation to
the judgment of acceptability has been directly challenged in two 
recent papers. Weskott and Fanselow (2011) and Sprouse (2011) both 
present critiques of Bard et al. (1996). Weskott and Fanselow 
compared magnitude estimation data to standard judgments on binary 
and 7-point scales, and claim that magnitude estimation does not 
yield more information than other judgment tasks, and moreover can 
produce spurious variance. And Sprouse, on the basis of recent 
formalizations of magnitude estimation in the psychophysics 
literature, presents experimental evidence that participants cannot 
make ratio judgments of acceptability (for example, a judgment that 
one sentence is precisely half as acceptable as another), which 
suggests that the magnitude estimation task probably provides the 
same interval-level data as other judgment tasks.</p>

<h3><a name="InfExpEli">3.4  Informal and experimental elicitation</a></h3>

<p>

Part of the dispute over the reliability of informal methods of 
acceptability judgment elicitation and collection is between 
different groups of Essentialists. Experimentally trained 
psycholinguists advocate using and adapting various experimental 
methods that have been developed in the cognitive and behavioral 
sciences to collect acceptability judgments. And while the debate is 
often cast in terms of which method is absolutely better, a more 
appropriate question might be when one method is to be preferred to 
the others. Those inclined toward less experimentally controlled 
methods point out that there are many clear and uncontroversial 
acceptability judgments that do not need to be shown to be reliable. 
Advocates of experimental methods point out that many purportedly 
clear, uncontroversial judgments have turned out to be unreliable, 
and led to false empirical generalizations about languages. Both seem
to be right in different cases.</p>

<p>

Chomsky has frequently stated his view that the experimental data-gathering 
techniques developed in the behavioral sciences are neither used nor 
needed in linguistic theorizing.  For example:</p>

<blockquote>
The gathering of data is informal; there has been little use of
experimental approaches (outside of phonetics) or of complex
techniques of data collection and data analysis of a sort that can
easily be devised, and that are widely used in the behavioral
sciences. The arguments in favor of this informal procedure seem to me
quite compelling; basically, they turn on the realization that for the
theoretical problems that seem most critical today, it is not at all
difficult to obtain a mass of crucial data without use of such
techniques. Consequently, linguistic work, at what I believe to be its
best, lacks many of the features of the behavioral sciences. (Chomsky
1969: 56)
</blockquote>

<p>

He also expressed the opinion that using experimental behavioral data
collection methods in linguistics &#8220;would be a waste of time and
energy&#8221; (1969: 81).</p>

<p>

Although many Emergentists&#8212;the intellectual heirs of 
Sapir&#8212;would accept &#8216;ask-the-informant&#8217; data, we 
might expect them to tend to accept experimental data-gathering 
methods that have been developed in the social sciences. There is 
little doubt that strict followers of the methodology preferred by 
Bloomfield in his later career would disapprove of &#8216;ask the 
informant&#8217; methods. Charles Hockett remarked:</p>

<blockquote>
A language, as a set of habits, is a fragile thing, subject to minor
modification in the slightest breeze of circumstance; this, indeed, is
its great source of power. But this is also why the
transformationalists (like the rest of us!), using themselves as
informants, have such a hard time deciding whether certain candidates
for sentencehood are really &#8216;in their dialect&#8217; or not; and
it is why Bloomfield, in his field work, would never elicit paradigms,
for fear he would induce his informant to say something under the
artificial conditions of talking with an outsider that he would never
have said in his own everyday surroundings. (Hockett 1968:
89&#8211;90, fn. 31)
</blockquote>

<p>

We might expect Bloomfield, having abandoned his earlier Wundtian 
psychological leanings, to be suspicious of any method that could be 
cast as introspective. And we might expect many contemporary 
Externalists to prefer more experimentally controlled methods too. 
(We shall see below that to some extent they do.)</p>

<p>

Derwing (1973) was one early critic of Chomsky&#8217;s view (1969) that 
experimentally controlled data collection is useless; but it was 
nearly 25 years before systematic research into possible confounding 
variables in acceptability judgment data started being conducted on 
any significant scale. In the same year that Bard et al. (1996) 
appeared, Carson Sch&#252;tze (1996) published a monograph with the 
following goal statement:</p>

<blockquote>
I aim to demonstrate&#8230;that grammaticality judgments and other
sorts of linguistic intuition, while indispensible forms of data for
linguistic theory, require new ways of being collected and used. A
great deal is known about the instability and unreliability of
judgments, but rather than propose that they be abandoned, I endeavor
to explain the source of their shiftiness and how it can be
minimized. (1996: 1)
</blockquote>

<p>

In a similar vein, Wayne Cowart stated that he wanted to 
&#8220;describe a family of practical methods that yield demonstrably
reliable data on patterns of sentence acceptability.&#8221; He 
observes that the stability and reliability of acceptability judgment
collection is</p>

<blockquote>
complicated by the fact that there seems to be no consensus on how to
gather judgments apart from a widespread tolerance for informal
methods in which the linguist consults her own intuitions and those of
the first handy informant (what we might call the &#8220;Hey,
Sally&#8221; method). (Cowart 1997: 2)
</blockquote>

<p>

Sch&#252;tze also expresses the importance of using experimental 
methods developed in cognitive science:</p>

<blockquote>
[M]y claim is that <em>none</em> of the variables that confound
metalinguistic data are peculiar to judgments about language. Rather
they can be shown to operate in some other domain in a similar
way. (This is quite similar to Valian&#8217;s (1982) claim that the data of
more traditional psychological experiments have all the same problems
that judgment data have.) (Sch&#252;tze 1996: 14)
</blockquote>

<p>

The above can be read as sympathetic to the Essentialist preference 
for elicited judgments.</p>

<p>

Among the findings of Sch&#252;tze and Cowart about informal judgment
collection methods are these:</p>

<ul class="jfy">

<li>There is really no agreement in linguistics on what counts as an
  informal method (though see Sprouse and Almeida 2010 (Other Internet
  Resources) for a flat contradiction of this claim).</li>

<li>The collection of acceptability judgment data is just as
  vulnerable to the influence of extraneous variables as are other
  kinds of psychological data.</li>

<li>Judgment samples can be biased in informal judgment
  collection.</li>

<li>Experimenter bias is often not controlled for in informal
  judgment collection.</li>

<li>Judgment materials are often not carefully prepared to present a
  relevant, well-ordered, contrasting set of minimal pairs.</li>

<li>The instability of one-off speaker judgments can be controlled
  for by gathering judgments from a given speaker across time.</li>
</ul>

<p>

Although Sch&#252;tze (1996) and Cowart (1997) are both critical of 
traditional Essentialist informal elicitation methods, their primary 
concern is to show how the claims of Essentialist linguistics can be 
made less vulnerable to legitimate complaints about informal data 
collection methods. Broadly speaking, they are friends of 
Essentialism. Critics of Essentialism have raised similar concerns in
less friendly terms, but it is important to note that the debate over
the reliability of informal methods is a debate within Essentialist 
linguistics as well.</p>

<h3><a name="WhaInfMetAct">3.5  What informal methods actually are</a></h3>

<p>

Informal methods of acceptability judgment data have often been 
described as excessively casual. Ferreira described the informal 
method this way:</p>

<blockquote>
An example sentence that is predicted to be ungrammatical is
contrasted with some other sentence that is supposed to be similar in
all relevant ways; these two sentences constitute a &#8220;minimal
pair&#8221;. The author of the article provides the judgment that the
sentence hypothesized to be bad is in fact ungrammatical, as indicated
by the star annotating the example. But there are serious problems
with this methodology. The example that is tested could have
idiosyncratic properties due to its unique lexical
content. Occasionally a second or third minimal pair is provided, but
no attempt is made to consider the range of relevant extraneous
variables that must be accounted for and held constant to make sure
there isn&#8217;t some correlated property that is responsible for the
contrast in judgments. Even worse, the &#8220;subject&#8221; who
provides the data is not a na&#239;ve informant, but is in fact the
theorist himself or herself, and that person has a stake in whether
the sentence is judged grammatical or ungrammatical. That is, the
person&#8217;s theory would be falsified if the prediction were wrong, and
this is a potential source of bias. (Ferreira 2005: 372)
</blockquote>

<p>

(It would be appropriate to read &#8216;grammatical&#8217; and 
&#8216;grammaticality&#8217; in Ferreira&#8217;s text as meaning 
&#8216;acceptable&#8217; and &#8216;acceptability&#8217;.)</p>

<p>

This critical characterization exemplifies the kind of method that 
Sch&#252;tze and Cowart aimed to improve on. More recently, Gibson 
and Fedorenko describe the traditional informal method this way:</p>

<blockquote>
As has often been noted in recent years (Cowart, 1997; Edelman &amp;
Christiansen, 2003; Featherston, 2007; Ferreira, 2005; Gibson &amp;
Fedorenko, 2010a; Marantz, 2005; Myers, 2009; Sch&#252;tze, 1996;
Wasow &amp; Arnold, 2005), the results obtained using this method are
not necessarily generalisable because of (a) the small number of
experimental participants (typically one); (b) the small number of
experimental stimuli (typically one); (c) cognitive biases on the part
of the researcher and participants; and (d) the effect of the
preceding context (e.g., other constructions the researcher may have
been recently considering). (Gibson and Fedorenko, 2013)
</blockquote>

<p>

While some Essentialists have acknowledged these problems with the 
reliability of informal methods, others have, in effect, denied their
relevance. For example, Colin Phillips (2010) argues that 
&#8220;there is little evidence for the frequent claim that sloppy 
data-collection practices have harmed the development of linguistic 
theories&#8221;. He admits that not all is epistemologically well in 
syntactic theory, but adds, &#8220;I just don&#8217;t think that the 
problems will be solved by a few rating surveys.&#8221; He concludes:</p>

<blockquote>
I do not think that we should be fooled into thinking that informal
judgment gathering is the root of the problem or that more formalized
judgment collection will solve the problems. (Phillips 2010: 61)
</blockquote>

<p>

To suggest that informal methods are as fully reliable as controlled 
experimental ones would be a serious charge, implying that 
researchers like Bard, Robinson, Sorace, Cowart, Sch&#252;tze, 
Gibson, Fedorenko, and others have been wasting their time. But 
Phillips actually seems to be making a different claim. He suggests 
first that informally gathered data has not actually harmed 
linguistics, and second that linguists are in danger of being 
&#8220;fooled&#8221; by critics who invent stories about unreliable 
data having harmed linguistics.</p>

<p>

The harm that Phillips claims has not occurred relates to the charge 
that &#8220;mainstream linguistics&#8221; (he means the current 
generative Essentialist framework called &#8216;Minimalism&#8217;) is
&#8220;irrelevant&#8221; to broader interests in the cognitive 
sciences, and has lost &#8220;the initiative in language 
study&#8221;. Of course, Phillips is right in a sense: one cannot 
insure that experimental judgment collection methods will address 
every way in which Minimalist theorizing is irrelevant to particular 
endeavors (language description, language teaching, natural language 
processing, or broader questions in cognitive psychological 
research). But this claim does not bear on what Sch&#252;tze (1996) 
and Cowart (1997) show about the unreliability of informal methods.</p>

<p>

Phillips does not fully accept the view of Chomsky (1969) that 
experimental methods are useless for data gathering (he says, 
&#8220;I do not mean to argue that comprehensive data gathering 
studies of acceptability are worthless&#8221;). But his defense of 
informal methods of data collection rests on whether these methods 
have damaged Essentialist theory testing:</p>

<blockquote>
  <p>

The critiques I have read present no evidence of the supposed damage 
that informal intuitions have caused, and among those who do provide 
specific examples it is rare to provide clear evidence of the 
supposed damage that informal intuitions have caused&#8230;</p>

  <p>

What I am specifically questioning is whether informal (and 
occasionally careless) gathering of acceptability judgments has 
actually held back progress in linguistics, and whether more careful 
gathering of acceptability judgments will provide the key to future 
progress.</p>
</blockquote>

<p>

Either Phillips is fronting the surprising opinion that generative 
theorizing has never been led down the wrong track by demonstrably 
unreliable data, or he is changing the subject. And unless clear 
criteria are established for what counts as &#8220;damage&#8221; and 
&#8220;holding back,&#8221; Phillips is not offering any testable 
hypothesis about data collection methodology. For example, Phillips 
discounts the observation of Sch&#252;tze (1996) that conflicting 
judgments of relative unacceptability of violations of two linguistic
universals held back the development of Government and Binding (GB), 
on the grounds that two sets of conflicting judgments and their 
analyses &#8220;are now largely forgotten, supplanted by theories 
that have little to say about such examples.&#8221; But the fact that
the proposed universals are discarded principles of UG is irrelevant 
to the effect that unreliable data once had on the (now largely 
abandoned) GB theory. A methodological concern cannot be dismissed on
the basis of a move to a new theory that abandons the old theory but 
not its methods!</p>

<p>

More recently, Bresnan (2007) claims that many theoretical claims 
have arguably been supported by unreliable informally gathered 
syntactic acceptability judgments. She observes:</p>

<blockquote>
Erroneous generalizations based on linguistic intuitions about
isolated, constructed examples occur throughout all parts of the
grammar. They often seriously underestimate the space of grammatical
possibility (Taylor 1994, 1996, Bresnan &amp; Nikitina 2003, Fellbaum
2005, L&#248;drup 2006, among others), reflect relative frequency
instead of categorical grammaticality (Labov 1996, Lapata 1999,
Manning 2003), overlook complex constraint interactions (Green 1971,
Gries 2003) and processing effects (Arnon et al. 2005a, b), and fail
to address the problems of investigator bias (Labov 1975, Naro 1980,
Chambers 2003: 34) and social intervention (Labov 1996, Milroy 2001,
Cornips &amp; Poletto 2005).  (Bresnan 2007: 301)
</blockquote>

<p>

Her discussion supports the view that various highly abstract 
theoretical hypotheses have been defended through the use of 
generalizations based on unreliable data.</p>

<p>

The debate over the harm that the acceptance of informally collected 
data has had on theory testing is somewhat difficult to understand 
for Essentialist, Externalist, and Emergentist researchers who have 
been trained in the methods of the cognitive and behavioral sciences.
Why try to support one&#8217;s theories of universal grammar, or of the 
grammars of particular languages, by using questionably reliable 
data?</p>

<p>

One clue might be found in Culicover and Jackendoff (2010), who 
write:</p>

<blockquote>
[T]heoreticians&#8217; subjective judgments are essential in formulating
linguistic theories. It would cripple linguistic investigation if it
were required that all judgments of ambiguity and grammaticality be
subject to statistically rigorous experiments on naive
subjects. (Culicover and Jackendoff 2010)
</blockquote>

<p>

The worry is that use of experimental methods is so resource
consumptive that it would impede the formulation of linguistic
theories. But this changes the subject from the importance of using
reliable data as evidence in theory <em>testing</em> to using only
experimentally gathered data in theory <em>formulation</em>. We are
not aware of anyone who has ever suggested that at the stage of
hypothesis development or theory formulation the linguist should
eschew intuition. Certainly Bard et al., Sch&#252;tze, Cowart, Gibson
&amp; Fedorenko, and Ferreira say no such thing. The relevant issue
concerns what data should be used to <em>test</em> theories, which is
a very different matter.</p>

<p>

We noted earlier that there are clear and uncontroversial 
acceptability judgments, and that these judgments are reliable data. 
The difficulty lies in distinguishing the clear, uncontroversial, and
reliable data from what only appears to be clear, uncontroversial, 
and reliable to a research community at a time. William Labov, the 
founder of modern quantitative sociolinguistics, who takes an 
Emergentist approach, proposed a set of working methodological 
principles in Labov (1975) for adjudicating when experimental methods
should be employed.</p>

<blockquote>
  <p>

<strong>The Consensus Principle</strong>: If there is no reason to 
think otherwise, assume that the judgments of any native speaker are 
characteristic of all speakers.</p>

  <p>

<strong>The Experimenter Principle</strong>: If there is any 
disagreement on introspective judgments, the judgments of those who 
are familiar with the theoretical issues may not be counted as 
evidence.</p>

  <p>

<strong>The Clear Case Principle</strong>: Disputed judgments should 
be shown to include at least one consistent pattern in the speech 
community or be abandoned. If differing judgments are said to 
represent different dialects, enough investigation of each dialect 
should be carried out to show that each judgment is a clear case in 
that dialect. (Labov 1975, quoted in Sch&#252;tze 1996: 200)</p>
</blockquote>

<p>

If we accept that &#8216;introspective judgments&#8217; are 
acceptability judgments, then Labov&#8217;s rules of thumb are guides for 
when to deploy experimental methods, although they no doubt need 
refinement. However, it seems vastly more likely that careful 
development of such methodological rules of thumb can serve to 
improve the reliability of linguistic data and adjudicate these 
methodological disputes that seem largely independent of any 
particular approach to linguistics.</p>

<h3><a name="CorDat">3.6  Corpus data</a></h3>

<p>

In linguistics, the goal of collecting corpus data is to identify and
organize a representative sample of a written and/or spoken variety 
from which characteristics of the entire variety or genre can be 
induced. Concordances of word usage in linguistic context have long 
been used to aid in the translation and interpretation of literary 
and sacred texts of particular authors (e.g. Plato, Aristotle, 
Aquinas) and of particular texts (e.g. the Torah, the rest of the Old
Testament, the Gospels, the Epistles). Formal textual criticism, the 
identification of antecedently existing oral traditions that were 
later redacted into Biblical texts, and author identification (e.g. 
figuring out which of the Epistles were written by Paul and which 
were probably not) began to develop in the late 19th century.</p>

<p>

The development of computational methods for collecting, analyzing, 
and searching corpora have seen rapid development as computer memory 
has become less expensive and search and analysis programs have 
become faster. The first computer searchable corpus of American 
English, the Brown Corpus, developed in the 1960s, contained just 
over one million word tokens. The British National Corpus (BNC) is a 
balanced corpus containing over 100 million words&#8212;a hundredfold
size increase&#8212;of which 90% is written prose published from 1991
to 1994 and 10% is spoken English. Between 2005 and 2007, 
billion-word corpora were released for British English (ukWaC), 
German (deWaC), and Italian (itWaC)&#8212;a thousand times bigger 
than the Brown corpus. And the entire World Wide Web probably holds 
about a thousand times as much as that&#8212;around a trillion words.
Thus corpus linguistics has gone from megabytes of data (&#8764; 
10<sup>3</sup>kB) to terabytes of data (&#8764; 10<sup>9</sup>kB) in 
fifty years.</p>

<p>

Just as a central issue concerning acceptability judgment data 
concerns its reliability as evidence for empirical generalizations 
about languages or idiolects, a central question concerning the 
collection of corpus data concerns whether or not it is 
representative of the language variety it purports to represent. Some
linguists make the criterion of representativeness definitional: they
call a collection of samples of language use a corpus only if it has 
been carefully balanced between different genres (conversation, 
informal writing, journalism, literature, etc.), regional varieties, 
or whatever.</p>

<p>

But corpora are of many different kinds. Some are just very large
compilations of text from individual sources such as newspapers of
record or the World Wide Web&#8212;compilations large enough for the
diversity in the source to act as a surrogate for representativeness.
For example, a billion words of a newspaper, despite coming from a
single source, will include not only journalists&#8217; news reports and
prepared editorials but also quoted speech, political rhetoric, humor
columns, light features, theater and film reviews, readers&#8217; letters,
fiction items, and so on, and will thus provide examples of a much
wider variety of styles than one might have thought.</p>

<p>

Corpora are cleaned up through automatic or manual removal of such 
elements as numerical tables, typographical slips, spelling mistakes,
markup tags, accidental repetitions (<i>the the</i>), larger-scale 
duplications (e.g., copies on mirror sites), boilerplate text 
(<i>Opinions expressed in this email do not necessarily 
reflect</i>&#8230;), and so on (see Baroni et al. 2009 for a fuller 
discussion of corpus cleaning).</p>

<p>

The entire web itself can be used as a corpus to some degree, despite
its constantly changing content, its multilinguality, its many tables
and images, and its total lack of quality control; but when it is, the
outputs of searches are nearly always cleaned by disregarding unwanted
results. For example, Google searches are blind to punctuation,
capitalization, and sentence boundaries, so search results for <i>to
be</i> will unfortunately include irrelevant cases, such as where a
sentence like <i>Do you want to?</i> happens to be followed by a
sentence like <i>Be careful</i>.</p>

<p>

Corpora can be annotated in ways that permit certain kinds of 
analysis and grammar testing. One basic kind of annotation is 
part-of-speech tagging, in which each word is labeled with its 
syntactic category. Another is lemmatization, which classifies the 
different morphologically inflected forms of a word as belonging 
together (<i>goes</i>, <i>gone</i>, <i>going</i>, and <i>went</i> 
belong with <i>go</i>, for example). A more thoroughgoing kind of 
annotation involves adding markup that encodes trees representing 
their structure; an example like <i>That road leads to the 
freeway</i> might be marked up as a Clause within which the first two
words make up a Noun Phrase (NP), the last four constitute a Verb 
Phrase (VP), and so on, giving a structural analysis represented 
thus:</p>

<div class="figure">
  <img src="https://plato.stanford.edu/entries/linguistics/clause.png" class="nocaption" alt="Structural analysis of 'That road leads to the highway'">
</div>

<p>

Such a diagram is isomorphic to (and the one shown was computed 
directly from) a labeled bracketing like this:</p>

<p class="indent monospace smaller">
(.Clause. (.NP. (.D. &#8216;that&#8217; )
  (.N. &#8216;road&#8217; ) ) (.VP. (.V. &#8216;leads&#8217; )
  (.PP. (.P. &#8216;to&#8217; ) (.NP. (.D. &#8216;the&#8217; )
  (.N. &#8216;freeway&#8217; ) ) ) ) )
</p>

<p>

and this in turn could be represented in a markup language like XML 
as:</p>

<blockquote>
  <pre>
&lt;clause&gt;
  &lt;nounphrase&gt;
    &lt;determiner&gt;that&lt;/determiner&gt;
    &lt;noun&gt;road&lt;/noun&gt;
  &lt;/nounphrase&gt;
  &lt;verbphrase&gt;
    &lt;verb&gt;leads&lt;/verb&gt;
    &lt;prepphrase&gt;
      &lt;prep&gt;to&lt;/prep&gt;
      &lt;nounphrase&gt;
        &lt;determiner&gt;the&lt;/determiner&gt;
        &lt;noun&gt;freeway&lt;/noun&gt;
      &lt;/nounphrase&gt;
    &lt;/prepphrase&gt;
  &lt;/verbphrase&gt;
&lt;/clause&gt;
</pre>
</blockquote>

<p>

A corpus annotated with tree structure is known as a 
<strong>treebank</strong>. Clearly, such a corpus is not a raw record
of attested utterances at all; it is a combination of a collection of
attested utterances together with a systematic attempt at analysing
their structure. Whether the analysis is added manually or
semi-automatically, it is ultimately based on native speaker
judgments. (Treebanks are often developed by graduate student
annotators tutored by computational linguists; naturally, consistency
between annotators is an issue that needs regular attention. See
Artstein and Poesio, 2008, for discussion of the methodological
issues.).</p>

<p>

One of the purposes of a treebank is to permit the further 
investigation of a language and the checking of further linguistic 
hypotheses by searching a large database of previously established 
analyses. It can also be used to test grammars, natural language 
processing systems, or machine learning programs.</p>

<p>

Going beyond syntactic parse trees, it is possible to annotate 
corpora further, with information of a semantic and pragmatic nature.
There is ongoing computational linguistic research aimed at 
discovering whether, for example, semantic annotation that is 
semi-automatically added might suffice for recognition of whether a 
product review is positive or negative (what computational linguists 
call &#8216;sentiment analysis&#8217;).</p>

<p>

Notice, then, that using corpus data does not mean abandoning or 
escaping from the use of intuitions about acceptability or 
grammatical structure: the results of a corpus search are generally 
filtered through the judgments of an investigator who decides which 
pieces of corpus data are to be taken at face value and which are 
just bad hits or irrelevant noise.</p>

<p>

Difficult methodological issues arise in connection with the 
collection, annotation, and use of corpus data. For example, there is
the issue of extremely rare expression tokens. Are they accurately 
recorded tokens of expression types that turn up only in consequence 
of sporadic errors and should be dismissed as irrelevant unless the 
topic of interest is performance errors? Are they due to errors in 
the compilation of the corpus itself, corresponding to neither 
accepted usage nor sporadic speech errors? Or are they perfectly 
grammatical but (for some extraneous reason) very rare, at least in 
that particular corpus?</p>

<p>

Many questions arise about what kind of corpus is best suited to the 
research questions under consideration, as well as what kind of 
annotation is most appropriate. For example, as Ferreira (2005: 375) 
points out, some large corpora, insofar as they have <i>not</i> been 
cleaned of speech errors, provide relevant data for studying the 
distribution of speech disfluencies. In addition, probabilistic 
information about the relation between a particular verb and its 
arguments has been used to show that &#8220;verb-argument preferences
[are] an essential part of the process of sentence 
interpretation&#8221; (Roland and Jurafsky 2002: 325): acceptability 
judgments on individual expressions do not provide information about 
the distribution of a verb and its arguments in various kinds of 
speech and writing. Studying conveyed meaning in context and 
identification of speech acts will require a kind of data that 
decontextualized acceptability judgments do not provide but 
semantically annotated corpora might.</p>

<p>

Many Essentialists have been skeptical of the reliability of 
uncleaned, unanalyzed corpus data as evidence to support linguistic 
theorizing, because it is assumed to be replete with strings that any
native speaker would judge unacceptable. And many Emergentists and 
Externalists, as well as some Essentialists, have charged that 
informally gathered acceptability judgments can be highly unreliable 
too. Both worries are apposite; but the former does not hold for 
adequately cleaned and analyzed corpora, and the latter does not hold
for judgment data that has been gathered using appropriately 
controlled methods. In certain contested cases of acceptability, it 
will of course be important to use both corpus and controlled 
elicitation methods to cross-compare.</p>

<p>

Notice that we have not in any way suggested that our three broad 
approaches to linguistics should differ in the kinds of data they use
for theory testing: Essentialists are not limited to informal 
elicitation; nor are Emergentists and Externalists denied access to 
it. In matters of methodology, at least, there is in principle an 
open market&#8212;even if many linguists seem to think otherwise.</p>

<h2><a name="Who">4.  Whorfianism</a></h2>

<p>

Emergentists tend to follow Edward Sapir in taking an interest in 
interlinguistic and intralinguistic variation. Linguistic 
anthropologists have explicitly taken up the task of defending a 
famous claim associated with Sapir that connects linguistic variation
to differences in thinking and cognition more generally. The claim is
very often referred to as the <strong>Sapir-Whorf Hypothesis</strong>
(though this is a largely infelicitous label, as we shall see).</p>

<p>

This topic is closely related to various forms of 
relativism&#8212;epistemological, ontological, conceptual, and 
moral&#8212;and its general outlines are discussed elsewhere in this 
encyclopedia; see the section on language in the Summer 2015
archived version of the entry on  
 <a href="https://plato.stanford.edu/archives/sum2015/entries/relativism/index.html#3.1">relativism</a> (&#167;3.1). 
Cultural versions of moral relativism 
suggest that, given how much cultures differ, what is moral for you 
might depend on the culture you were brought up in. A somewhat 
analogous view would suggest that, given how much language structures
differ, what is thinkable for you might depend on the language you 
use. (This is actually a kind of conceptual relativism, but it is 
generally called linguistic relativism, and we will continue that 
practice.)</p>

<p>

Even a brief skim of the vast literature on the topic is not remotely
plausible in this article; and the primary literature is in any case 
more often polemical than enlightening. It certainly holds no general
answer to what science has discovered about the influences of 
language on thought. Here we offer just a limited discussion of the 
alleged hypothesis and the rhetoric used in discussing it, the vapid 
and not so vapid forms it takes, and the prospects for actually 
devising testable scientific hypotheses about the influence of 
language on thought.</p>

<p>

Whorf himself did not offer a hypothesis. He presented his &#8220;new
principle of linguistic relativity&#8221; (Whorf 1956: 214) as a fact
discovered by linguistic analysis:</p>

<blockquote>
When linguists became able to examine critically and scientifically a
large number of languages of widely different patterns, their base of
reference was expanded; they experienced an interruption of phenomena
hitherto held universal, and a whole new order of significances came
into their ken. It was found that the background linguistic system (in
other words, the grammar) of each language is not merely a reproducing
instrument for voicing ideas but rather is itself the shaper of ideas,
the program and guide for the individual&#8217;s mental activity, for his
analysis of impressions, for his synthesis of his mental stock in
trade. Formulation of ideas is not an independent process, strictly
rational in the old sense, but is part of a particular grammar, and
differs, from slightly to greatly, between different grammars. We
dissect nature along lines laid down by our native languages. The
categories and types that we isolate from the world of phenomena we do
not find there because they stare every observer in the face; on the
contrary, the world is presented in a kaleidoscopic flux of
impressions which has to be organized by our minds&#8212;and this
means largely by the linguistic systems in our minds. We cut nature
up, organize it into concepts, and ascribe significances as we do,
largely because we are parties to an agreement to organize it in this
way&#8212;an agreement that holds throughout our speech community and
is codified in the patterns of our language. The agreement is, of
course, an implicit and unstated one,
but <span style="font-variant:small-caps;">its terms are absolutely
obligatory</span>; we cannot talk at all except by subscribing to the
organization and classification of data which the agreement
decrees. (Whorf 1956: 212&#8211;214; emphasis in original)
</blockquote>

<p>

Later, Whorf&#8217;s speculations about the &#8220;sensuously and 
operationally different&#8221; character of different snow types for 
&#8220;an Eskimo&#8221; (Whorf 1956: 216) developed into a familiar 
journalistic meme about the Inuit having dozens or scores or hundreds
of words for snow; but few who repeat that urban legend recall 
Whorf&#8217;s emphasis on its being grammar, rather than lexicon, that cuts
up and organizes nature for us.</p>

<p>

In an article written in 1937, posthumously published in an academic 
journal (Whorf 1956: 87&#8211;101), Whorf clarifies what is most 
important about the effects of language on thought and world-view. He
distinguishes &#8216;phenotypes&#8217;, which are overt grammatical 
categories typically indicated by morphemic markers, from what he 
called &#8216;cryptotypes&#8217;, which are covert grammatical 
categories, marked only implicitly by distributional patterns in a 
language that are not immediately apparent. In English, the past 
tense would be an example of a phenotype (it is marked by the 
-<i>ed</i> suffix in all regular verbs). Gender in personal names and
common nouns would be an example of a cryptotype, not systematically 
marked by anything. In a cryptotype, &#8220;class membership of the 
word is not apparent until there is a question of using it or 
referring to it in one of these special types of sentence, and then 
we find that this word belongs to a class requiring some sort of 
distinctive treatment, which may even be the negative treatment of 
excluding that type of sentence&#8221; (p. 89).</p>

<p>

Whorf&#8217;s point is the familiar one that linguistic structure is 
comprised, in part, of distributional patterns in language use that 
are not explicitly marked. What follows from this, according to 
Whorf, is not that the existing lexemes in a language (like its words
for snow) comprise covert linguistic structure, but that patterns 
shared by word classes constitute linguistic structure. In 
&#8216;Language, mind, and reality&#8217; (1942; published 
posthumously in <em>Theosophist</em>, a magazine published in India 
for the followers of the 19th-century spiritualist Helena Blavatsky) 
he wrote:</p>

<blockquote>
Because of the systematic, configurative nature of higher mind, the
&#8220;patternment&#8221; aspect of language always overrides and
controls the &#8220;lexation&#8221;&#8230;or name-giving
aspect. Hence the meanings of specific words are less important than
we fondly fancy. Sentences, not words, are the essence of speech, just
as equations and functions, and not bare numbers, are the real meat of
mathematics. We are all mistaken in our common belief that any word
has an &#8220;exact meaning.&#8221; We have seen that the higher mind
deals in symbols that have no fixed reference to anything, but are
like blank checks, to be filled in as required, that stand for
&#8220;any value&#8221; of a given variable, like
&#8230;the <i>x</i>, <i>y</i>, <i>z</i> of algebra. (Whorf 1942: 258)
</blockquote>

<p>

Whorf apparently thought that only personal and proper names have an 
exact meaning or reference (Whorf 1956: 259).</p>

<p>

For Whorf, it was an unquestionable fact that language influences 
thought to some degree:</p>

<blockquote>
Actually, thinking is most mysterious, and by far the greatest light
upon it that we have is thrown by the study of language. This study
shows that the forms of a person&#8217;s thoughts are controlled by
inexorable laws of pattern of which he is unconscious. These patterns
are the unperceived intricate systematizations of his own
language&#8212;shown readily enough by a candid comparison and
contrast with other languages, especially those of a different
linguistic family. His thinking itself is in a language&#8212;in
English, in Sanskrit, in Chinese. [footnote omitted] And every
language is a vast pattern-system, different from others, in which are
culturally ordained the forms and categories by which the personality
not only communicates, but analyzes nature, notices or neglects types
of relationship and phenomena, channels his reasoning, and builds the
house of his consciousness. (Whorf 1956: 252)
</blockquote>

<p>

He seems to regard it as necessarily true that language affects 
thought, given</p>

<ol type="i">

<li>the fact that language must be used in order to think, and</li>

<li>the facts about language structure that linguistic analysis discovers.</li>
</ol>

<p>

He also seems to presume that the only structure and logic that
thought has is grammatical structure. These views are not the ones
that after Whorf&#8217;s death came to be known as &#8216;the Sapir-Whorf
Hypothesis&#8217; (a sobriquet due to Hoijer 1954). Nor are they what
was called the &#8216;Whorf thesis&#8217; by Brown and Lenneberg
(1954) which was concerned with the relation of obligatory lexical
distinctions and thought. Brown and Lenneberg (1954) investigated this
question by looking at the relation of color terminology in a language
and the classificatory abilities of the speakers of that language. The
issue of the relation between obligatory lexical distinctions and
thought is at the heart of what is now called &#8216;the Sapir-Whorf
Hypothesis&#8217; or &#8216;the Whorf Hypothesis&#8217; or
&#8216;Whorfianism&#8217;.</p>

<h3><a name="BanWho">4.1  Banal Whorfianism</a></h3>

<p>

No one is going to be impressed with a claim that some aspect of your
language may affect how you think in some way or other; that is 
neither a philosophical thesis nor a psychological hypothesis. So it 
is appropriate to set aside entirely the kind of so-called hypotheses
that Steven Pinker presents in <em>The Stuff of Thought</em> (2007: 
126&#8211;128) as &#8220;five banal versions of the Whorfian 
hypothesis&#8221;:</p>

<ul>

<li>&#8220;Language affects thought because we get much of our
  knowledge through reading and conversation.&#8221;</li>

<li>&#8220;A sentence can frame an event, affecting the way people
  construe it.&#8221;</li>

<li>&#8220;The stock of words in a language reflects the kinds of
  things its speakers deal with in their lives and hence think
  about.&#8221;</li>

<li>&#8220;[I]f one uses the word language in a loose way to refer
  to meanings,&#8230; then language is thought.&#8221;</li>

<li>&#8220;When people think about an entity, among the many
  attributes they can think about is its name.&#8221;</li>
</ul>

<p>

These are just truisms, unrelated to any serious issue about 
linguistic relativism.</p>

<p>

We should also set aside some methodological versions of linguistic 
relativism discussed in anthropology. It may be excellent advice to a
budding anthropologist to be aware of linguistic diversity, and to be
on the lookout for ways in which your language may affect your 
judgment of other cultures; but such advice does not constitute a 
hypothesis.</p>

<h3><a name="SoCalSapWhoHyp">4.2  The so-called Sapir-Whorf hypothesis</a></h3>

<p>

The term &#8220;Sapir-Whorf Hypothesis&#8221; was coined by Harry 
Hoijer in his contribution (Hoijer 1954) to a conference on the work 
of Benjamin Lee Whorf in 1953. But anyone looking in Hoijer&#8217;s paper 
for a clear statement of the hypothesis will look in vain. Curiously,
despite his stated intent &#8220;to review and clarify the 
Sapir-Whorf hypothesis&#8221; (1954: 93), Hoijer did not even attempt
to state it. The closest he came was this:</p>

<blockquote>
The central idea of the Sapir-Whorf hypothesis is that language
functions, not simply as a device for reporting experience, but also,
and more significantly, as a way of defining experience for its
speakers.
</blockquote>

<p>

The claim that &#8220;language functions&#8230;as a way of defining 
experience&#8221; appears to be offered as a kind of vague 
metaphysical insight rather than either a statement of linguistic 
relativism or a testable hypothesis.</p>

<p>

And if Hoijer seriously meant that what qualitative experiences a 
speaker <em>can</em> have are constituted by that speaker&#8217;s language,
then surely the claim is false. There is no reason to doubt that 
non-linguistic sentient creatures like cats can experience (for 
example) pain or heat or hunger, so having a language is not a 
necessary condition for having experiences. And it is surely not 
sufficient either: a robot with a sophisticated natural language 
processing capacity could be designed without the capacity for 
conscious experience.</p>

<p>

In short, it is a mystery what Hoijer meant by his &#8220;central 
idea&#8221;.</p>

<p>

Vague remarks of the same loosely metaphysical sort have continued to
be a feature of the literature down to the present. The statements 
made in some recent papers, even in respected refereed journals, 
contain non-sequiturs echoing some of the remarks of Sapir, Whorf, 
and Hoijer. And they come from both sides of the debate.</p>

<h3><a name="AntWhoRhe">4.3  Anti-Whorfian rhetoric</a></h3>

<p>

Lila Gleitman is an Essentialist on the other side of the 
contemporary debate: she is against linguistic relativism, and 
against the broadly Whorfian work of Stephen Levinson&#8217;s group at the 
Max Planck Institute for Psycholinguistics. In the context of 
criticizing a particular research design, Li and Gleitman (2002) 
quote Whorf&#8217;s claim that &#8220;language is the factor that limits 
free plasticity and rigidifies channels of development&#8221;. But in
the claim cited, Whorf seems to be talking about the psychological 
topic that holds universally of human conceptual development, not 
claiming that linguistic relativism is true.</p>

<p>

Li and Gleitman then claim (p. 266) that such (Whorfian) views 
&#8220;have diminished considerably in academic favor&#8221; in part 
because of &#8220;the universalist position of Chomskian linguistics,
with its potential for explaining the striking similarity of language
learning in children all over the world.&#8221; But there is no clear
conflict or even a conceptual connection between Whorf&#8217;s views about 
language placing limits on developmental plasticity, and Chomsky&#8217;s 
thesis of an innate universal architecture for syntax. In short, 
there is no reason why Chomsky&#8217;s I-languages could not be innately 
constrained, but (once acquired) cognitively and developmentally 
constraining.</p>

<p>

For example, the supposedly deep linguistic universal of 
&#8216;recursion&#8217; (Hauser et al. 2002) is surely quite 
independent of whether the inventory of colour-name lexemes in your 
language influences the speed with which you can discriminate between
color chips. And conversely, universal tendencies in color naming 
across languages (Kay and Regier 2006) do <em>not</em> show that 
color-naming differences among languages are without effect on 
categorical perception (Thierry et al. 2009).</p>

<h3><a name="StrWeaWho">4.4  Strong and weak Whorfianism</a></h3>

<p>

One of the first linguists to defend a general form of universalism 
against linguistic relativism, thus presupposing that they conflict, 
was Julia Penn (1972). She was also an early popularizer of the 
distinction between &#8216;strong&#8217; and &#8216;weak&#8217; 
formulations of the Sapir-Whorf Hypothesis (and an opponent of the 
&#8216;strong&#8217; version).</p>

<p>

&#8216;Weak&#8217; versions of Whorfianism state that language 
<em>influences</em> or defeasibly shapes thought. 
&#8216;Strong&#8217; versions state that language <em>determines</em>
thought, or fixes it in some way. The weak versions are commonly 
dismissed as banal (because of course there must be <em>some</em> 
influence), and the stronger versions as implausible.</p>

<p>

The weak versions are considered banal because they are not 
adequately formulated as testable hypotheses that could conflict with
relevant evidence about language and thought.</p>

<p>

Why would the strong versions be thought implausible? For a language 
to make us think in a particular way, it might seem that it must at 
least temporarily prevent us from thinking in other ways, and thus 
make some thoughts not only inexpressible but unthinkable. If this 
were true, then strong Whorfianism would conflict with the Katzian 
effability claim. There would be thoughts that a person couldn&#8217;t 
think because of the language(s) they speak.</p>

<p>

Some are fascinated by the idea that there are inaccessible thoughts;
and the notion that learning a new language gives access to entirely 
new thoughts and concepts seems to be a staple of popular writing 
about the virtues of learning languages. But many scientists and 
philosophers intuitively rebel against violations of effability: 
thinking about concepts that no one has yet named is part of their 
job description.</p>

<p>

The resolution lies in seeing that the language could <em>affect 
certain aspects of our cognitive functioning without making certain 
thoughts unthinkable for us</em>.</p>

<p>

For example, Greek has separate terms for what we call light blue and
dark blue, and no word meaning what &#8216;blue&#8217; means in 
English: Greek forces a choice on this distinction. Experiments have 
shown (Thierry et al. 2009) that native speakers of Greek react 
faster when categorizing light blue and dark blue color 
chips&#8212;apparently a genuine effect of language on thought. But 
that does not make English speakers blind to the distinction, or 
imply that Greek speakers cannot grasp the idea of a hue falling 
somewhere between green and violet in the spectrum.</p>

<p>

There is no general or global ineffability problem. There is, though,
a peculiar aspect of strong Whorfian claims, giving them a local 
analog of ineffability: the content of such a claim <em>cannot be 
expressed in any language it is true of</em>. This does not make the 
claims self-undermining (as with the standard objections to 
relativism); it doesn&#8217;t even mean that they are untestable. They are 
somewhat anomalous, but nothing follows concerning the speakers of 
the language in question (except that they cannot state the 
hypothesis using the basic vocabulary and grammar that they 
ordinarily use).</p>

<p>

If there were a true hypothesis about the limits that basic English 
vocabulary and constructions puts on what English speakers can think,
the hypothesis would turn out to be inexpressible in English, using 
basic vocabulary and the usual repertoire of constructions. That 
might mean it would be hard for us to discuss it in an article in 
English unless we used terminological innovations or syntactic 
workarounds. But that doesn&#8217;t imply anything about English speakers&#8217; 
ability to grasp concepts, or to develop new ways of expressing them 
by coining new words or elaborated syntax.</p>

<h3><a name="ConEvaWhoHyp">4.5  Constructing and evaluating Whorfian hypotheses</a></h3>

<p>

A number of considerations are relevant to formulating, testing, and 
evaluating Whorfian hypotheses.</p>

<p>

Genuine hypotheses about the effects of language on thought will 
always have a duality: there will be a linguistic part and a 
non-linguistic one. The linguistic part will involve a claim that 
some feature is present in one language but absent in another.</p>

<p>

Whorf himself saw that it was only obligatory features of languages
that established &#8220;mental patterns&#8221; or &#8220;habitual
thought&#8221; (Whorf 1956: 139), since if it were optional then the
speaker could optionally do it one way or do it the other way.  And so
this would not be a case of &#8220;constraining the conceptual
structure&#8221;.  So we will likewise restrict our attention to
obligatory features here.</p>

<p>

Examples of relevant obligatory features would include lexical 
distinctions like the light vs. dark blue forced choice in Greek, or 
the forced choice between &#8220;in (fitting tightly)&#8221; vs. 
&#8220;in (fitting loosely)&#8221; in Korean. They also include 
grammatical distinctions like the forced choice in Spanish 2nd-person
pronouns between informal/intimate and formal/distant (informal 
<i>t&#250;</i> vs. formal <i>usted</i> in the singular; informal 
<i>vosotros</i> vs. formal <i>ustedes</i> in the plural), or the 
forced choice in Tamil 1st-person plural pronouns between inclusive 
(&#8220;we = me and you and perhaps others&#8221;) and exclusive 
(&#8220;we = me and others not including you&#8221;).</p>

<p>

The non-linguistic part of a Whorfian hypothesis will contrast the 
psychological effects that habitually using the two languages has on 
their speakers. For example, one might conjecture that the habitual 
use of Spanish induces its speakers to be sensitive to the formal and
informal character of the speaker&#8217;s relationship with their 
interlocutor while habitually using English does not.</p>

<p>

So testing Whorfian hypotheses requires testing two independent 
hypotheses with the appropriate kinds of data. In consequence, 
evaluating them requires the expertise of both linguistics and 
psychology, and is a multidisciplinary enterprise. Clearly, the 
linguistic hypothesis may hold up where the psychological hypothesis 
does not, or conversely.</p>

<p>

In addition, if linguists discovered that some linguistic feature was
optional in two different languages, then even if psychological experiments 
showed differences between the two populations of speakers, this 
would not show linguistic determination or influence. The cognitive 
differences might depend on (say) cultural differences.</p>

<p>

A further important consideration concerns the strength of the 
inducement relationship that a Whorfian hypothesis posits between a 
speaker&#8217;s language and their non-linguistic capacities. The claim 
that your language shapes or influences your cognition is quite 
different from the claim that your language makes certain kinds of 
cognition impossible (or obligatory) for you. The strength of any 
Whorfian hypothesis will vary depending on the kind of relationship 
being claimed, and the ease of revisability of that relation.</p>

<p>

A testable Whorfian hypothesis will have a schematic form something 
like this:</p>

<ul>

<li><strong>Linguistic part</strong>: Feature <i>F</i> is obligatory
  in <i>L</i><sub>1</sub> but optional in <i>L</i><sub>2</sub>.</li>

<li><strong>Psychological part</strong>: Speaking a language with
  obligatory feature <i>F</i> bears relation <i>R</i> to the cognitive
  effect <i>C</i>.</li>
</ul>

<p>

The relation <i>R</i> might in principle be causation or 
determination, but it is important to see that it might merely be 
correlation, or slight favoring; and the non-linguistic cognitive 
effect <i>C</i> might be readily suppressible or revisable.</p>

<p>

Dan Slobin (1996) presents a view that competes with Whorfian 
hypotheses as standardly understood. He hypothesizes that <em>when 
the speakers are using their cognitive abilities in the service of a 
linguistic ability</em> (speaking, writing, translating, etc.), the 
language they are planning to use to express their thought will have 
a temporary online effect on how they express their thought. The 
claim is that as long as language users are thinking in order to 
frame their speech or writing or translation in some language, the 
mandatory features of that language will influence the way they 
think.</p>

<p>

On Slobin&#8217;s view, these effects quickly attenuate as soon as the 
activity of thinking for speaking ends. For example, if a speaker is 
thinking for writing in Spanish, then Slobin&#8217;s hypothesis would 
predict that given the obligatory formal/informal 2nd-person pronoun 
distinction they would pay greater attention to the formal/informal 
character of their social relationships with their audience than if 
they were writing in English. But this effect is not permanent. As 
soon as they stop thinking for speaking, the effect of Spanish on 
their thought ends.</p>

<p>

Slobin&#8217;s non-Whorfian linguistic relativist hypothesis raises the 
importance of psychological research on bilinguals or people who 
currently use two or more languages with a native or near-native 
facility. This is because one clear way to test Slobin-like 
hypotheses relative to Whorfian hypotheses would be to find out 
whether language correlated non-linguistic cognitive differences 
between speakers hold for bilinguals only when are thinking for 
speaking in one language, but not when they are thinking for speaking
in some other language. If the relevant cognitive differences 
appeared and disappeared depending on which language speakers were 
planning to express themselves in, it would go some way to vindicate 
Slobin-like hypotheses over more traditional Whorfian Hypotheses. Of 
course, one could alternately accept a broadening of Whorfian 
hypotheses to include Slobin-like evanescent effects. Either way, 
attention must be paid to the persistence and revisability of the 
linguistic effects.</p>

<p>

Kousta et al. (2008) shows that &#8220;for bilinguals there is 
intraspeaker relativity in semantic representations and, therefore, 
[grammatical] gender does not have a conceptual, non-linguistic 
effect&#8221; (843). Grammatical gender is obligatory in the 
languages in which it occurs and has been claimed by Whorfians to 
have persistent and enduring non-linguistic effects on 
representations of objects (Boroditsky et al. 2003). However, Kousta 
et al. supports the claim that bilinguals&#8217; semantic representations 
vary depending on which language they are using, and thus have 
transient effects. This suggests that although some semantic 
representations of objects may vary from language to language, their 
non-linguistic cognitive effects are transitory.</p>

<p>

Some advocates of Whorfianism have held that if Whorfian hypotheses 
were true, then meaning would be globally and radically 
indeterminate. Thus, the truth of Whorfian hypotheses is equated with
global linguistic relativism&#8212;a well known self-undermining form
of relativism. But as we have seen, not all Whorfian hypotheses are 
global hypotheses: they are about what is induced by particular 
linguistic features. And the associated non-linguistic perceptual and
cognitive differences can be quite small, perhaps insignificant. For 
example, Thierry et al. (2009) provides evidence that an obligatory 
lexical distinction between light and dark blue affects Greek 
speakers&#8217; color perception in the left hemisphere only. And the 
question of the degree to which this affects sensuous experience is 
not addressed.</p>

<p>

The fact that Whorfian hypotheses need not be global linguistic 
relativist hypotheses means that they do not conflict with the claim 
that there are language universals. Structuralists of the first half 
of the 20th century tended to disfavor the idea of universals: Martin
Joos&#8217;s characterization of structuralist linguistics as claiming that
&#8220;languages can differ without limit as to either extent or 
direction&#8221; (Joos 1966, 228) has been much quoted in this 
connection. If the claim that languages can vary without limit were 
conjoined with the claim that languages have significant and 
permanent effects on the concepts and worldview of their speakers, a 
truly profound global linguistic relativism would result. But neither
conjunct should be accepted. Joos&#8217;s remark is regarded by nearly all 
linguists today as overstated (and merely a caricature of the 
structuralists), and Whorfian hypotheses do not have to take a global
or deterministic form.</p>

<p>

John Lucy, a conscientious and conservative researcher of Whorfian 
hypotheses, has remarked:</p>

<blockquote>
We still know little about the connections between particular language
patterns and mental life&#8212;let alone how they operate or how
significant they are&#8230;a mere handful of empirical studies
address the linguistic relativity proposal directly and nearly all are
conceptually flawed. (Lucy 1996, 37)
</blockquote>

<p>

Although further empirical studies on Whorfian hypotheses have been 
completed since Lucy published his 1996 review article, it is hard to
find any that have satisfied the criteria of:</p>

<ul>

<li>adequately utilizing both the relevant linguistic and psychological research,</li>

<li>focusing on obligatory rather than optional linguistic features,</li>

<li>stating hypotheses in a clear testable way, and</li>

<li>ruling out relevant competing Slobin-like hypotheses.</li>
</ul>

<p>

There is much important work yet to be done on testing the range of 
Whorfian hypotheses and other forms of linguistic conceptual 
relativism, and on understanding the significance of any Whorfian 
hypotheses that turn out to be well supported.</p>

<h2><a name="LanAcq">5.  Language Acquisition</a></h2>

<p>

The three approaches to linguistic theorizing have at least something
to say about how languages are acquired, or could in principle be 
acquired. Language acquisition has had a much higher profile since 
generative Essentialist work of the 1970s and 1980s gave it a central
place on the agenda for linguistic theory.</p>

<p>

Research into language acquisition falls squarely within the 
psychology of language; see the entry on 
 <a href="https://plato.stanford.edu/entries/innateness-language/">language and innateness</a>.
 In this section we do not aim to deal in detail with any of the voluminous
literature on psychological or computational experiments bearing on 
language acquisition, or with any of the empirical study of language 
acquisition by developmental linguists, or the &#8216;stimulus 
poverty&#8217; argument for the existence of innate knowledge about 
linguistic structure (Pullum and Scholz 2002). Our goals are merely 
to define the issue of <strong>linguistic nativism</strong>, set it 
in context, and draw morals for our three approaches from some of the
mathematical work on inductive language learning.</p>

<h3><a name="LinNat">5.1  Linguistic nativism</a></h3>

<p>

The reader with prior acquaintance with the literature of linguistics
will notice that we have not made reference to any partitioning of 
linguists into two camps called &#8216;empiricists&#8217; and 
&#8216;rationalists&#8217; (see e.g. Matthews 1984, Cowie 1999). We 
draw a different distinction relating to the psychological and 
biological prerequisites for first language acquisition. It divides 
nearly all Emergentists and Externalists from most Essentialists. It 
has often been confused with the classical empiricist/rationalist 
issue.</p>

<p>

<strong>General nativists</strong> maintain that the prerequisites 
for language acquisition are just general cognitive abilities and 
resources. <strong>Linguistic nativists</strong>, by contrast, claim 
that human infants have access to at least some specifically 
linguistic information that is not learned from linguistic 
experience. Table 3 briefly sketches the differences between the two 
views.</p>

<div class="figure">
<table class="hrules vert-top cellpad-small avoid-break centered">
<tr>
 <td><span class="sc">general nativists</span></td>
 <td><span class="sc">linguistic nativists</span></td>
</tr>

      <tr>
	<td>
	  Languages are acquired mainly through the exercise of
	  defeasible inductive methods, based on experience of
	  linguistic communication
	</td>
	<td>
	  Language cannot be acquired by defeasible inductive methods;
	  its structural principles must to a very large degree be
	  unlearned
	</td>
      </tr>
      <tr>
	<td>
	  The unlearned capacities that underpin language acquisition
	  constitute a uniquely human complex of non-linguistic
	  dispositions and mechanisms that also subserve other
	  cognitive functions
	</td>
	<td>
	  In addition to various broadly language-relevant cognitive
	  and perceptual capacities, language acquisition draws on an
	  unlearned system of &#8216;universal grammar&#8217; that
	  constrains language form
	</td>
      </tr>
      <tr>
	<td>
	  Various non-human animal species may well have most or all
	  of the capacities that humans use for language
	  acquisition&#8212;though no non-human species seems to have
	  the whole package, so interspecies differences are a matter
	  of degree
	</td>
	<td>
	  There is a special component of the human mind which has the
	  development of language as its key function, and no
	  non-human species has anything of the sort, so there is a
	  difference in kind between the abilities of humans and other
	  animals
	</td>
      </tr>
  </table>
<p class="center"><span class="figlabel">Table 3:</span>
General and linguistic nativism contrasted</p>
</div>

<p>

There does not really seem to be anyone who is a complete 
non-nativist: nobody really thinks that a creature with no unlearned 
capacities at all could acquire a language. That was the point of the
much-quoted remark by Quine (1972: 95&#8211;96) about how &#8220;the 
behaviorist is knowingly and cheerfully up to his neck in innate 
mechanisms of learning-readiness&#8221;. Geoffrey Sampson (2001, 
2005) is about as extreme an opponent of linguistic nativism as one 
can find, but even he would not take the failure of language 
acquisition in his cat to be unrelated to the cognitive and physical 
capabilities of cats.</p>

<p>

The issue on which empirical research can and should be done is 
whether some of the unlearned prerequisites that humans enjoy have 
specifically linguistic content. For a philosophically-oriented 
discussion of the matter, see chapters 4&#8211;6 of Stainton (2006). 
For extensive debate about &#8220;the argument from poverty of the 
stimulus&#8221;, see Pullum and Scholz (2002) together with the six 
critiques published in the same issue of <em>The Linguistic 
Review</em> and the responses to those critiques by Scholz and Pullum
(2002).</p>

<h3><a name="LanLea">5.2  Language learnability</a></h3>

<p>

Linguists have given considerable attention to considerations of 
in-principle <strong>learnability</strong>&#8212;not so much the 
course of language acquisition as tracked empirically (the work of 
developmental psycholinguists) but the question of how languages of 
the human sort could possibly be learned by any kind of learner. The 
topic was placed squarely on the agenda by Chomsky (1965); and a 
hugely influential mathematical linguistics paper by Gold (1967)has 
dominated much of the subsequent discussion.</p>

<h4>5.2.1  The Gold paradigm</h4>

<p>

Gold began by considering a reformulation of the standard 
philosophical problem of induction. The trouble with the question 
&#8216;Which hypothesis is correct given the totality of the 
data?&#8217; is of course the one that Hume saw: if the domain is 
unbounded, no finite amount of data can answer the question. Any 
finite body of evidence will be consistent with arbitrarily many 
hypotheses that are not consistent with each other. But Gold proposed
replacing the question with a very different one: <em>Which tentative
hypothesis is the one to pick</em>, given the data provided so far, 
assuming a finite number of wrong guesses can be forgiven?</p>

<p>

Gold assumed that the hypotheses, in the case of language learning, 
were generative grammars (or alternatively parsers; he proves results
concerning both, but for brevity we follow most of the literature and
neglect the very similar results on parsers). The learner&#8217;s task is 
conceived of as responding to an unending input data stream 
(ultimately complete, in that every expression eventually turns up) 
by enunciating a sequence of guesses at grammars.</p>

<p>

Although Gold talks in developmental psycholinguistic terms about 
language learners learning grammars by trial and error, his extremely
abstract proofs actually make no reference to the linguistic content 
of languages or grammars at all. The set of all finite grammars 
formulable in any given metalanguage is computably enumerable, so 
grammars can be systematically numbered. Inputs&#8212;grammatical 
expressions from the target language&#8212;can also be numerically 
encoded. We end up being concerned simply with the existence or 
non-existence of certain functions from natural number sequences to 
natural numbers.</p>

<p>

A successful learner is one who uses a procedure that is guaranteed 
to eventually hit on a correct grammar. For single languages, this is
trivial: if the target language is <i>L</i> and it is generated by a 
grammar <i>G</i>, then the procedure &#8220;Always guess 
<i>G</i>&#8221; does the job, and every language is learnable. What 
makes the problem interesting is applying it to <em>classes</em> of 
grammars. A successful learner for a class <i>C</i> is one who uses a
procedure that is guaranteed to succeed no matter what grammar from 
<i>C</i> is the target and no matter what the data stream is like (as
long as it is complete and contains no ungrammatical examples).</p>

<p>

Gold&#8217;s work has interesting similarities with earlier philosophical 
work on inductive learning by Hilary Putnam (1963; it is not clear 
whether Gold was aware of this paper). Putnam gave an informal proof 
of a sort of incompleteness theorem for inductive regularity-learning
devices: no matter what algorithm is used in a machine for inducing 
regularities from experience, and thus becoming able to predict 
events, there will always be some possible environmental regularities
that will defeat it. (As a simple example, imagine an environment 
giving an unbroken sequence of presentations all having some property
<i>a</i>. If there is a positive integer <i>n</i> such that after 
<i>n</i> presentations the machine will predict that presentation 
number n + 1 will also have property <i>a</i>, then the machine will 
be defeated by an environment consisting of <i>n</i> presentations of
<i>a</i> followed by one with the incompatible property 
<i>b</i>&#8212;the future need not always resemble the past. But if 
on the other hand there is no such <i>n</i>, then an environment 
consisting of an unending sequence of <i>a</i> presentations will 
defeat it.)</p>

<p>

Gold&#8217;s theorems are founded on certain specific idealizing 
assumptions about the language learning situation, some of which are 
intuitively very generous to the learner. The main ones are these:</p>

<ul> 

<li><strong>Pre-set grammar class</strong>. A class of grammars from
  among which to select is fixed ab initio, and the learner&#8217;s strategy
  can be one that only works for that class.</li>

<li><strong>Pre-set vocabulary</strong>. A finite universal
  vocabulary of elements <i>V</i> is fixed ab initio, and the learner
  can rely on not encountering any other elements (though the learner
  does not know which subset of <i>V</i> is used in the target
  language).</li>

<li><strong>Unending input</strong>. The input (the evidence
  presented to the learner) goes on forever&#8212;though it may
  contain arbitrary repetitions, and a successful learner will always
  reach a point where no future evidence will cause a change of
  guess.</li>

<li><strong>Exhaustive evidence</strong>. Ultimately every
  expression in the language will appear in the evidence presented to
  the learner.</li>

<li><strong>No noise</strong>. Every input example is a grammatical
  expression of the target language.</li>

<li><strong>No ordering restrictions</strong>. Any expression may
  appear at any point in the input data stream.</li>

<li><strong>No memory limit</strong>. The learner can remember every
  expression ever presented.</li>

<li><strong>No time limit</strong>. Learning must be achieved after
  some finite time, but no fixed bound is set in advance.</li>

<li><strong>Generative grammar target</strong>. What is ultimately
  learned is a generative grammar.</li>

<li><strong>No statistics</strong>. Frequency of particular
  expressions in the input plays no role in the learning process.</li>
</ul>

<p>

The most celebrated of the theorems Gold proved (using some reasoning
remarkably similar to that of Putnam 1963) showed that a language 
learner could be similarly hostage to malign environments. Imagine a 
learner being exposed to an endless and ultimately exhaustive 
sequence of presented expressions from some target 
language&#8212;Gold calls such a sequence a &#8216;text&#8217;. 
Suppose the learner does not know in advance whether the language is 
infinite, or is one of the infinitely many finite languages over the 
vocabulary <i>V</i>. Gold reasons roughly thus:</p>

<ul class="jfy">

<li>There must be some <i>n</i> such that an environment consisting
  of a sequence of <i>n</i> presented expressions all taken from a
  certain finite language <i>L</i><sub>1</sub> (possibly with many
  repetitions) will cause the learner to guess the target language
  is <i>L</i><sub>1</sub>. (If there is not, then we already know how
  to baffle the learner: the learner will be unable to
  learn <i>L</i><sub>1</sub> from any text.)</li>

<li>But if there is such an <i>n</i>, then the learner will be
  baffled by any infinite target language that is a superset of them
  all: a text consisting of <i>n</i> presentations of expressions
  from <i>L</i><sub>1</sub> followed by <i>n</i> presentations of a
  slightly larger finite language <i>L</i><sub>2</sub>, and so on
  forever (there is no largest finite language, and ex hypothesi the
  learner will keep trying them all).</li>
</ul>

<p>

Leaping too soon to the conclusion that the target language is 
infinite will be disastrous, because there will be no way to 
retrench: no presented examples from a finite language 
<i>L</i><sub><i>k</i></sub> will ever conflict with the hypothesis 
that the target is some infinite superset of 
<i>L</i><sub><i>k</i></sub>.</p>

<p>

The relevance of all this to the philosophy of linguistics is that 
the theorem just sketched has been interpreted by many linguists, 
psycholinguists, and philosophers as showing that humans could not 
learn languages by inductive inference based on examples of language 
use, because <em>all</em> of the well-known families of languages 
defined by different types of generative grammar have the crucial 
property of allowing grammars for every finite language and for at 
least some infinite supersets of them. But Gold&#8217;s paper has often 
been over-interpreted. A few examples of the resultant mistakes 
follow.</p>

<p>

<strong>It&#8217;s not about underdetermination</strong>. Gold&#8217;s negative 
results are sometimes wrongly taken to be an unsurprising reflection 
of the underdetermination of theories by finite bodies of evidence 
(Hauser et al. 2002 seem to make this erroneous equation on p. 1577; 
so do Fodor and Crowther 2002, implicitly&#8212;see Scholz and Pullum
2002, 204&#8211;206). But the failure of text-identifiability for 
certain classes of languages is different from underdetermination in 
a very important way, because there are infinite classes of infinite 
languages that <em>are</em> identifiable from text. The first chapter
of Jain et al. (1999) discusses an illustrative example (basically, 
it is the class containing, for all <i>n</i> &gt; 0, the set of all strings 
with length greater than <i>n</i>). There are infinitely many others.
For example, Shinohara (1990) showed that for any positive integer 
<i>n</i> the class of all languages generated by a context-sensitive 
grammar with not more than <i>n</i> rules is learnable from text.</p>

<p>

<strong>It&#8217;s not about stimulus poverty</strong>. It has also 
sometimes been assumed that Gold is giving some kind of argument from
poverty of the stimulus (there are signs of this in Cowie 1999, 
194ff; Hauser et al. 2002, 1577; and Prinz 2002, 210). This is very 
clearly a mistake (as both Laurence and Margolis 2001 and Matthews 
2007 note): in Gold&#8217;s text-learning scenario there is no stimulus 
poverty at all. Every expression in the language eventually turns up 
in the learner&#8217;s input.</p>

<p>

<strong>It&#8217;s not all bad news</strong>. It is sometimes forgotten 
that Gold established a number of optimistic results as well as the 
pessimistic one about learning from text. Given what he called an 
&#8216;informant&#8217; environment rather than a text environment, 
we see strikingly different results. An informant environment is an 
infinite sequence of presentations sorted into two lists, positive 
instances (expressions belonging to the target language) and negative
instances (not in the language). Almost all major language-theoretic 
classes are identifiable in the limit from an informant environment 
(up to and including the class of all languages with a primitive 
recursive characteristic function, which comes close to covering any 
language that could conceivably be of linguistic interest), and all 
computably enumerable languages become learnable if texts are allowed
to be sequenced in particular ways (see the results in Gold 1967 on 
&#8216;anomalous text&#8217;).</p>

<p>

Gold did not give a necessary condition for a class to be identifiable
in the limit from text, but Angluin (1980) later provided one (in a
result almost but not quite obtained by Wexler and Hamburger
1973). Angluin showed that a class <i>C</i> is text-identifiable iff
every language <i>L</i> in <i>C</i> has a finite &#8220;telltale&#8221; subset
<i>T</i> such that if <i>T</i> is also proper subset of some other
language in <i>C</i>, that other language is not a proper subset of
<i>L</i>.  This condition precludes guessing too large a language.
Once all the members of the telltale subset for <i>L</i> have been
received as input, the learner can safely make <i>L</i> the current
conjecture.  The language to be identified must be either <i>L</i> or
(if subsequent inputs include new sentences not in <i>L</i>) some
larger language, but it can&#8217;t be a proper subset of <i>L</i>.</p>


<p>

Johnson (2004) provides a useful review of several other 
misconceptions about Gold&#8217;s work; e.g., the notion that it might be 
the absence of semantics from the input that makes identification 
from text impossible (this is not the case).</p>

<h4>5.2.2  Gold&#8217;s paradox</h4>

<p>

Some generative Essentialists see a kind of paradox in Gold&#8217;s 
results&#8212;a reductio on one or more of the assumptions he makes 
about in-principle learnability. To put it very crudely, learning 
generative grammars from presented grammatical examples seems to have
been proved impossible, yet children do learn their first languages, 
which for generative Essentialists means they internalize generative 
psychogrammars, and it is claimed to be an empirical fact that they 
get almost no explicit evidence about what is <em>not</em> in the 
language (Brown and Hanlon 1970 is invariably cited to support this).
Contradiction. Gold himself suggested three escape routes from the 
apparent paradox:</p>

 <ol>
 <li>Assume tighter limits on the pre-set grammar
class. Perhaps, for example, learners have an
&#8216;innate&#8217; grasp of some definition of the pre-set
grammar class that guarantees its learnability. (For example,
identifiability in the limit from text could be guaranteed by
ensuring that the class of candidate languages does not
contain both (a) some infinite set of finite languages and (b)
some infinite language that is the union of all of them.)</li>

<li>Assume learners get systematic information about what
is <em>not</em> in the language (perhaps indirectly, in ways
not yet recognized), so that the environment is of the
informant type rather than the text type.</li>

<li>Assume some helpful feature is present in learning
environments. The &#8216;no order restrictions&#8217;
assumption might be false: there could be regularities in the
order of expressions in texts that can support inferences
about what is ungrammatical.</li>
</ol>

<p>

All three of these paths have been subsequently explored. Path (1) 
appealed to generative Essentialists. Chomsky (1981) suggested an 
extreme restriction: that universal grammar permitted only finitely 
many grammars. This claim (for which Chomsky had little basis: see 
Pullum 1983) would immediately guarantee that not all finite 
languages are humanly learnable (there are infinitely many finite 
languages, so for most of them there would be no permissible 
grammar). Osherson and Weinstein (1984) even proved that under three 
fairly plausible assumptions about the conditions on learning, 
finiteness of the class of languages is necessary&#8212;that is, a 
class <em>must</em> be finite if it is to be identifiable from text. 
However, they also proved that this is not sufficient: there are very
small finite classes of languages that are <em>not</em> identifiable 
from text, so it is logically possible for text-identification to be 
impossible even given only a finite number of languages (grammars). 
These two results show that Chomsky&#8217;s approach cannot be the whole 
answer.</p>

<p>

Path (2) proposes investigation of children&#8217;s input with an eye to 
finding covert sources of negative evidence. Various psycholinguists 
have pursued this idea; see the entry on 
 <a href="https://plato.stanford.edu/entries/innateness-language/">language and innateness</a>
 in this encyclopedia, and (to cite one example) the results of 
Chouinard and Clark (2003) on hitherto unnoticed sources of negative 
evidence in the infant&#8217;s linguistic environment, such as parental 
corrections.</p>

<p>

Path (3) suggests investigating the nature of children&#8217;s linguistic 
environments more generally. Making evidence available to the learner
in some fixed order can certainly alter the picture quite radically 
(Gold proved that if some primitive-recursive generator controls the 
text it can in effect encode the identity of the target language so 
that all computably enumerable languages become identifiable from 
text). It is possible in principle that limitations on texts (or on 
learners&#8217; uptake) might have positive rather than negative effects on
learnability (see Newport 1988; Elman 1993; Rohde and Plaut 1999; and
the entry on 
 <a href="https://plato.stanford.edu/entries/innateness-language/">language and innateness</a>).
 </p>

<h4>5.2.3  The claimed link to &#8216;rationalism&#8217; versus &#8216;empiricism&#8217;</h4>

<p>

Gold&#8217;s suggested strategy of restricting the pre-set class of 
grammars has been interpreted by some as a defense of rationalist 
rather than empiricist theories of language acquisition. For example,
Wexler and Culicover state:</p>

<blockquote>
Empiricist theory allows for a class of sensory or peripheral
processing mechanisms by means of which the organism receives data. In
addition, the organism possesses some set of inductive principles or
learning mechanisms&#8230;Rationalist theory also assumes that a
learner has sensory mechanisms and inductive principles. But
rationalist theory assumes that in addition the learner possesses a
rich set of principles concerning the general nature of the ability
that is to be learned. (Wexler and Culicover 1980: 5)
</blockquote>

<p>

Wexler and Culicover claim that &#8216;empiricist&#8217; learning 
mechanisms are both weak and general: not only are they &#8216;not 
related to the learning of any particular subject matter or cognitive
ability&#8217; but they are not &#8216;limited to any particular 
species&#8217;. It is of course not surprising that empiricist 
learning fails if it is defined in a way that precludes drawing a 
distinction between the cognitive abilities of humans and fruit 
flies.</p>

<p>

Equating Gold&#8217;s idea of restricting the class of grammars with the 
idea of a &#8216;rationalist&#8217; knowledge acquisition theory, 
Wexler and Culicover try to draw out the consequences of Gold&#8217;s 
paradigm for the Essentialist linguistic theory of Chomsky (1965). 
They show how a very tightly restricted class of transformational 
grammars could be regarded as text-identifiable under extremely 
strong assumptions (e.g., that all languages have the same innately 
known deep structures).</p>

<p>

Matthews (1984) follows Wexler and Culicover&#8217;s lead and draws a more 
philosophically oriented moral:</p>

<blockquote>
The significance of Gold&#8217;s result becomes apparent if one considers
that (i) empiricists assume that there are no constraints on the class
of possible languages (besides perhaps that natural languages be
recursively enumerable), and (ii) the learner employs a maximally
powerful learning strategy&#8212;there are no strategies that could
accomplish what that assumed by Gold cannot. These two facts, given
Gold&#8217;s unsolvability result for text data, effectively dispose of the
empiricist claim that there exists a &#8216;discovery
procedure&#8217;. (1989: 60)
</blockquote>

<p>

The actual relation of Gold&#8217;s results to the empiricism/rationalism 
controversy seems to us rather different. Gold&#8217;s paradigm looks a lot
more like a formalization of so-called &#8216;rationalism&#8217;. The
fixed class of candidate hypotheses (grammars) corresponds to what is
given by universal grammar&#8212;the innate definition of the 
essential properties of language. What Gold actually shows, 
therefore, is not &#8220;the plausibility of rationalism&#8221; but 
rather the inadequacy of a huge range of rationalist theories: under 
a wide range of different choices of universal grammar, language 
acquisition appears to remain impossible.</p>

<p>

Moreover, Matthews ignores (as most linguists have) the existence of 
large and interesting classes of languages that are 
text-identifiable.</p>

<p>

Gold&#8217;s result, like Putnam&#8217;s earlier one, does show that a certain 
kind of trial-and-error inductive learning is insufficient to permit 
learning of arbitrary environmental regularities. There has to be 
some kind of initial bias in the learning procedure or in the data. 
But &#8216;empiricism&#8217;, the supposed opponent of 
&#8216;rationalism&#8217;, is not to be equated with a denial of the 
existence of learning biases. No one doubts that humans have 
inductive biases. To quote Quine again, &#8220;Innate biases and 
dispositions are the cornerstone of behaviorism, and have been 
studied by behaviorists&#8221; (1972: 95&#8211;96). As Lappin and 
Shieber (2007) stress, there cannot be such a thing as a learning 
procedure (or processing mechanism) with no biases at all.</p>

<p>

The biases posited in Emergentist theories of language acquisition 
are found, at least in part, in the non-linguistic social and 
cognitive bases of human communication. And the biases of Externalist
approaches to language acquisition are to be found in the 
distributional and stochastic structure of the learning input and the
multitude of mechanisms that process that input and their 
interactions. All contemporary approaches to language acquistion have
acknowledged Gold&#8217;s results, but those results do not by themselves 
vindicate any one of our three approaches to the study of language.</p>

<p>

Gold&#8217;s explicit equation of acquiring a language with identifying a 
generative grammar that exactly generates it naturally makes his work
seem relevant to generative Essentialists (though even for them, his 
results do not provide anything like a sufficient reason for adopting
the linguistic nativist position). But another key assumption, that 
nothing about the statistical structure of the input plays a role in 
the acquisition process, is being questioned by increasing numbers of
Externalists, many of whom have used Bayesian modeling to show that 
the absence of positive evidence can function as a powerful source of
(indirect) negative evidence: learning can be driven by what is not 
found as well as by what is (see e.g. Foraker et al. (2009)).</p>

<p>

Most Emergentists simply reject the assumption that what is learned 
is a generative grammar. They see the acquisition task as a matter of
learning the details of an array of constructions (roughly, 
meaning-bearing ways of structurally composing words or phrases) and 
how to use them to communicate. How such learning is accomplished 
needs a great deal of further study, but Gold&#8217;s paper did not show it
to be impossible.</p>

<h2><a name="LanEvo">6.  Language Evolution</a></h2>

<p>

Over the past two decades a large amount of work has been done on 
topics to which the term &#8216;language evolution&#8217; is 
attached, but there are in fact four distinct such topics:</p>

<ol type="a">

<li>the phylogenetic emergence of <strong>non-human
  communication</strong> capacities, systems, and behaviors in various
  animals;</li>

<li>the phylogenetic emergence of uniquely <strong>human
  communication</strong> capacities, systems, and behaviors;</li>

<li>the phylogenetic emergence, unique in humans, of the capacity
  (or capacities)
  to <strong>develop</strong>, <strong>acquire</strong>,
  and <strong>use</strong> language;</li>

<li>the course of <strong>historical evolution</strong> through
  intergenerational changes in particular languages as they are
  acquired and used by humans.</li>
</ol>

<h3><a name="PhyEme">6.1  Phylogenetic emergence</a></h3>

<p>

Emergentists tend to regard any of the topics (a)&#8211;(d) as 
potentially relevant to the study of language evolution. 
Essentialists tend to focus solely on (c). Some Essentialists even 
deny that (a) and (b) have any relevance to the study of (c); for 
example:</p>

<blockquote>
  There is nothing useful to be said about behavior or thought at the
  level of abstraction at which animal and human communication fall
  together&#8230; [H]uman language, it appears, is based on entirely
  different principles. This, I think, is an important point, often
  overlooked by those who approach language as a natural, biological
  phenomenon; in particular, it seems rather pointless, for these
  reasons, to speculate about the evolution of human language from
  simpler systems&#8230; (Chomsky 1968: 62)
</blockquote>

<p>

Other generative Essentialists, like Pinker and Bloom (1990) and 
Pinker and Jackendoff (2005), seem open to the view that even the 
most elemental aspects of topic (b) can be directly relevant to the 
study of (c). This division among Essentialists reflects a division 
among their views about the role of adaptive explanations in the 
emergence of (b) and especially (c). For example:</p>

<blockquote>
We know very little about what happens when 10<sup>10</sup> neurons
are crammed into something the size of a basketball, with further
conditions imposed by the specific manner in which this system
developed over time. It would be a serious error to suppose that all
properties, or the interesting properties of the structures that
evolved, can be &#8216;explained&#8217; in terms of &#8216;natural
selection&#8217;. (Chomsky 1975:59, quoted by Newmeyer 1998 and
Jackendoff 2002)
</blockquote>

<p>

The view expressed here that all (or even most) interesting 
properties of the language faculty are not adaptations conflicts with
the basic explanatory strategy of evolutionary psychology found in 
the neo-Darwinian Essentialist views of Pinker and Bloom. 
Piattelli-Palmarini (1989), following Chomsky, adopts a fairly 
standard Bauplan critique of adaptationism. On this view the language
faculty did not originate as an adaptation, but more plausibly 
&#8220;may have originally arisen for some purely architectural or 
structural reason (perhaps overall brain size, or the sheer 
duplication of pre-existing modules), or as a by product of the 
evolutionary pressures&#8221; (p. 19), i.e., it is a kind of Gouldian
spandrel.</p>

<p>

More recently, some Essentialist-leaning authors have rejected the 
view that no analogies and homologies between animal and human 
communication are relevant to the study of language. For example, in 
the context of commenting on Hauser et al. (2002), Tecumseh Fitch 
(2010) claims that &#8220;Although Language, writ large, is unique to
our species (many probably most) of the mechanisms involved in 
language have analogues or homologues in other animals.&#8221; 
However, the view that the investigation of animal communication can 
shed light on human language is still firmly rejected by some. For 
example, Bickerton (2007: 512) asserts that &#8220;nothing resembling
human language could have developed from prior animal call 
systems.&#8221;</p>

<p>

Bickerton fronts the following simple argument for his view:</p>

<blockquote>
If any adaptation is unique to a species, the selective pressure that
drove it must also be unique to that species; otherwise the adaptation
would have appeared elsewhere, at least in rudimentary form. (2007:
514)
</blockquote>

<p>

Thus, the mere fact that language is unique to humans is sufficient 
to rule out monkey and primate call systems as preadapations for 
language. But, contra Bickerton, a neo-Darwinian like Jackendoff 
(2002) appeals to the work of Dunbar (1998), Power (1998), Worden 
(1998) to provide a selectionist story which assumes that cooperation
in hunting, defense (Pinker and Bloom 1990), and &#8220; 
&#8216;social grooming&#8217; or deception&#8221; are selective 
forces that operated on human ancestors to drive increases in 
expressive power that distinguishes non-human communication and human
linguistic capacities and systems.</p>

<p>

While generative Essentialists debate among themselves about the 
plausibility of adaptive explanations for the emergence of essential 
features of a modular language capacity, Emergentists are perhaps 
best characterized as seeking broad evolutionary explanations of the 
features of languages (topic (c)) and communicative capacities 
(topics (b) and (c)) conceived in non-essentialist, non-modular ways.
And as theorists who are committed to exploring non-modular views 
of linguistic capacities (topic (c)), the differences and 
similarities between (a) and (b) are potentially relevant to (c).</p>

<p>

Primatologists like Cheney and Seyfarth, psychologists like 
Tomasello, anthropologists like Terrence Deacon, and linguists like 
Phillip Lieberman share an interest in investigating the 
communicative, anatomical, and cognitive characteristics of non-human
animals to identify biological differences between humans, and 
monkeys and primates. In the following paragraph we discuss Cheney 
and Seyfarth (2005) as an example, but we could easily have chosen 
any of a number of other theorists.</p>

<p>

Cheney and Seyfarth (2005) emphasize that non-human primates have a 
small, stimulus specific repertoire of vocal productions that are not 
&#8220;entirely involuntary,&#8221; and this contrasts with their 
&#8220;almost openended ability to learn novel sound-meaning 
pairs&#8221; (p. 149). They also emphasize that vocalizations in 
monkeys and apes are used to communicate information about the 
vocalizer, not to provide information intended to &#8220;rectify 
false beliefs in others or instruct others&#8221; (p. 150). Non-human
primate communication consists in the mainly involuntary broadcasting
of the vocalizer&#8217;s current affective state. Moreover, although Cheney
and Seyfarth recognize that the vervet monkey&#8217;s celebrated call 
system (Cheney and Seyfarth 1990) is &#8220;functionally 
referential&#8221; in context, their calls have no explicit meaning 
since they lack &#8220;any propositional structure&#8221;. From this 
they conclude:</p>

<blockquote>
The communication of non-human animals lacks three features that are
abundantly present in the utterances of young children: a rudimentary
ability to attribute mental states different from their own to others,
the ability to generate new words, and lexical syntax. (2005: 151)
</blockquote>

<p>

By &#8216;lexical syntax&#8217; Cheney and Seyfarth mean a kind of 
semantic compositionality of characteristic vocalizations. If a 
vocalization (call) were to have lexical syntax, the semantic 
significance of the whole would depend on the relation of the 
structure of parts of the call to the structure of what they signify.
The absence of &#8216;lexical syntax&#8217; in call systems suggests 
that it is illegitimate to think of them as having anything like 
semantic structure at all.</p>

<p>

Despite the rudimentary character of animal communication systems 
when compared with human languages, Cheney and Seyfarth argue that 
monkeys and apes exhibit at least five characteristics that are 
pre-adaptations for human communication:</p>

<ol type="i">

<li>their vocalizations are representational;</li>

<li>they have competitive/cooperative relations in which alliances,
  friendships, and rivalries that &#8220;create selective pressures
  for the kind of complex, abstract conceptual abilities that are
  likely to have proceeded the earlier linguistic
  communication&#8221;;</li>

<li>because of (ii), their representations of social relations
  between individuals and themselves are hierarchally structured;</li>

<li>certain monkeys, e.g. baboons, have open-ended, rule-governed
  systems of social knowledge;</li>

<li>their knowledge is propositional.</li>
</ol>

<p>

It is, of course, controversial to claim that monkeys have 
rule-governed propositional social knowledge systems as claimed in 
(iv) and (v). But Emergentists, Externalists, and Essentialists could
all, in principle, agree that there are both unique characteristics 
of human communicative capacities and characteristics of such 
capacities that are shared with non-humans. For example, by the age 
of one, human infants can use direction of gaze and focus of 
attention to infer the referent of a speaker&#8217;s utterance (Baldwin and
Moses 1994). By contrast, this sort of social referencing capacity in
monkeys and apes is rudimentary. This suggests that a major component
of humans&#8217; capacity to infer a specific referent is lacking in 
non-humans.</p>

<p>

Disagreements between the approaches might be due to the perceived 
significance of non-human communicative capacities and their relation
to uniquely human ones.</p>

<h3><a name="HisEvo">6.2  Historical evolution</a></h3>

<p>

We mentioned earlier that both early 20th-century linguistics 
monographs and contemporary introductory textbooks include 
discussions of historical linguistics, i.e., that branch that studies
the history and prehistory of changes in particular languages, how 
they are related to each other, and how and why they change.</p>

<p>

The last decade has seen two kinds of innovations related to studying
changes in particular languages. One, which we will call 
&#8216;linguistic phylogeny&#8217;, concerns the application of 
stochastic phylogenetic methods to investigate prehistoric population
and language dispersion (Gray and Jordan 2000, Gray 2005, Atkinson 
and Gray 2006, Gray et al. 2009). These methods answer questions 
about how members of a family of languages are related to each other 
and dispersed throughout a geographic area. The second, which we will
call the effects of transmission, examines how interpreted artificial
languages (sets of signifier/signified pairs) change under a range of
transmission conditions (Kirby et al. 2008, Kirby 2001, Hurford 
2000), thus providing evidence about how the process of transmission 
affects the characteristics, especially the structure, of the 
transmitted interpreted system.</p>

<h4>6.2.1 Linguistic phylogeny</h4>

<p>

Russell Gray and his colleagues have taken powerful phylogenetic 
methods that were developed by biologists to investigate molecular 
evolution, and applied them to linguistic data in order to answer 
questions about the evolution of language families. For example, Gray
and Jordan (2000) used a parsimony analysis of a large language data 
set to adjudicate between competing hypotheses about the speed of the
spread of Austronesian languages through the Pacific. More recently, 
Greenhill et al. (2010) used a NeighbourNet analysis to evaluate the 
relative rates of change in the typological and lexical features of 
Austronesian and Indo-European. These results bear on hypotheses 
about the relative stability of language types over lexical features 
of those languages, and how far back in time that stability extends. 
If there were highly conserved typological and lexical features, then
it might be possible to identify relationships between languages that
date beyond the 8000 (plus or minus 2000) year limit that is imposed 
by lexical instability.</p>

<h4>6.2.2 Effects of transmission</h4>

<p>

The computational and laboratory experiments of Kirby and his 
collaborators have shown that under certain conditions of iterated 
learning, any given set of signifier/signified pairs in which the 
mapping is initially arbitrary will change to exhibit a very general 
kind of compositional structure. Iterated learning has been studied 
in both computational and laboratory experiments by means of 
diffusion chains, i.e., sequences of learners. A primary 
characteristic of such sequences of transmission is that what is 
transmitted from learner to learner will change in an iterated 
learning environment, in a way that depends on the conditions of 
transmission.</p>

<p>

The children&#8217;s game called &#8216;Telephone&#8217; in the USA 
(&#8216;Chinese Whispers&#8217; in the UK), provides an example of 
diffusion chains under which what is transmitted is not stable. In a 
diffusion chain learning situation what a chain member has actually 
learned from an earlier member of the chain is presented as the input
to the next learner, and what that learner has actually learned 
provides the input to the following learner. In cases where the 
initial learning task is very simple: i.e., where what is transmitted
is both simple, completely transmitted, and the transmission channel 
is not noisy, what is transmitted is stable over iterated 
transmissions even in cases when the participants are young children 
and chimpanzees (Horner et al. 2006). That is, there is little change
in what is transmitted over iterated transmissions. However, in cases
where what is transmitted is only partially presented, very complex, 
or the channel is noisy, then there is a decrease in the fidelity of 
what is transmitted across iterations just like there is in the 
children&#8217;s game of Telephone.</p>

<p>

What Kirby and colleagues show is that when the initial input to a 
diffusion chain is a reasonably complex set of arbitrary 
signal/signifier pairs, e.g. one in which 27 complex signals of 6 
letters are randomly assigned to 27 objects varying on dimensions of 
color, kind of motion, and shape, what is transmitted becomes more 
and more compositional over iterated transmission. Here, 
&#8216;compositional&#8217; is being used to refer to the high degree
to which sub-strings of the signals come to be systematically paired 
with specific phenomenal sub-features of what is signified. The 
transmission conditions in these experiments were free of noise, and 
for each iteration of the learning task only half of the possible 27 
signifier/signified pairs were presented to participants. Under this 
kind of transmission bottleneck a high degree of sign/signified 
structure emerged.</p>

<p>

A plausible interpretation of these results is that the developing 
structure of the collection of signs is a consequence of the repeated
forced inference by participants from 14 signs and signifieds in the 
training set to the entire set of 27 pairs. A moral could be that 
iterated forced prediction of the sign/signified pairs in the entire 
set, on the basis of exposure to only about half of them, induced the
development of a systematic, compositional structure over the course 
of transmission. It is reasonable to conjecture that this resulting 
structure reflects effects of human memory, not a domain-specific 
language module&#8212;although further work would be required to rule
out many other competing hypotheses.</p>

<p>

Thus Kirby and his colleagues focus on something very different from 
the prerequisites for language emergence. Linguistic nativists have 
been interested in how primates like us could have become capable of 
acquiring systems with the structural properties of natural 
languages. Kirby and his colleagues (while not denying that human 
cognitive evolution is of interest) are studying how <em>languages 
evolve to be capable of being acquired by primates like us</em>.</p>

</div>

<div id="bibliography">

<h2><a name="Bib">Bibliography</a></h2>

<ul class="hanging">

<li>Adger, David, 2003, <i>Core Syntax: A Minimalist Approach</i>, New
York: Oxford University Press.</li>

<li>Akmajian, Adrian, Demers, Richard, Farmer, Ann, and Harnish,
Robert, 2010, <i>Linguistics: An Introduction to Language and
Communication</i>, Cambridge, Massachusetts: MIT Press, 6th ed.</li>

<li>Angluin, Dana, 1980, &#8220;Inductive inference of formal
languages from positive data&#8221;, <i>Information and Control</i>, 45:
117&#8211;135.</li>

<li>Artstein, Ron and Poesio, Massimo, 2008, &#8220;Inter-Coder
Agreement for Computational Linguistics&#8221;, <i>Computational
Linguistics</i>, 34: 555&#8211;596.</li>

<li>Atkinson, Quentin D. and Gray, Russell D., 2006, &#8220;How old is
the Indo-European language family? Progress or more moths to the
flame?&#8221;, in J. Clackson, P. Forster, and C. Renfrew,
(eds.), <i>Phylogenetic Methods and the Prehistory of Languages</i>,
Cambridge: MacDonald Institute for Archaeological Research,
91&#8211;109.</li>

<li>Baldwin, Dare A. and Moses, L. J., 1994, &#8220;Early
understanding of referential intent and attentional focus: Evidence
from language and emotion&#8221;, in C. Lewis and P. Mitchell
(eds.), <i>Children&#8217;s Early Understanding of Mind: Origins and
Development</i>, Hillsdale, NJ: Lawrence Erlbaum, 133&#8211;156.</li>

<li>Bard, Ellen, Robertson, David, and Sorace, Antonella, 1996,
&#8220;Magnitude estimation of linguistic acceptability&#8221;,
<i>Language</i>, 72(1): 32&#8211;68.</li>

<li>Barkow, J. H., Cosmides, Leda, and Tooby, J., 1992, <i>The Adapted
Mind: Evolutionary Psychology and the Generation of Culture</i>, New
York: Oxford University Press.</li>

<li>Barlow, Michael and Kemmer, Suzanne (eds.), 2002, <i>Usage-Based
Models of Language</i>, Stanford: CSLI Press.</li>

<li>Baroni, M., Bernardini, S., Ferraresi, A., and Zanchetta, E.,
2009, &#8220;The WaCky Wide Web: A collection of very large
linguistically processed web-crawled corpora&#8221;, <i>Journal of
Language Resources and Evaluation</i>, 43(3): 209&#8211;226.</li>

<li>Bates, Elizabeth, Elman, Jeffrey, Johnson, Mark, Karmiloff-Smith,
Annette, Parisi, Domenico, and Plunkett, Kim, 1998, &#8220;Innateness
and emergentism&#8221;, William Bechtel and George Graham (eds.), <i>A
Companion to Cognitive Science</i>, Oxford: Basil Blackwell,
590&#8211;601.</li>

<li>Bickerton, Derek, 2007, &#8220;Language evolution: A brief guide
for linguists&#8221;, <i>Lingua</i>, 117: 510&#8211;526.</li>

<li>Bloomfield, Leonard, 1914, <i>An Introduction to the Study of
Language</i>, New York, NY: Henry Holt.</li>

<li>&#8211;&#8211;&#8211;, 1933, <i>Language</i>, New York, NY: Henry
Holt.</li>

<li>&#8211;&#8211;&#8211;, 1939, <i>Linguistic Aspects of Science</i>
(International Encyclopedia of Unified Science: Volume 1/Number 4),
Chicago: University of Chicago Press.</li>

<li>Boroditsky, Lera, Schmidt, L., and Phillips, W., 2003, &#8220;Sex,
syntax, and semantics&#8221;, in Dedre Gentner and Susan Goldin-Meadow
(eds.), <i>Language in Mind: Advances in the Study of Language and
Cognition</i>, Cambridge, Massachusetts: MIT Press, 61&#8211;80.</li>

<li>Bresnan, Joan, 2007, &#8220;A few lessons from typology&#8221;,
<i>Linguistic Typology</i>, 11: 297&#8211;306.</li>

<li>Bresnan, Joan, Cueni, Anna, Nikitina, Tatiana, and Baayen, Harald,
2007, &#8220;Predicting the dative alternation&#8221;, in G. Boume,
I. Kraemer, and J. Zwarts (eds.), <i>Cognitive Foundations of
Interpretation</i>, Amsterdam: Royal Netherlands Academy of Science,
69&#8211;94.</li>

<li>Bresnan, Joan and Ford, Marilyn, 2010, &#8220;Predicting
syntax: Processing dative constructions in American and Australian
varieties of English&#8221;, <i>Language</i>, 86: 168&#8211;213.</li>

<li>Brown, Roger and Hanlon, Camille, 1970, &#8220;Derivational
complexity and order of acquisition in child speech&#8221;, in
Cognition and the Development of Language, J. R. Hayes, (ed.), New
York: John Wiley and Sons, 11&#8211;54.</li>

<li>Brown, Roger and Lenneberg, Eric, 1954, &#8220;A study in
language and cognition&#8221;, <i>Journal of Abnormal and Social
Psychology</i>, 49: 445&#8211;453.</li>

<li>Bybee, Joan and McClelland, J. L., 2005, &#8220;Alternatives to
the combinatorial paradigm of linguistic theory based on domain
general principles of human cognition&#8221;, <i>The Linguistic Review</i>,
22(2&#8211;4): 381&#8211;410.</li>

<li>Cheney, D. L. and Seyfarth, R. M., 1990, &#8220;The assessment by
vervet monkeys of their own and another species&#8217; alarm
calls&#8221;, <i>Animal Behavior</i>, 40: 754&#8211;764.</li>

<li>&#8211;&#8211;&#8211;, 2005, &#8220;Constraints and preadaptations
in the earliest stages of language evolution&#8221;, <i>The Linguistic
Review</i>, 22: 135&#8211;159.</li>

<li>Chierchia, Gennaro, 1998, &#8220;Reference to kinds across
languages&#8221;, <i>Natural Language Semantics</i>, 6: 339&#8211;405.</li>

<li>Chomsky, Noam, 1955, &#8220;The logical structure of linguistic
theory&#8221;, Unpublished manuscript; revised in 1956 and distributed
from MIT Library; published with some abridgement in 1975 by Plenum
Press, New York.</li>

<li>&#8211;&#8211;&#8211;, 1959, &#8220;On certain formal properties
of grammars&#8221;, <i>Information and Control</i>, 1: 91&#8211;112.</li>

<li>&#8211;&#8211;&#8211;, 1965, <i>Aspects of the Theory of
Syntax</i>, Cambridge, Massachusetts: MIT Press.</li>

<li>&#8211;&#8211;&#8211;, 1968, <i>Language and Mind</i>, New York:
Harper and Row.</li>

<li>&#8211;&#8211;&#8211;, 1969, &#8220;Linguistics and
philosophy&#8221;, in Language and Philosophy: A Symposium, Sidney
Hook, (ed.), New York: New York University Press, 51&#8211;94.</li>

<li>&#8211;&#8211;&#8211;, 1975, <i>Reflections on Language</i>, New
York, NY: Pantheon.</li>

<li>&#8211;&#8211;&#8211;, 1979, <i>Language and Responsibility</i>,
[translated by John Viertel from the 1977 French edition produced by
Mitsou Ronat], Hassocks, Sussex: Harvester Press.</li>

<li>&#8211;&#8211;&#8211;, 1981, <i>Lectures on Government and
Binding</i>, Dordrecht: Foris.</li>

<li>&#8211;&#8211;&#8211;, 1986, <i>Knowledge of Language: Its Nature,
Origin and Use</i>, Westport, CT: Praeger.</li>

<li>&#8211;&#8211;&#8211;, 1988, <i>Language and Problems of
Knowledge</i>, Cambridge, Massachusetts: MIT Press.</li>

<li>&#8211;&#8211;&#8211;, 1992, &#8220;Explaining language
use&#8221;, <i>Philosophical Topics</i>, 20: 205&#8211;231.</li>

<li>&#8211;&#8211;&#8211;, 1995, &#8220;Language and nature&#8221;,
Mind, 104: 1&#8211;61.</li>

<li>&#8211;&#8211;&#8211;, 2003, &#8220;Reply to Millikan&#8221;, in
Louise M. Antony and Norbert Hornstein (eds.), <i>Chomsky and His
Critics</i>, Oxford: Blackwell, 308&#8211;315.</li>

<li>Chouinard, M. M. and Clark, E. V., 2003, &#8220;Adult
reformulations of child errors as negative evidence&#8221;, <i>Journal of
Child Language</i>, 30(3): 637&#8211;669.</li>

<li>Cowart, Wayne, 1997, <i>Experimental Syntax: Applying Objective
Methods to Sentence Judgments</i>, Newbury Park, CA: Sage
Publications.</li>

<li>Cowie, Fiona, 1999, <i>What&#8217;s Within? Nativism Reconsidered</i>,
New York: Oxford University Press.</li>

<li>Culicover, Peter W. and Jackendoff, Ray S., 2010,
&#8220;Quantitative methods alone are not enough: Response to Gibson
and Fedorenko&#8221;, <i>Trends in Cognitive Science</i>, 14(6):
234&#8211;235.</li>

<li>den Dikken, Marcel, Bernstein, Judy, Tortora, Christina, and
Zanuttini, Raffaella, 2007, &#8220;Data and grammar: Means and
individuals&#8221;, <i>Theoretical Linguistics</i>, 33: 269&#8211;318.</li>

<li>Derwing, Bruce, 1973, <i>Transformational Grammar as a Theory of
Language Acquisition: A Study in the Empirical, Conceptual and
Methodological Foundations of Contemporary Linguistics</i>, Cambridge:
Cambridge University Press.</li>

<li>Devitt, Michael, 2006, <i>Ignorance of Language</i>, Oxford:
Clarendon Press.</li>

<li>Dummett, Michael, 1986, &#8220;&#8216;A nice derangement of
epitaphs&#8217;: Some comments on Davidson and Hacking&#8221;, in
<i>Truth and Interpretation</i>, Ernest Lepore (ed.), Oxford: Blackwell,
459&#8211;476.</li>

<li>Dunbar, Robin, 1998, &#8220;Theory of mind and the evolution of
language&#8221;, in James R. Hurford, Michael Studdert-Kennedy, and
Chris Knight (eds.), <i>Approaches to the Evolution of Language</i>,
Cambridge: Cambridge University Press, 92&#8211;110.</li>

<li>Eckert, Penelope, 1989, <i>Jocks and Burnouts: Social Categories
and Identity in the High School</i>, New York, NY: Teachers College
Press.</li>

<li>Edelman, Shimon and Christiansen, Morten, 2003, &#8220;How
seriously should we take minimalist syntax?&#8221;, <i>Trends in
Cognitive Sciences</i>, 7: 60&#8211;61.</li>

<li>Elman, Jeffrey L., 1993, &#8220;Learning and development in neural
networks: The importance of starting small&#8221;, <i>Cognition</i>, 48:
71&#8211;99.</li>

<li>Ferreira, Fernanda, 2005, &#8220;Psycholinguistics, formal
grammars, and cognitive science&#8221;, <i>The Linguistic Review</i>, 22:
365&#8211;380.</li>

<li>Fitch, W. Tecumseh, 2010, &#8220;Prolegomena to a future science
of biolinguistics&#8221;, <i>Biolinguistics</i>, 3(4): 283&#8211;320.</li>

<li>Fodor, Janet Dean and Crowther, Carrie, 2002, &#8220;Understanding
stimulus poverty arguments&#8221;, <i>The Linguistic Review</i>,
19(1&#8211;2): 105&#8211;146.</li>

<li>Fodor, Jerry A., 1983, <i>The Modularity of Mind: An Essay on
Faculty Psychology</i>, Cambridge, Massachusetts: MIT Press.</li>

<li>Fodor, Jerry A. and Pylyshyn, Zenon W., 1988, &#8220;Connectionism
and cognitive architecture: A critical analysis&#8221;, <i>Cognition</i>,
28: 3&#8211;71.</li>

<li>Foraker, Stephani, Regier, Terry, Khetarpal, Naveen, Perfors, Amy,
and Tenenbaum, Joshua, 2009, &#8220;Indirect evidence and the poverty
of the stimulus: The case of anaphoric one&#8221;, <i>Cognitive Science</i>,
33: 287&#8211;300.</li>

<li>George, Alexander, 1989, &#8220;How not to become confused about
linguistics&#8221;, in <i>Reflections on Chomsky</i>, Alexander George
(ed.), Oxford: Basil Blackwell, 90&#8211;110.</li>

<li>Gibson, Edward and Fedorenko, Evelina, 2013, &#8220;The need for
quantitative methods in syntax and semantics
research&#8221;, <i>Language and Cognitive Processes</i>, 28:
88&#8211;124.  (See also the authors&#8217; summary in <i>Trends in
Cognitive Sciences</i>, 2010, Volume 14, pp. 233&#8211;234.)</li>

<li>Gold, E. Mark, 1967, &#8220;Language identification in the
limit&#8221;, <i>Information and Control</i>, 10: 447&#8211;474.</li>

<li>Goldberg, Adele, 1995, <i>Constructions: A Construction Grammar
Approach to Argument Structure</i>, Chicago: University of Chicago
Press.</li>

<li>Gray, Russell D., 2005, &#8220;Pushing the time barrier in the
quest for language roots&#8221;, <i>Science</i>, 209: 307&#8211;308.</li>

<li>Gray, Russell D., Drummond, A. J., and Greenhill, S. J., 2009,
&#8220;Phylogenies reveal expansion pulses and pauses in Pacific
settlement&#8221;, <i>Science</i>, 323: 479&#8211;483.</li>

<li>Gray, Russell D. and Jordan, Fiona M., 2000, &#8220;Language trees
support the express-train sequence of Austronesian expansion&#8221;,
<i>Nature</i>, 405: 1052&#8211;1055. Minor technical correction noted in
<i>Nature</i>, 409: 743 (8 February 2001).</li>

<li>Greenhill, S. J., Atkinson, Q. D., Meade, A., and Gray, R. D.,
2010, &#8220;The shape and tempo of language evolution&#8221;,
<i>Proceedings of the Royal Society B</i>, 277: 2443&#8211;2450.</li>

<li>Harris, Zellig, 1957, &#8220;Co-occurrence and transformation in
linguistic structure&#8221;, <i>Language</i>, 33: 283&#8211;340.</li>

<li>Hauser, Marc D., Chomsky, Noam, and Fitch, W. Tecumseh, 2002
[HCF], &#8220;The faculty of language: What is it, who has it, and how
did it evolve&#8221;, <i>Science</i>, 298: 1569&#8211;1579.</li>

<li>Hockett, Charles F., 1968, <i>The State of the Art</i>, The Hague:
Mouton.</li>

<li>Hoijer, Harry, 1954, &#8220;The Sapir Whorf hypothesis&#8221;, in
<i>Language in Culture</i>, Harry Hoijer (ed.), Chicago: University of
Chicago Press, 92&#8211;105.</li>

<li>Hopper, Paul and Thompson, Sandra, 1993, &#8220;Language
universals, discourse pragmatics, and semantics&#8221;, <i>Linguistic
Sciences</i>, 15: 357&#8211;376.</li>

<li>Horner, Victoria, Whiten, Andrew, Flynn, Emma, and de Waal, Frans
B. M., 2006, &#8220;Faithful replication of foraging techniques along
cultural transmission chains by chimpanzees and children&#8221;,
<i>Proceedings of the National Academy of Sciences</i>, 103:
13878&#8211;13883.</li>

<li>Hurford, James R., 2000, &#8220;Social transmission favours
linguistic generalization&#8221;, in J. R. Hurford,
M. Studdert-Kennedy, and C. Knight (eds.), <i>The Evolutionary
Emergence of Language: Social Function and the Origins of Linguistic
Form</i>, Cambridge: Cambridge University Press, 219&#8211;230.</li>

<li>Itkonen, Esa, 1978, <i>Grammatical Theory and Metascience: A
Critical Investigation into the Methodological and Philosophical
Foundations of &#8216;Autonomous&#8217; Linguistics</i>, Berlin:
Walter de Gruyter.</li>

<li>Jackendoff, Ray S., 2002, <i>Foundations of Language: Brain,
Meaning, Grammar, Evolution</i>, Oxford: Oxford University Press.</li>

<li>Jacobson, Pauline, 1996, &#8220;The syntax/semantics interface in
categorial grammar&#8221;, in <i>Handbook of Contemporary Semantic
Theory</i>, Shalom Lappin (ed.), Cambridge, Massachusetts: Oxford
University Press, 89&#8211;116.</li>

<li>Jain, Sanjay, Osherson, Daniel N., Royer, James S., and Sharma,
Arun, 1999, <i>Systems That Learn</i>, Cambridge, Massachusetts: MIT
Press, 2nd ed.</li>

<li>Johnson, Kent, 2004, &#8220;Gold&#8217;s theorem and cognitive
science&#8221;, <i>Philosophy of Science</i>, 70(4): 571&#8211;592.</li>

<li>&#8211;&#8211;&#8211;, 2007, &#8220;On the systematicity of
language and thought&#8221;, <i>Journal of Philosophy</i>, 101:
111&#8211;139.</li>

<li>Joos, Martin (ed.), 1966, <i>Readings in Linguistics I: The
Development of Descriptive Linguistics in America 1925&#8211;56</i>,
Chicago, IL: University of Chicago Press, fourth ed.</li>

<li>Karlsson, Fred, 2007, &#8220;Constraints on multiple
center-embedding of clauses&#8221;, <i>Journal of Linguistics</i>,
43(2): 365&#8211;392.</li>

<li>Katz, Jerrold J., 1980, &#8220;Chomsky on meaning&#8221;,
<i>Language</i>, 56(1): 1&#8211;41.</li>

<li>&#8211;&#8211;&#8211;, 1981, <i>Language and Other Abstract
Objects</i>, Totowa, NJ: Rowman and Littlefield.</li>

<li>&#8211;&#8211;&#8211; (ed.), 1985, <i>Philosophy of
Linguistics</i>, Oxford: Oxford University Press.</li>

<li>&#8211;&#8211;&#8211;, 1998, <i>Realistic Rationalism</i>,
Cambridge, Massachusetts: MIT Press.</li>

<li>Kay, Paul and Regier, Terry, 2006, &#8220;Language, thought and
color: Recent developments&#8221;, <i>Trends in Cognitive
Sciences</i>, 10(2): 51&#8211;53.</li>

<li>Kay, Paul, Berlin, Brent, Maffi, Luisa, Merrifield, William,
2011, <i>The World Color Survey</i>, Stanford, Center for the Study of
Language and Information.</li>

<li>Kirby, Simon, 2001, &#8220;Spontaneous evolution of linguistic
structure: An iterated learning model of the emergence of regularity
and irregularity&#8221;, <i>IEEE Transactions on Evolutionary
Computation</i>, 5(2): 102&#8211;110.</li>

<li>Kirby, Simon, Cornish, Hannah, and Smith, Kenny, 2008,
&#8220;Cumulative cultural evolution in the laboratory: An
experimental approach to the origins of structure in human
language&#8221;, <i>Proceedings of the National Academy of
Sciences</i>, 101(31): 10681&#8211;10686.</li>

<li>Kousta, S. T., Vinson, D. P., and Vigliocco, G., 2008,
&#8220;Investigating linguistic relativity through bilingualism: The
case of grammatical gender&#8221;, <i>Journal of Experimental Psychology:
Learning, Memory, and Cognition</i>, 34(4): 843&#8211;858.</li>

<li>Labov, William, 1966, <i>The Social Stratification of English in
New York City</i>, Washington, DC: Center for Applied Linguistics. 2nd
edition Cambridge University Press, 2006.</li>

<li>&#8211;&#8211;&#8211;, 1975, &#8220;Empirical foundations of
linguistic theory&#8221;, in <i>The Scope of American Linguistics</i>,
R. Austerlitz (ed.), Lisse: Peter de Ridder, 77&#8211;133.</li>

<li>&#8211;&#8211;&#8211;, 1996, &#8220;When intuitions fail&#8221;,
in L. McNair, K. Singer, L. Dobrin, and M. Aucon (eds.), <i>Papers
from the Parasession on Theory and Data in Linguistics</i>, Chicago:
Chicago Linguistic Society, 77&#8211;106.</li>

<li>Lappin, Shalom and Shieber, Stuart, 2007, &#8220;Machine learning
theory and practice as a source of insight into universal
grammar&#8221;, <i>Journal of Linguistics</i>, 43: 393&#8211;427.</li>

<li>Larson, Richard, 1988, &#8220;On the double object
construction&#8221;, <i>Linguistic Inquiry</i>, 19: 335&#8211;391.</li>

<li>Laurence, Stephen and Margolis, Eric, 2001, &#8220;The poverty of
the stimulus argument&#8221;, <i>British Journal of Philosophy of
Science</i>, 52(2): 217&#8211;276.</li>

<li>Levelt, W. J. M., 2008, <i>An Introduction to the Theory of Formal
Languages and Automata</i>, Amsterdam: John Benjamins.</li>

<li>Lewis, David, 1969, <i>Convention: A Philosophical Study</i>,
Cambridge, Massachusetts: Harvard University Press.</li>

<li>Li, P. and Gleitman, Lila, 2002, &#8220;Turning the tables:
Language and spatial reasoning&#8221;, <i>Cognition</i>, 83:
265&#8211;294.</li>

<li>Lucy, John, 1996, &#8220;The scope of linguistic relativity: An
analysis and review of empirical research&#8221;, in J. Gumperz and
S. Levinson (eds.), <i>Rethinking Linguistic Relativity</i>,
Cambridge: Cambridge University Press, 37&#8211;69.</li>

<li>MacWhinney, Brian, 2005, &#8220;The emergence of grammar from
perspective taking&#8221;, in D. Pecher and R. A. Zwaan (eds.), <i>The
Grounding of Cognition</i>, Cambridge: Cambridge University Press,
198&#8211;223.</li>

<li>Martinet, Andr&#233;, 1960, <i>Elements of General Linguistics</i>,
London: Faber.</li>

<li>Matthews, Robert, 1984, &#8220;The plausibility of
rationalism&#8221;, <i>Journal of Philosophy</i>, 81:
492&#8211;515. Reprinted in Matthews and Demopoulos (1989),
51&#8211;75.</li>

<li>Matthews, Robert, 2007, &#8220;The case for linguistic
nativism&#8221;, in <i>Contemporary Debates in Cognitive Science</i>,
Robert J. Stainton, (ed.), Oxford: Blackwell, 81&#8211;96.</li>

<li>Matthews, Robert and Demopoulos, William (eds.),
1989, <i>Learnability and Language Acquisition</i>, Dordrecht:
Foris.</li>

<li>Millikan, Ruth Garrett, 2003, &#8220;In defense of public
language&#8221;, in L. M. Antony and N. Hornstein (eds.), <i>Chomsky
and His Critics</i>, Oxford: Blackwell, 215&#8211;237.</li>

<li>Montague, Richard, 1974, <i>Formal Philosophy: Selected Papers of
Richard Montague</i>, New Haven: Yale University Press. Edited by
R. Thomason.</li>

<li>Morris, Charles, 1938, <i>Foundations of the Theory of Signs</i>,
Chicago: University of Chicago Press.</li>

<li>Napoli, Donna Jo, 1996, <i>Linguistics: An Introduction</i>, New
York, NY: Oxford University Press.</li>

<li>Newmeyer, Frederick J., 1986, <i>Linguistic Theory in America</i>,
New York: Academic Press, 2nd ed.</li>

<li>&#8211;&#8211;&#8211;, 1991, &#8220;Functional explanation in
linguistics and the origins of language&#8221;, <i>Language and
Communication</i>, 11(1&#8211;2): 3&#8211;28.</li>

<li>&#8211;&#8211;&#8211;, 1998, &#8220;On the supposed
&#8216;counterfunctionality&#8217; of universal grammar: Some
evolutionary implications&#8221;, in J. R. Hurford,
M. Studdert-Kennedy, and C. Knight (eds.), <i>Approaches to the
Evolution of Language</i>, Cambridge: Cambridge University Press,
305&#8211;319.</li>

<li>&#8211;&#8211;&#8211;, 2007, &#8220;Commentary on Sam Featherston,
&#8216;Data in generative grammar: The stick and the
carrot&#8221;&#8217;, <i>Theoretical Linguistics</i>, 33: 395&#8211;399.</li>

<li>Newport, Elissa L., 1988, &#8220;Constraints on learning and their
role in language acquisition: Studies of the acquisition of American
sign language&#8221;, <i>Language Sciences</i>, 10: 147&#8211;172.</li>

<li>O&#8217;Grady, William, 2008, &#8220;The emergentist program&#8221;,
<i>Lingua</i>, 118: 447&#8211;464.</li>

<li>Osherson, Daniel N., Stob, Michael, and Weinstein, Scott, 1984,
&#8220;Learning theory and natural language&#8221;, <i>Cognition</i>, 17(1):
1&#8211;28. Reprinted in Matthews and Demopoulos (1989),
19&#8211;50.</li>

<li>Partee, Barbara, 1975, &#8220;Montague grammar and
transformational grammar&#8221;, <i>Linguistic Inquiry</i>, 6:
203&#8211;300.</li>

<li>Pelletier, Francis Jeffry, 1991, &#8220;The principle of semantic
compositionality&#8221;, <i>Topoi</i>, 13: 11&#8211;24; reprinted,
with additions, in S. Davis and B. Gillon, <i>Semantics: A Reader</i>,
Oxford: Oxford University Press, 2004, pp. 133&#8211;156.</li>

<li>Penn, Julia, 1972, <i>Linguistic Relativity versus Innate Ideas:
The Origins of the Sapir-Whorf Hypothesis in German Thought</i>,
Paris: Mouton.</li>

<li>Phillips, Colin, 2010, &#8220;Should we impeach armchair
linguists?&#8221;, in S. Iwasaki, H. Hoji, P. Clancy, and S.-O. Sohn
(eds.), <i>Japanese-Korean Linguistics 17</i>, Stanford, CA: CSLI
Publications, 49&#8211;64.</li>

<li>Piattelli-Palmarini, Massimo, 1989, &#8220;Evolution, selection,
and cognition: From &#8216;learning&#8217; to parameter setting in
biology and the study of language&#8221;, <i>Cognition</i>, 31:
1&#8211;44.</li>

<li>Pinker, Steven, 1994, <i>The Language Instinct: The New Science of
Language and Mind</i>, New York, NY: Morrow Press.</li>

<li>&#8211;&#8211;&#8211;, 2007, <i>The Stuff of Thought: Language as
a Window into Human Nature</i>, New York, NY: Viking Penguin.</li>

<li>Pinker, Steven and Bloom, Paul, 1990, &#8220;Natural language and
natural selection&#8221;, <i>Behavioral and Brain Sciences</i>, 13:
707&#8211;726.</li>

<li>Pinker, Steven and Jackendoff, Ray S., 2005, &#8220;The faculty of
language: What&#8217;s special about it?&#8221;, <i>Cognition</i>, 95:
201&#8211;236.</li>

<li>Power, Camilla, 1998, &#8220;&#8216;Old wives&#8217; tales&#8217;:
The gossip hypothesis and the reliability of cheap signals&#8221;, in
J. R. Hurford, M. Studdert-Kennedy, and C. Knight
(eds.), <i>Approaches to the Evolution of Language</i>, Cambridge:
Cambridge University Press, 111&#8211;129.</li>

<li>Prinz, Jesse, 2002, <i>Furnishing the Mind: Concepts and Their
Perceptual Basis</i>, Cambridge, Massachusetts: MIT Press.</li>

<li>Pullum, Geoffrey K., 1983, &#8220;How many possible human
languages are there?&#8221;, <i>Linguistic Inquiry</i>, 14:
447&#8211;467.</li>

<li>Pullum, Geoffrey K. and Scholz, Barbara C., 1997,
&#8220;Theoretical linguistics and the ontology of linguistic
structure&#8221;, in T. Haukioja, M.-L. Helasvuo, and M. Miestamo,
(eds.), <i>SKY 1997: 1997 Yearbook of the Linguistic Association of
Finland</i>, Turku: Suomen kielitieteelinen yhdistys [Linguistic
Association of Finland], 25&#8211;47.</li>

<li>&#8211;&#8211;&#8211;, 2002, &#8220;Empirical assessment of
stimulus poverty arguments&#8221;, <i>The Linguistic Review</i>, 19:
9&#8211;50.</li>

<li>&#8211;&#8211;&#8211;, 2007, &#8220; Systematicity and Natural
Language Syntax&#8221;, <i>Croatian Journal of Philosophy</i>, 21:
375&#8211;402.</li>

<li>&#8211;&#8211;&#8211;, 2010, &#8220;Recursion and the infinitude
claim&#8221;, in <i>Recursion in Human Language</i>, Harry van der Hulst
(ed.), Berlin: Mouton de Gruyter, no. 104 in Studies in Generative
Grammar, 113&#8211;138.</li>

<li>Putnam, Hilary, 1963, &#8220;Probability and confirmation&#8221;,
in <i>The Voice of America Forum Lectures</i> (Philosophy of Science Series,
No. 10), Hilary Putnam, (ed.), Washington, D.C.: United States
Information Agency. Reprinted in <i>Mathematics, Matter and Method</i>,
Cambridge: Cambridge University Press, 1975, 293&#8211;304.</li>

<li>Quine, Willard Van Orman, 1972, &#8220;Linguistics and
philosophy&#8221;, in <i>Language and Philosophy: A Symposium</i>, Sidney
Hook (ed.), New York: New York University Press, 95&#8211;98.</li>

<li>&#8211;&#8211;&#8211;, 1987, <i>Quiddities: An Intermittently
Philosophical Dictionary</i>, Cambridge, Massachusetts: Harvard
University Press.</li>

<li>Rohde, D. L. T. and Plaut, D. C., 1999, &#8220;Language
acquisition in the absence of explicit negative evidence: How
important is starting small?&#8221;, <i>Cognition</i>, 72: 67&#8211;109.</li>

<li>Roland, Doug and Jurafsky, Daniel, 2002, &#8220;Verb sense and
verb subcategorization probabilities&#8221;, in S. Stevenson and
P. Merlo (eds.), <i>The Lexical Basis of Sentence Processing: Formal,
Computational, and Experimental Issues</i>, Amsterdam: John Benjamins,
325&#8211;346.</li>

<li>Sampson, Geoffrey, 2001, <i>Empirical Linguistics</i>, London:
Continuum Press.</li>

<li>&#8211;&#8211;&#8211;, 2005, <i>The Language Instinct Debate</i>,
London: Continuum Press.</li>

<li>Sapir, Edward, 1921, <i>Language</i>, New York, NY: Harcourt.</li>

<li>&#8211;&#8211;&#8211;, 1929, &#8220;The status of linguistics as a
science&#8221;, <i>Language</i>, 5: 207&#8211;214. Reprinted in David
Mandelbaum (ed.), <i>Selected Writings of Edward Sapir in Language
Culture and Personality</i>,  Berkeley and Los Angeles:
University of California Press, 1968, 160&#8211;166.</li>

<li>Saussure, Ferdinand de, 1916, <i>Cours de linguistique
g&#233;n&#233;rale</i>, Paris and Lausanne: Payot. Edited and
published after Saussure&#8217;s death by Charles Bally and Albert Sechehaye
with the collaboration of Albert Riedlinger.  English translation by
Roy Harris (1998) in Ferinand de Saussure <i>Course in General
Linguistics</i>, New York: Open Court.</li>

<li>Scholz, Barbara C. and Pullum, Geoffrey K., 2002, &#8220;Searching
for arguments to support linguistic nativism&#8221;, <i>The Linguistic
Review</i>, 19: 185&#8211;223.</li>

<li>&#8211;&#8211;&#8211;, 2006, &#8220;Irrational nativist
exuberance&#8221;, in <i>Contemporary Debates in Cognitive Science</i>,
Robert J. Stainton (ed.), Oxford: Basil Blackwell, 59&#8211;80.</li>

<li>&#8211;&#8211;&#8211;, 2007, &#8220;Tracking the origins of
generative grammar&#8221;, <i>Journal of Linguistics</i>, 43:
701&#8211;723.</li>

<li>Sch&#252;tze, Carson, 1996, <i>The Empirical Base of Linguistics:
Grammaticality Judgments and Linguistic Methodology</i>, Chicago:
University of Chicago Press.</li>

<li>Seuren, Pieter A. M., 1998, <i>Western Linguistics: An Historical
Introduction</i>, Oxford: Blackwell.</li>

<li>Shinohara, T., 1990, &#8220;Inductive inference of monotonic
formal systems from positive data&#8221;, in S. Arikawa, S. Goto,
S. Ohsuga, and T. Yokomori (eds.), <i>Algorithmic Learning Theory</i>,
Berlin: Springer, 339&#8211;351.</li>

<li>Slobin, Dan, 1996, &#8220;From thought and language to thinking
for speaking&#8221;, in J. Gumperz and S. Levinson
(eds.), <i>Rethinking Linguistic Relativity</i>, Cambridge: Cambridge
University Press, 70&#8211;96.</li>

<li>Soames, Scott, 1984, &#8220;Linguistics and psychology&#8221;,
<i>Linguistics and Philosophy</i>, 7: 155&#8211;179.</li>

<li>Sprouse, Jon, 2011, &#8220;A test of the cognitive assumptions of
magnitude estimation: Commutativity does not hold for acceptability
judgments&#8221;, <i>Language</i>, 87(2): 274&#8211;288.</li>

<li>Stainton, Robert J. (ed.), 2006, <i>Contemporary Debates in
Cognitive Science</i>, Oxford: Blackwell.</li>

<li>Steedman, Mark, 2000, <i>The Syntactic Process</i>, Cambridge,
Massachusetts: MIT Press.</li>

<li>Szabolcsi, Anna, 1997, &#8220;Strategies for scope taking&#8221;,
in <i>Ways of Scope Taking</i>, Anna Szabolcsi (ed.), Dordrecht: Kluwer,
109&#8211;155.</li>

<li>Thierry, Guillaume, Athanasopulous, Panos, Wiggett, Alison,
Dering, Benjamin, and Kuipers, Jan-Rouke, 2009, &#8220;Unconscious
effects of language-specific terminology on pre-attentive color
perception&#8221;, <i>Proceedings of the National Academy of Sciences</i>,
106(11): 4567&#8211;4570.</li>

<li>Tomalin, Marcus, 2006, <i>Linguistics and the Formal Sciences: The
Origins of Generative Grammar</i>, Cambridge: Cambridge University
Press.</li>

<li>Tomasello, Michael, 1998, &#8220;Introduction&#8221;, in <i>The New
Psychology of Language: Cognitive and Functional Approaches to
Language Structure</i>, Michael Tomasello (ed.), Mahwah, NJ: Lawrence
Erlbaum.</li>

<li>&#8211;&#8211;&#8211;, 2003, <i>Constructing a Language: A
Usage-Based Theory of Language Acquisition</i>, Cambridge,
Massachusetts: Harvard University Press.</li>

<li>Tomlin, Russell S., 1990, &#8220;Functionalism in second language
acquisition&#8221;, <i>Studies in Second Language Acquisition</i>, 12:
155&#8211;177.</li>

<li>Valian, Virginia, 1982, &#8220;Psycholinguistic experiment and
linguistic intuition&#8221;, in T. W. Simon and R. J. Scholes
(eds.), <i>Language, Mind, and Brain</i>, Hillsdale, NJ: Lawrence
Erlbaum, 179&#8211;188.</li>

<li>Van Valin, Robert, 1991, &#8220;Functionalist linguistic theory
and language acquisition&#8221;, <i>First Language</i>, 11: 7&#8211;40.</li>

<li>Voegelin, Carl F. and Harris, Zellig S., 1951, &#8220;Methods for
determining intelligibility among dialects of natural
languages&#8221;, <i>Proceedings of the American Philosophical Society</i>,
95(3): 322&#8211;329.</li>

<li>Wasow, Thomas and Arnold, Jennifer, 2005, &#8220;Intuitions in
linguistic argumentation&#8221;, <i>Lingua</i>, 115: 1481&#8211;1496.</li>

<li>Weinreich, Max, 1945, &#8220;Der yivo un di problemen fun undzer
tsayt&#8221;, <i>Yivo Bleter</i>, 25: 3&#8211;18.</li>

<li>Weskott, Thomas and Fanselow, Gisbert, 2011, &#8220;On the
informativity of different measures of linguistic
acceptability&#8221;, <i>Language</i>, 87(2): 249&#8211;273.</li>

<li>Wexler, Kenneth and Culicover, Peter, 1980, <i>Formal Principles
of Language Acquisition</i>, Cambridge, Massachusetts: MIT Press.</li>

<li>Wexler, Kenneth and Hamburger, Henry, 1973, &#8220;On the
insufficiency of surface data for the learning of transformational
languages&#8221;, in J. Hintikka, J. Moravcsik, and P. Suppes
(eds.), <i>Approaches to Natural Language</i>, Dordrecht: Reidel,
16&#8211;179.</li>

<li>Whorf, Benjamin Lee, 1956, <i>Language, Thought and Reality</i>,
Cambridge University Press: MIT Press. Edited by John B. Carroll.</li>

<li>Worden, Robert, 1998, &#8220;The evolution of language from social
intelligence&#8221;, in J. R. Hurford, M. Studdert-Kennedy, and
C. Knight (eds.), <i>Approaches to the Evolution of Language</i>,
Cambridge: Cambridge University Press, 148&#8211;166.</li>

</ul>

</div>

<div id="academic-tools">

<h2 id="Aca">Academic Tools</h2>

<blockquote>
<table class="vert-top">
<tr>
<td><img src="https://plato.stanford.edu/symbols/sepman-icon.jpg" alt="sep man icon"></td>
<td><a href="https://plato.stanford.edu/cgi-bin/encyclopedia/archinfo.cgi?entry=linguistics" target="other">How to cite this entry</a>.</td>
</tr>

<tr>
<td><img src="https://plato.stanford.edu/symbols/sepman-icon.jpg" alt="sep man icon"></td>
<td><a href="https://leibniz.stanford.edu/friends/preview/linguistics/" target="other">Preview the PDF version of this entry</a> at the
 <a href="https://leibniz.stanford.edu/friends/" target="other">Friends of the SEP Society</a>.</td>
</tr>

<tr>
<td><img src="https://plato.stanford.edu/symbols/inpho.png" alt="inpho icon"></td>
<td><a href="https://www.inphoproject.org/entity?sep=linguistics&amp;redirect=True" target="other">Look up this entry topic</a>
 at the <a href="https://www.inphoproject.org/" target="other">Internet Philosophy Ontology Project</a>
 (InPhO).</td>
</tr>

<tr>
<td><img src="https://plato.stanford.edu/symbols/pp.gif" alt="phil papers icon"></td>
<td><a href="https://philpapers.org/sep/linguistics/" target="other">Enhanced bibliography for this entry</a>
at <a href="https://philpapers.org/" target="other">PhilPapers</a>, with links to its database.</td>
</tr>

</table>
</blockquote>

</div>

<div id="other-internet-resources">

<h2><a name="Oth">Other Internet Resources</a></h2>

<ul>

<li>Sprouse, Jon and Almeida, Diogo, 2010, 
 &#8220;<a href="http://www.socsci.uci.edu/~jsprouse/papers/empirical_foundation.pdf" target="other">A formal experimental investigation of the empirical foundation of generative syntactic theory</a>, 
  manuscript.&#8221;</li>

<li><a href="http://en.wikipedia.org/wiki/Treebank" target="other">Treebank</a>,
  Wikipedia.</li>

<li><a href="http://www.ldc.upenn.edu/Catalog/readme_files/switchboard.readme.html" target="other">The Switchboard Corpus</a>.</li>

<li><a href="http://www.natcorp.ox.ac.uk/docs/URG/" target="other">The British National Corpus</a>.</li>

</ul>

</div>

<div id="related-entries">

<h2><a name="Rel">Related Entries</a></h2>

<p>

 <a href="https://plato.stanford.edu/entries/analysis/">analysis</a> |
 <a href="https://plato.stanford.edu/entries/assertion/">assertion</a> |
 <a href="https://plato.stanford.edu/entries/compositionality/">compositionality</a> |
 <a href="https://plato.stanford.edu/entries/defaults-semantics-pragmatics/">defaults in semantics and pragmatics</a> |
 <a href="https://plato.stanford.edu/entries/descriptions/">descriptions</a> |
 <a href="https://plato.stanford.edu/entries/logical-empiricism/">empiricism: logical</a> |
 <a href="https://plato.stanford.edu/entries/idiolects/">idiolects</a> |
 <a href="https://plato.stanford.edu/entries/innate-acquired/">innate/acquired distinction</a> |
 <a href="https://plato.stanford.edu/entries/innateness-language/">innateness: and language</a> |
 <a href="https://plato.stanford.edu/entries/language-thought/">language of thought hypothesis</a> |
 <a href="https://plato.stanford.edu/entries/computational-linguistics/">linguistics: computational</a> |
 <a href="https://plato.stanford.edu/entries/logic-intensional/">logic: intensional</a> |
 <a href="https://plato.stanford.edu/entries/mental-representation/">mental representation</a> |
 <a href="https://plato.stanford.edu/entries/pragmatics/">pragmatics</a> |
 <a href="https://plato.stanford.edu/entries/prop-attitude-reports/">propositional attitude reports</a> |
 <a href="https://plato.stanford.edu/entries/reference/">reference</a> |
 <a href="https://plato.stanford.edu/entries/relativism/">relativism</a> |
 <a href="https://plato.stanford.edu/entries/rigid-designators/">rigid designators</a>

</p>

</div>

<div id="acknowledgments">

<h3>Acknowledgments</h3>

<p>

The authors are very grateful to the two SEP referees, Tom Wasow and
William Starr, who provided careful reviews of our drafts; to
Bonnie Webber and Zoltan Galsi for insightful comments and advice;
and to Dean Mellow for some helpful corrections.
BCS was the lead author of this article throughout the lengthy period
of its preparation, and worked on it in collaboration with FJP and GKP
until the post-refereeing revision was submitted at the end of April
2011.  She died two weeks later, on May 14.  FJP and GKP oversaw the
few final corrections that were made when the HTML version was
produced in September 2011.</p>

</div>



</div><!-- #aueditable --><!--DO NOT MODIFY THIS LINE AND BELOW-->

<!-- END ARTICLE HTML -->

</div> 